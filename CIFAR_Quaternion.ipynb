{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb827bdd-8e7a-4529-b171-bfe39f21755e",
   "metadata": {},
   "source": [
    "# RealNet CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44ea5659-ba60-47df-99d3-b04c1f55ea06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch device: cuda\n",
      "======================================================================\n",
      "BLOCK 1: RealNet Training (CIFAR-10)\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "  • Dataset: CIFAR-10 (32×32 RGB)\n",
      "  • Input dimension: 3072 (32×32×3 flattened)\n",
      "  • Batch size: 128\n",
      "  • Patience: 10\n",
      "  • Seeds: [42, 123, 456]\n",
      "  • Architecture: 3072 → 16 → 64 → 10\n",
      "======================================================================\n",
      "\n",
      "Creating stratified samples...\n",
      "  Sampling took 0.00s\n",
      "  Train samples: 15000 (stratified, 1500 per class)\n",
      "  Test samples:  3000 (stratified, 300 per class)\n",
      "\n",
      "======================================================================\n",
      "SEED 42\n",
      "======================================================================\n",
      "\n",
      "  Training RealNet (seed=42)...\n",
      "  [Real] Epoch  1 | loss=2.0209 | test_acc=0.3273 | time=0.7s\n",
      "  [Real] Epoch  2 | loss=1.8281 | test_acc=0.3470 | time=1.1s\n",
      "  [Real] Epoch  3 | loss=1.7635 | test_acc=0.3577 | time=1.6s\n",
      "  [Real] Epoch  4 | loss=1.7250 | test_acc=0.3617 | time=1.9s\n",
      "  [Real] Epoch  5 | loss=1.6946 | test_acc=0.3710 | time=2.3s\n",
      "  [Real] Epoch  6 | loss=1.6776 | test_acc=0.3710 | time=2.7s\n",
      "  [Real] Epoch  7 | loss=1.6463 | test_acc=0.3630 | time=3.1s\n",
      "  [Real] Epoch  8 | loss=1.6154 | test_acc=0.3730 | time=3.5s\n",
      "  [Real] Epoch  9 | loss=1.6088 | test_acc=0.3753 | time=4.0s\n",
      "  [Real] Epoch 10 | loss=1.5864 | test_acc=0.3817 | time=4.4s\n",
      "  [Real] Epoch 11 | loss=1.5683 | test_acc=0.3740 | time=4.8s\n",
      "  [Real] Epoch 12 | loss=1.5486 | test_acc=0.3860 | time=5.2s\n",
      "  [Real] Epoch 13 | loss=1.5296 | test_acc=0.3843 | time=5.6s\n",
      "  [Real] Epoch 14 | loss=1.5265 | test_acc=0.3827 | time=6.0s\n",
      "  [Real] Epoch 15 | loss=1.4951 | test_acc=0.3813 | time=6.4s\n",
      "  [Real] Epoch 16 | loss=1.4895 | test_acc=0.3793 | time=6.8s\n",
      "  [Real] Epoch 17 | loss=1.4768 | test_acc=0.3917 | time=7.3s\n",
      "  [Real] Epoch 18 | loss=1.4662 | test_acc=0.3953 | time=7.7s\n",
      "  [Real] Epoch 19 | loss=1.4526 | test_acc=0.3973 | time=8.1s\n",
      "  [Real] Epoch 20 | loss=1.4447 | test_acc=0.3897 | time=8.5s\n",
      "  [Real] Epoch 21 | loss=1.4271 | test_acc=0.3820 | time=8.9s\n",
      "  [Real] Epoch 22 | loss=1.4293 | test_acc=0.3757 | time=9.3s\n",
      "  [Real] Epoch 23 | loss=1.4119 | test_acc=0.3840 | time=9.7s\n",
      "  [Real] Epoch 24 | loss=1.4068 | test_acc=0.3870 | time=10.1s\n",
      "  [Real] Epoch 25 | loss=1.3962 | test_acc=0.3813 | time=10.5s\n",
      "  [Real] Epoch 26 | loss=1.3853 | test_acc=0.3960 | time=10.9s\n",
      "  [Real] Epoch 27 | loss=1.3881 | test_acc=0.3860 | time=11.3s\n",
      "  [Real] Epoch 28 | loss=1.3623 | test_acc=0.3900 | time=11.7s\n",
      "  [Real] Epoch 29 | loss=1.3606 | test_acc=0.3833 | time=12.1s\n",
      "  [Real] Early stop at epoch 29 (no improvement for 10 epochs)\n",
      "\n",
      "  → Saved preprocessor state from seed 42\n",
      "\n",
      "======================================================================\n",
      "SEED 123\n",
      "======================================================================\n",
      "\n",
      "  Training RealNet (seed=123)...\n",
      "  [Real] Epoch  1 | loss=1.9978 | test_acc=0.3283 | time=0.4s\n",
      "  [Real] Epoch  2 | loss=1.8132 | test_acc=0.3567 | time=0.9s\n",
      "  [Real] Epoch  3 | loss=1.7601 | test_acc=0.3523 | time=1.3s\n",
      "  [Real] Epoch  4 | loss=1.7176 | test_acc=0.3880 | time=1.7s\n",
      "  [Real] Epoch  5 | loss=1.6878 | test_acc=0.3850 | time=2.1s\n",
      "  [Real] Epoch  6 | loss=1.6528 | test_acc=0.3907 | time=2.5s\n",
      "  [Real] Epoch  7 | loss=1.6329 | test_acc=0.3910 | time=2.9s\n",
      "  [Real] Epoch  8 | loss=1.6106 | test_acc=0.3867 | time=3.3s\n",
      "  [Real] Epoch  9 | loss=1.5985 | test_acc=0.3820 | time=3.8s\n",
      "  [Real] Epoch 10 | loss=1.5776 | test_acc=0.3910 | time=4.2s\n",
      "  [Real] Epoch 11 | loss=1.5712 | test_acc=0.4080 | time=4.6s\n",
      "  [Real] Epoch 12 | loss=1.5525 | test_acc=0.3990 | time=5.0s\n",
      "  [Real] Epoch 13 | loss=1.5326 | test_acc=0.3943 | time=6.1s\n",
      "  [Real] Epoch 14 | loss=1.5207 | test_acc=0.4037 | time=6.5s\n",
      "  [Real] Epoch 15 | loss=1.5134 | test_acc=0.3927 | time=6.9s\n",
      "  [Real] Epoch 16 | loss=1.4963 | test_acc=0.4017 | time=7.3s\n",
      "  [Real] Epoch 17 | loss=1.4777 | test_acc=0.3950 | time=7.7s\n",
      "  [Real] Epoch 18 | loss=1.4688 | test_acc=0.3897 | time=8.2s\n",
      "  [Real] Epoch 19 | loss=1.4598 | test_acc=0.3920 | time=8.6s\n",
      "  [Real] Epoch 20 | loss=1.4584 | test_acc=0.4077 | time=9.0s\n",
      "  [Real] Epoch 21 | loss=1.4383 | test_acc=0.3930 | time=9.4s\n",
      "  [Real] Early stop at epoch 21 (no improvement for 10 epochs)\n",
      "\n",
      "======================================================================\n",
      "SEED 456\n",
      "======================================================================\n",
      "\n",
      "  Training RealNet (seed=456)...\n",
      "  [Real] Epoch  1 | loss=2.0038 | test_acc=0.3460 | time=0.4s\n",
      "  [Real] Epoch  2 | loss=1.8185 | test_acc=0.3640 | time=0.9s\n",
      "  [Real] Epoch  3 | loss=1.7599 | test_acc=0.3680 | time=1.3s\n",
      "  [Real] Epoch  4 | loss=1.7233 | test_acc=0.3727 | time=1.8s\n",
      "  [Real] Epoch  5 | loss=1.6974 | test_acc=0.3753 | time=2.2s\n",
      "  [Real] Epoch  6 | loss=1.6636 | test_acc=0.3820 | time=2.6s\n",
      "  [Real] Epoch  7 | loss=1.6303 | test_acc=0.3860 | time=3.0s\n",
      "  [Real] Epoch  8 | loss=1.6089 | test_acc=0.3873 | time=3.4s\n",
      "  [Real] Epoch  9 | loss=1.5883 | test_acc=0.3923 | time=3.8s\n",
      "  [Real] Epoch 10 | loss=1.5799 | test_acc=0.3850 | time=4.2s\n",
      "  [Real] Epoch 11 | loss=1.5520 | test_acc=0.3883 | time=4.6s\n",
      "  [Real] Epoch 12 | loss=1.5503 | test_acc=0.4017 | time=5.1s\n",
      "  [Real] Epoch 13 | loss=1.5302 | test_acc=0.3897 | time=5.5s\n",
      "  [Real] Epoch 14 | loss=1.5149 | test_acc=0.3967 | time=5.9s\n",
      "  [Real] Epoch 15 | loss=1.5073 | test_acc=0.3980 | time=6.3s\n",
      "  [Real] Epoch 16 | loss=1.4952 | test_acc=0.3970 | time=6.8s\n",
      "  [Real] Epoch 17 | loss=1.4872 | test_acc=0.3933 | time=7.2s\n",
      "  [Real] Epoch 18 | loss=1.4698 | test_acc=0.3927 | time=7.6s\n",
      "  [Real] Epoch 19 | loss=1.4535 | test_acc=0.3893 | time=8.0s\n",
      "  [Real] Epoch 20 | loss=1.4378 | test_acc=0.3890 | time=8.4s\n",
      "  [Real] Epoch 21 | loss=1.4339 | test_acc=0.3923 | time=8.8s\n",
      "  [Real] Epoch 22 | loss=1.4194 | test_acc=0.3960 | time=9.2s\n",
      "  [Real] Early stop at epoch 22 (no improvement for 10 epochs)\n",
      "\n",
      "======================================================================\n",
      "REALNET SUMMARY (CIFAR-10)\n",
      "======================================================================\n",
      "\n",
      "Accuracy:   0.4023 ± 0.0044\n",
      "Time:       10.3s ± 1.3s\n",
      "Epochs:     24.0 ± 3.6\n",
      "Parameters: 50,906\n",
      "\n",
      "Per-seed results:\n",
      "  Seed 42: acc=0.3973, time=12.1s, epochs=29\n",
      "  Seed 123: acc=0.4080, time=9.4s, epochs=21\n",
      "  Seed 456: acc=0.4017, time=9.2s, epochs=22\n",
      "\n",
      "✓ Saved results to: realnet_cifar10_results.pt\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Block 1: RealNet Training (CIFAR-10)\n",
    "=====================================\n",
    "Trains the baseline Real MLP on 3 seeds using CIFAR-10.\n",
    "Saves the trained preprocessor for reuse in subsequent blocks.\n",
    "\n",
    "Outputs:\n",
    "- realnet_results.pt: Contains results dict and trained preprocessor state\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "print(f\"Using PyTorch device: {device}\")\n",
    "\n",
    "# ============================================\n",
    "# Comprehensive seed setting for reproducibility\n",
    "# ============================================\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    \"\"\"Set seeds for all RNG sources for reproducibility\"\"\"\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    # Python\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # NumPy\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # PyTorch deterministic mode\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # CuPy (if available)\n",
    "    try:\n",
    "        import cupy as cp\n",
    "        cp.random.seed(seed)\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    # PennyLane (if available)\n",
    "    try:\n",
    "        import pennylane as qml\n",
    "        qml.numpy.random.seed(seed)\n",
    "    except (ImportError, AttributeError):\n",
    "        pass\n",
    "\n",
    "# ============================================\n",
    "# Shared Bottleneck Preprocessor: 3072 → 16\n",
    "# ============================================\n",
    "\n",
    "class SharedPreprocessor(nn.Module):\n",
    "    \"\"\"Shared classical feature extractor: 3072 → 16\"\"\"\n",
    "    def __init__(self, input_dim=3072, bottleneck_dim=16):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, bottleneck_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten to (batch, 3072)\n",
    "        x = torch.tanh(self.fc(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Real-valued Head and Network\n",
    "# ============================================\n",
    "\n",
    "class RealHead(nn.Module):\n",
    "    \"\"\"Standard MLP: 16 → 64 → 10\"\"\"\n",
    "    def __init__(self, bottleneck_dim=16, hidden_dim=64, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(bottleneck_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RealNet(nn.Module):\n",
    "    \"\"\"Complete Real network: Preprocessor + RealHead\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.preprocessor = SharedPreprocessor(3072, 16)\n",
    "        self.head = RealHead(16, 64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.preprocessor(x)\n",
    "        return self.head(features)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Training and Evaluation\n",
    "# ============================================\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, device, show_progress=False):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        total_samples += x.size(0)\n",
    "\n",
    "        if show_progress and batch_idx % 50 == 0:\n",
    "            print(f\"    Batch {batch_idx}/{len(loader)}, samples: {total_samples}\", end=\"\\r\")\n",
    "\n",
    "    if show_progress:\n",
    "        print()\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += x.size(0)\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "def train_with_early_stopping(model, train_loader, test_loader, optimizer,\n",
    "                              device, max_epochs=40, patience=10, name=\"Model\"):\n",
    "    if isinstance(device, str):\n",
    "        device = torch.device(device)\n",
    "\n",
    "    best_acc = 0.0\n",
    "    epochs_without_improvement = 0\n",
    "    start = time.time()\n",
    "    last_acc = 0.0\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        loss = train_one_epoch(model, train_loader, optimizer, device, show_progress=False)\n",
    "        acc = evaluate(model, test_loader, device)\n",
    "        last_acc = acc\n",
    "\n",
    "        elapsed = time.time() - start\n",
    "        print(f\"  [{name}] Epoch {epoch:2d} | loss={loss:.4f} \"\n",
    "              f\"| test_acc={acc:.4f} | time={elapsed:.1f}s\")\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"  [{name}] Early stop at epoch {epoch} \"\n",
    "                  f\"(no improvement for {patience} epochs)\")\n",
    "            break\n",
    "\n",
    "    total_time = time.time() - start\n",
    "    return {\n",
    "        \"best_acc\": best_acc,\n",
    "        \"final_acc\": last_acc,\n",
    "        \"time\": total_time,\n",
    "        \"epochs\": epoch,\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Data utilities\n",
    "# ============================================\n",
    "\n",
    "def stratified_sample_from_targets(dataset, n_samples_per_class, seed=42):\n",
    "    \"\"\"\n",
    "    Create a stratified sample with n_samples_per_class from each class.\n",
    "    Uses dataset.targets directly - NO image loading during sampling.\n",
    "    \"\"\"\n",
    "    # CIFAR-10 exposes targets as a list\n",
    "    if hasattr(dataset, 'targets'):\n",
    "        targets = np.array(dataset.targets)\n",
    "    else:\n",
    "        # Fallback for wrapped datasets\n",
    "        targets = np.array([dataset[i][1] for i in range(len(dataset))])\n",
    "    \n",
    "    rng = np.random.RandomState(seed)\n",
    "    \n",
    "    sampled_indices = []\n",
    "    for c in range(10):\n",
    "        # Find all indices for class c\n",
    "        idx_c = np.where(targets == c)[0]\n",
    "        k = min(n_samples_per_class, len(idx_c))\n",
    "        # Sample without replacement\n",
    "        selected = rng.choice(idx_c, size=k, replace=False).tolist()\n",
    "        sampled_indices.extend(selected)\n",
    "    \n",
    "    # Shuffle the combined indices\n",
    "    rng.shuffle(sampled_indices)\n",
    "    return sampled_indices\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Main\n",
    "# ============================================\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"BLOCK 1: RealNet Training (CIFAR-10)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nConfiguration:\")\n",
    "    print(\"  • Dataset: CIFAR-10 (32×32 RGB)\")\n",
    "    print(\"  • Input dimension: 3072 (32×32×3 flattened)\")\n",
    "    print(\"  • Batch size: 128\")\n",
    "    print(\"  • Patience: 10\")\n",
    "    print(\"  • Seeds: [42, 123, 456]\")\n",
    "    print(\"  • Architecture: 3072 → 16 → 64 → 10\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # CIFAR-10 normalization (channel-wise means and stds)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.4914, 0.4822, 0.4465],\n",
    "            std=[0.2470, 0.2435, 0.2616]\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    # Load full datasets\n",
    "    full_train_ds = datasets.CIFAR10(root=\"./data\", train=True,\n",
    "                                     download=True, transform=transform)\n",
    "    full_test_ds = datasets.CIFAR10(root=\"./data\", train=False,\n",
    "                                    download=True, transform=transform)\n",
    "\n",
    "    # Create stratified samples (fast - uses targets only, no image loading)\n",
    "    print(\"\\nCreating stratified samples...\")\n",
    "    t0 = time.time()\n",
    "    train_indices = stratified_sample_from_targets(full_train_ds, n_samples_per_class=1500, seed=42)\n",
    "    test_indices = stratified_sample_from_targets(full_test_ds, n_samples_per_class=300, seed=42)\n",
    "    print(f\"  Sampling took {time.time()-t0:.2f}s\")\n",
    "    \n",
    "    train_ds = Subset(full_train_ds, train_indices)\n",
    "    test_ds = Subset(full_test_ds, test_indices)\n",
    "    \n",
    "    print(f\"  Train samples: {len(train_ds)} (stratified, 1500 per class)\")\n",
    "    print(f\"  Test samples:  {len(test_ds)} (stratified, 300 per class)\")\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=128,\n",
    "                              shuffle=True, num_workers=4)\n",
    "    test_loader = DataLoader(test_ds, batch_size=256,\n",
    "                             shuffle=False, num_workers=4)\n",
    "\n",
    "    seeds = [42, 123, 456]\n",
    "    all_results = []\n",
    "    trained_preprocessor_state = None\n",
    "\n",
    "    for seed in seeds:\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(f\"SEED {seed}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        set_all_seeds(seed)\n",
    "\n",
    "        print(f\"\\n  Training RealNet (seed={seed})...\")\n",
    "        real_model = RealNet().to(device)\n",
    "        real_opt = torch.optim.Adam(real_model.parameters(), lr=1e-3)\n",
    "        \n",
    "        result = train_with_early_stopping(\n",
    "            real_model, train_loader, test_loader, real_opt, device,\n",
    "            max_epochs=40, patience=10, name=\"Real\"\n",
    "        )\n",
    "        result[\"params\"] = sum(p.numel() for p in real_model.parameters())\n",
    "        result[\"seed\"] = seed\n",
    "        \n",
    "        all_results.append(result)\n",
    "        \n",
    "        # Save preprocessor from first seed\n",
    "        if trained_preprocessor_state is None:\n",
    "            trained_preprocessor_state = real_model.preprocessor.state_dict()\n",
    "            print(f\"\\n  → Saved preprocessor state from seed {seed}\")\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"REALNET SUMMARY (CIFAR-10)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    accs = [r[\"best_acc\"] for r in all_results]\n",
    "    times = [r[\"time\"] for r in all_results]\n",
    "    epochs = [r[\"epochs\"] for r in all_results]\n",
    "    params = all_results[0][\"params\"]\n",
    "    \n",
    "    print(f\"\\nAccuracy:   {np.mean(accs):.4f} ± {np.std(accs):.4f}\")\n",
    "    print(f\"Time:       {np.mean(times):.1f}s ± {np.std(times):.1f}s\")\n",
    "    print(f\"Epochs:     {np.mean(epochs):.1f} ± {np.std(epochs):.1f}\")\n",
    "    print(f\"Parameters: {params:,}\")\n",
    "    \n",
    "    print(\"\\nPer-seed results:\")\n",
    "    for r in all_results:\n",
    "        print(f\"  Seed {r['seed']}: acc={r['best_acc']:.4f}, \"\n",
    "              f\"time={r['time']:.1f}s, epochs={r['epochs']}\")\n",
    "\n",
    "    # Save results and preprocessor\n",
    "    save_dict = {\n",
    "        \"results\": all_results,\n",
    "        \"preprocessor_state\": trained_preprocessor_state,\n",
    "        \"summary\": {\n",
    "            \"mean_acc\": np.mean(accs),\n",
    "            \"std_acc\": np.std(accs),\n",
    "            \"mean_time\": np.mean(times),\n",
    "            \"params\": params\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    torch.save(save_dict, \"realnet_cifar10_results.pt\")\n",
    "    print(f\"\\n✓ Saved results to: realnet_cifar10_results.pt\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11453524-4ec0-4e95-9621-0c6ea601172d",
   "metadata": {},
   "source": [
    "# QuatNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e099062a-f242-4ca1-a8a7-2645b2dedcd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch device: cuda\n",
      "======================================================================\n",
      "BLOCK 2: QuatNet Training (CIFAR-10, Frozen Preprocessor)\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "  • Dataset: CIFAR-10 (32×32 RGB)\n",
      "  • Batch size: 128\n",
      "  • Patience: 10\n",
      "  • Seeds: [42, 123, 456]\n",
      "  • Architecture: [frozen 3072→16] → 4 quats → 16 quats → 10\n",
      "======================================================================\n",
      "\n",
      "Loading frozen preprocessor from Block 1...\n",
      "✓ Loaded preprocessor state from realnet_cifar10_results.pt\n",
      "  Preprocessor frozen with 49,168 params\n",
      "\n",
      "Creating stratified samples...\n",
      "  Sampling took 0.00s\n",
      "  Train samples: 15000 (stratified, 1500 per class)\n",
      "  Test samples:  3000 (stratified, 300 per class)\n",
      "\n",
      "======================================================================\n",
      "SEED 42\n",
      "======================================================================\n",
      "\n",
      "  Training QuatNet (seed=42)...\n",
      "  [Quat] Epoch  1 | loss=2.7936 | test_acc=0.2123 | time=0.9s\n",
      "  [Quat] Epoch  2 | loss=2.0419 | test_acc=0.2943 | time=1.6s\n",
      "  [Quat] Epoch  3 | loss=1.8016 | test_acc=0.3283 | time=2.2s\n",
      "  [Quat] Epoch  4 | loss=1.6987 | test_acc=0.3450 | time=2.9s\n",
      "  [Quat] Epoch  5 | loss=1.6418 | test_acc=0.3553 | time=3.5s\n",
      "  [Quat] Epoch  6 | loss=1.6071 | test_acc=0.3593 | time=4.4s\n",
      "  [Quat] Epoch  7 | loss=1.5841 | test_acc=0.3620 | time=5.8s\n",
      "  [Quat] Epoch  8 | loss=1.5680 | test_acc=0.3623 | time=6.5s\n",
      "  [Quat] Epoch  9 | loss=1.5558 | test_acc=0.3653 | time=7.1s\n",
      "  [Quat] Epoch 10 | loss=1.5464 | test_acc=0.3673 | time=7.9s\n",
      "  [Quat] Epoch 11 | loss=1.5384 | test_acc=0.3650 | time=8.5s\n",
      "  [Quat] Epoch 12 | loss=1.5318 | test_acc=0.3687 | time=9.2s\n",
      "  [Quat] Epoch 13 | loss=1.5258 | test_acc=0.3633 | time=10.6s\n",
      "  [Quat] Epoch 14 | loss=1.5207 | test_acc=0.3657 | time=11.2s\n",
      "  [Quat] Epoch 15 | loss=1.5155 | test_acc=0.3673 | time=11.9s\n",
      "  [Quat] Epoch 16 | loss=1.5111 | test_acc=0.3683 | time=12.6s\n",
      "  [Quat] Epoch 17 | loss=1.5068 | test_acc=0.3720 | time=13.2s\n",
      "  [Quat] Epoch 18 | loss=1.5026 | test_acc=0.3753 | time=13.9s\n",
      "  [Quat] Epoch 19 | loss=1.4992 | test_acc=0.3747 | time=14.6s\n",
      "  [Quat] Epoch 20 | loss=1.4958 | test_acc=0.3747 | time=15.3s\n",
      "  [Quat] Epoch 21 | loss=1.4921 | test_acc=0.3737 | time=15.9s\n",
      "  [Quat] Epoch 22 | loss=1.4894 | test_acc=0.3747 | time=16.6s\n",
      "  [Quat] Epoch 23 | loss=1.4857 | test_acc=0.3747 | time=17.2s\n",
      "  [Quat] Epoch 24 | loss=1.4829 | test_acc=0.3747 | time=17.8s\n",
      "  [Quat] Epoch 25 | loss=1.4805 | test_acc=0.3747 | time=18.5s\n",
      "  [Quat] Epoch 26 | loss=1.4779 | test_acc=0.3733 | time=19.2s\n",
      "  [Quat] Epoch 27 | loss=1.4751 | test_acc=0.3723 | time=19.8s\n",
      "  [Quat] Epoch 28 | loss=1.4723 | test_acc=0.3737 | time=20.4s\n",
      "  [Quat] Early stop at epoch 28 (no improvement for 10 epochs)\n",
      "\n",
      "======================================================================\n",
      "SEED 123\n",
      "======================================================================\n",
      "\n",
      "  Training QuatNet (seed=123)...\n",
      "  [Quat] Epoch  1 | loss=2.6719 | test_acc=0.2157 | time=0.6s\n",
      "  [Quat] Epoch  2 | loss=2.0070 | test_acc=0.2967 | time=1.3s\n",
      "  [Quat] Epoch  3 | loss=1.7866 | test_acc=0.3297 | time=1.9s\n",
      "  [Quat] Epoch  4 | loss=1.6894 | test_acc=0.3457 | time=2.6s\n",
      "  [Quat] Epoch  5 | loss=1.6381 | test_acc=0.3513 | time=3.2s\n",
      "  [Quat] Epoch  6 | loss=1.6071 | test_acc=0.3540 | time=3.8s\n",
      "  [Quat] Epoch  7 | loss=1.5858 | test_acc=0.3590 | time=4.4s\n",
      "  [Quat] Epoch  8 | loss=1.5706 | test_acc=0.3560 | time=5.0s\n",
      "  [Quat] Epoch  9 | loss=1.5585 | test_acc=0.3563 | time=5.7s\n",
      "  [Quat] Epoch 10 | loss=1.5487 | test_acc=0.3613 | time=6.3s\n",
      "  [Quat] Epoch 11 | loss=1.5406 | test_acc=0.3587 | time=6.9s\n",
      "  [Quat] Epoch 12 | loss=1.5335 | test_acc=0.3620 | time=7.6s\n",
      "  [Quat] Epoch 13 | loss=1.5268 | test_acc=0.3653 | time=8.2s\n",
      "  [Quat] Epoch 14 | loss=1.5210 | test_acc=0.3670 | time=8.9s\n",
      "  [Quat] Epoch 15 | loss=1.5154 | test_acc=0.3703 | time=9.6s\n",
      "  [Quat] Epoch 16 | loss=1.5105 | test_acc=0.3683 | time=10.2s\n",
      "  [Quat] Epoch 17 | loss=1.5052 | test_acc=0.3693 | time=10.8s\n",
      "  [Quat] Epoch 18 | loss=1.5010 | test_acc=0.3690 | time=11.5s\n",
      "  [Quat] Epoch 19 | loss=1.4964 | test_acc=0.3697 | time=12.1s\n",
      "  [Quat] Epoch 20 | loss=1.4920 | test_acc=0.3723 | time=12.8s\n",
      "  [Quat] Epoch 21 | loss=1.4883 | test_acc=0.3733 | time=13.4s\n",
      "  [Quat] Epoch 22 | loss=1.4844 | test_acc=0.3740 | time=14.0s\n",
      "  [Quat] Epoch 23 | loss=1.4807 | test_acc=0.3700 | time=14.7s\n",
      "  [Quat] Epoch 24 | loss=1.4769 | test_acc=0.3720 | time=15.3s\n",
      "  [Quat] Epoch 25 | loss=1.4736 | test_acc=0.3730 | time=16.0s\n",
      "  [Quat] Epoch 26 | loss=1.4702 | test_acc=0.3707 | time=16.6s\n",
      "  [Quat] Epoch 27 | loss=1.4670 | test_acc=0.3730 | time=17.2s\n",
      "  [Quat] Epoch 28 | loss=1.4641 | test_acc=0.3733 | time=17.9s\n",
      "  [Quat] Epoch 29 | loss=1.4611 | test_acc=0.3747 | time=18.5s\n",
      "  [Quat] Epoch 30 | loss=1.4582 | test_acc=0.3753 | time=19.1s\n",
      "  [Quat] Epoch 31 | loss=1.4552 | test_acc=0.3753 | time=19.7s\n",
      "  [Quat] Epoch 32 | loss=1.4526 | test_acc=0.3783 | time=20.4s\n",
      "  [Quat] Epoch 33 | loss=1.4503 | test_acc=0.3733 | time=21.0s\n",
      "  [Quat] Epoch 34 | loss=1.4472 | test_acc=0.3750 | time=21.6s\n",
      "  [Quat] Epoch 35 | loss=1.4448 | test_acc=0.3740 | time=23.1s\n",
      "  [Quat] Epoch 36 | loss=1.4428 | test_acc=0.3760 | time=23.7s\n",
      "  [Quat] Epoch 37 | loss=1.4402 | test_acc=0.3753 | time=24.4s\n",
      "  [Quat] Epoch 38 | loss=1.4378 | test_acc=0.3753 | time=25.0s\n",
      "  [Quat] Epoch 39 | loss=1.4356 | test_acc=0.3790 | time=25.7s\n",
      "  [Quat] Epoch 40 | loss=1.4338 | test_acc=0.3750 | time=26.3s\n",
      "\n",
      "======================================================================\n",
      "SEED 456\n",
      "======================================================================\n",
      "\n",
      "  Training QuatNet (seed=456)...\n",
      "  [Quat] Epoch  1 | loss=2.6525 | test_acc=0.2293 | time=0.7s\n",
      "  [Quat] Epoch  2 | loss=2.0023 | test_acc=0.2990 | time=1.3s\n",
      "  [Quat] Epoch  3 | loss=1.7775 | test_acc=0.3273 | time=2.0s\n",
      "  [Quat] Epoch  4 | loss=1.6812 | test_acc=0.3447 | time=2.6s\n",
      "  [Quat] Epoch  5 | loss=1.6313 | test_acc=0.3483 | time=3.3s\n",
      "  [Quat] Epoch  6 | loss=1.6015 | test_acc=0.3600 | time=4.0s\n",
      "  [Quat] Epoch  7 | loss=1.5811 | test_acc=0.3610 | time=4.6s\n",
      "  [Quat] Epoch  8 | loss=1.5662 | test_acc=0.3687 | time=5.3s\n",
      "  [Quat] Epoch  9 | loss=1.5547 | test_acc=0.3707 | time=5.9s\n",
      "  [Quat] Epoch 10 | loss=1.5454 | test_acc=0.3677 | time=6.6s\n",
      "  [Quat] Epoch 11 | loss=1.5379 | test_acc=0.3690 | time=7.3s\n",
      "  [Quat] Epoch 12 | loss=1.5304 | test_acc=0.3687 | time=8.0s\n",
      "  [Quat] Epoch 13 | loss=1.5240 | test_acc=0.3723 | time=8.6s\n",
      "  [Quat] Epoch 14 | loss=1.5187 | test_acc=0.3713 | time=9.3s\n",
      "  [Quat] Epoch 15 | loss=1.5134 | test_acc=0.3743 | time=9.9s\n",
      "  [Quat] Epoch 16 | loss=1.5088 | test_acc=0.3700 | time=10.5s\n",
      "  [Quat] Epoch 17 | loss=1.5041 | test_acc=0.3733 | time=11.2s\n",
      "  [Quat] Epoch 18 | loss=1.5002 | test_acc=0.3747 | time=11.8s\n",
      "  [Quat] Epoch 19 | loss=1.4963 | test_acc=0.3743 | time=12.5s\n",
      "  [Quat] Epoch 20 | loss=1.4929 | test_acc=0.3710 | time=13.1s\n",
      "  [Quat] Epoch 21 | loss=1.4892 | test_acc=0.3730 | time=13.7s\n",
      "  [Quat] Epoch 22 | loss=1.4863 | test_acc=0.3727 | time=14.4s\n",
      "  [Quat] Epoch 23 | loss=1.4831 | test_acc=0.3767 | time=15.1s\n",
      "  [Quat] Epoch 24 | loss=1.4804 | test_acc=0.3757 | time=15.7s\n",
      "  [Quat] Epoch 25 | loss=1.4777 | test_acc=0.3753 | time=16.3s\n",
      "  [Quat] Epoch 26 | loss=1.4749 | test_acc=0.3767 | time=16.9s\n",
      "  [Quat] Epoch 27 | loss=1.4724 | test_acc=0.3770 | time=17.5s\n",
      "  [Quat] Epoch 28 | loss=1.4700 | test_acc=0.3793 | time=18.1s\n",
      "  [Quat] Epoch 29 | loss=1.4676 | test_acc=0.3790 | time=18.8s\n",
      "  [Quat] Epoch 30 | loss=1.4654 | test_acc=0.3797 | time=19.4s\n",
      "  [Quat] Epoch 31 | loss=1.4630 | test_acc=0.3763 | time=20.1s\n",
      "  [Quat] Epoch 32 | loss=1.4611 | test_acc=0.3733 | time=20.8s\n",
      "  [Quat] Epoch 33 | loss=1.4589 | test_acc=0.3790 | time=21.5s\n",
      "  [Quat] Epoch 34 | loss=1.4570 | test_acc=0.3797 | time=22.1s\n",
      "  [Quat] Epoch 35 | loss=1.4551 | test_acc=0.3797 | time=22.8s\n",
      "  [Quat] Epoch 36 | loss=1.4534 | test_acc=0.3770 | time=23.4s\n",
      "  [Quat] Epoch 37 | loss=1.4516 | test_acc=0.3793 | time=24.1s\n",
      "  [Quat] Epoch 38 | loss=1.4496 | test_acc=0.3733 | time=24.7s\n",
      "  [Quat] Epoch 39 | loss=1.4481 | test_acc=0.3833 | time=25.4s\n",
      "  [Quat] Epoch 40 | loss=1.4466 | test_acc=0.3783 | time=26.1s\n",
      "\n",
      "======================================================================\n",
      "QUATNET SUMMARY (CIFAR-10)\n",
      "======================================================================\n",
      "\n",
      "Accuracy:   0.3792 ± 0.0033\n",
      "Time:       24.3s ± 2.7s\n",
      "Epochs:     36.0 ± 5.7\n",
      "Parameters: 1,000 trainable (head), 50,168 total\n",
      "\n",
      "Per-seed results:\n",
      "  Seed 42: acc=0.3753, time=20.4s, epochs=28\n",
      "  Seed 123: acc=0.3790, time=26.3s, epochs=40\n",
      "  Seed 456: acc=0.3833, time=26.1s, epochs=40\n",
      "\n",
      "✓ Saved results to: quatnet_cifar10_results.pt\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Block 2: QuatNet Training (CIFAR-10)\n",
    "=====================================\n",
    "Loads frozen preprocessor from Block 1 and trains quaternion head on 3 seeds.\n",
    "\n",
    "Requirements:\n",
    "- realnet_cifar10_results.pt (from Block 1)\n",
    "\n",
    "Outputs:\n",
    "- quatnet_cifar10_results.pt: Contains quaternion head results\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "print(f\"Using PyTorch device: {device}\")\n",
    "\n",
    "# ============================================\n",
    "# Comprehensive seed setting for reproducibility\n",
    "# ============================================\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    \"\"\"Set seeds for all RNG sources for reproducibility\"\"\"\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    # Python\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # NumPy\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # PyTorch deterministic mode\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # CuPy (if available)\n",
    "    try:\n",
    "        import cupy as cp\n",
    "        cp.random.seed(seed)\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    # PennyLane (if available)\n",
    "    try:\n",
    "        import pennylane as qml\n",
    "        qml.numpy.random.seed(seed)\n",
    "    except (ImportError, AttributeError):\n",
    "        pass\n",
    "\n",
    "# ============================================\n",
    "# Quaternion utilities (PyTorch tensors)\n",
    "# ============================================\n",
    "\n",
    "def q_normalize(q):\n",
    "    norm = torch.linalg.norm(q, dim=-1, keepdim=True) + 1e-8\n",
    "    return q / norm\n",
    "\n",
    "def q_conj(q):\n",
    "    w, x, y, z = torch.unbind(q, dim=-1)\n",
    "    return torch.stack([w, -x, -y, -z], dim=-1)\n",
    "\n",
    "def q_mul(a, b):\n",
    "    \"\"\"Hamilton product of two quaternions\"\"\"\n",
    "    aw, ax, ay, az = torch.unbind(a, dim=-1)\n",
    "    bw, bx, by, bz = torch.unbind(b, dim=-1)\n",
    "\n",
    "    w = aw * bw - ax * bx - ay * by - az * bz\n",
    "    x = aw * bx + ax * bw + ay * bz - az * by\n",
    "    y = aw * by - ax * bz + ay * bw + az * bx\n",
    "    z = aw * bz + ax * by - ay * bx + az * bw\n",
    "\n",
    "    return torch.stack([w, x, y, z], dim=-1)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Shared Preprocessor (must match Block 1 - CIFAR-10)\n",
    "# ============================================\n",
    "\n",
    "class SharedPreprocessor(nn.Module):\n",
    "    \"\"\"Shared classical feature extractor: 3072 → 16\"\"\"\n",
    "    def __init__(self, input_dim=3072, bottleneck_dim=16):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, bottleneck_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten to (batch, 3072)\n",
    "        x = torch.tanh(self.fc(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Quaternion Head and Network\n",
    "# ============================================\n",
    "\n",
    "class QuaternionLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        \"\"\"\n",
    "        in_features, out_features are in \"quaternion units\".\n",
    "        Internally weight: (out_features, in_features, 4)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.empty(out_features, in_features, 4))\n",
    "        self.bias = nn.Parameter(torch.empty(out_features, 4))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.normal_(self.weight, mean=0.0, std=0.1)\n",
    "        with torch.no_grad():\n",
    "            self.weight[:] = q_normalize(self.weight)\n",
    "            nn.init.constant_(self.bias[..., 0], 1.0)\n",
    "            nn.init.constant_(self.bias[..., 1:], 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, in_features, 4)\n",
    "        Returns: (B, out_features, 4)\n",
    "        \"\"\"\n",
    "        w = self.weight.unsqueeze(0)\n",
    "        x_exp = x.unsqueeze(1)\n",
    "        prod = q_mul(w, x_exp)\n",
    "        out = prod.sum(dim=2) + self.bias\n",
    "        return out\n",
    "\n",
    "\n",
    "class QuatHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Quaternion head: 4 quats → 16 quats → 10 quats → 10 logits.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.quat_fc1 = QuaternionLinear(4, 16)\n",
    "        self.quat_fc2 = QuaternionLinear(16, num_classes)\n",
    "\n",
    "    def real_to_quat(self, x):\n",
    "        \"\"\"Convert 16 real features to 4 quaternions\"\"\"\n",
    "        B = x.size(0)\n",
    "        return x.view(B, 4, 4)\n",
    "\n",
    "    def quat_to_real(self, q):\n",
    "        \"\"\"Extract real part of quaternions for classification\"\"\"\n",
    "        return q[..., 0]\n",
    "\n",
    "    def forward(self, x):\n",
    "        q_in = self.real_to_quat(x)\n",
    "        hq = self.quat_fc1(q_in)\n",
    "        hq = q_normalize(hq)\n",
    "        hq = torch.tanh(hq)\n",
    "        q_out = self.quat_fc2(hq)\n",
    "        logits = self.quat_to_real(q_out)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class QuatNet(nn.Module):\n",
    "    \"\"\"Complete Quaternion network: Preprocessor + QuatHead\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.preprocessor = SharedPreprocessor(3072, 16)\n",
    "        self.head = QuatHead(10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.preprocessor(x)\n",
    "        return self.head(features)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Training and Evaluation\n",
    "# ============================================\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, device, show_progress=False):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        total_samples += x.size(0)\n",
    "\n",
    "        if show_progress and batch_idx % 50 == 0:\n",
    "            print(f\"    Batch {batch_idx}/{len(loader)}, samples: {total_samples}\", end=\"\\r\")\n",
    "\n",
    "    if show_progress:\n",
    "        print()\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += x.size(0)\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "def train_with_early_stopping(model, train_loader, test_loader, optimizer,\n",
    "                              device, max_epochs=40, patience=10, name=\"Model\"):\n",
    "    if isinstance(device, str):\n",
    "        device = torch.device(device)\n",
    "\n",
    "    best_acc = 0.0\n",
    "    epochs_without_improvement = 0\n",
    "    start = time.time()\n",
    "    last_acc = 0.0\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        loss = train_one_epoch(model, train_loader, optimizer, device, show_progress=False)\n",
    "        acc = evaluate(model, test_loader, device)\n",
    "        last_acc = acc\n",
    "\n",
    "        elapsed = time.time() - start\n",
    "        print(f\"  [{name}] Epoch {epoch:2d} | loss={loss:.4f} \"\n",
    "              f\"| test_acc={acc:.4f} | time={elapsed:.1f}s\")\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"  [{name}] Early stop at epoch {epoch} \"\n",
    "                  f\"(no improvement for {patience} epochs)\")\n",
    "            break\n",
    "\n",
    "    total_time = time.time() - start\n",
    "    return {\n",
    "        \"best_acc\": best_acc,\n",
    "        \"final_acc\": last_acc,\n",
    "        \"time\": total_time,\n",
    "        \"epochs\": epoch,\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Data utilities\n",
    "# ============================================\n",
    "\n",
    "def stratified_sample_from_targets(dataset, n_samples_per_class, seed=42):\n",
    "    \"\"\"\n",
    "    Create a stratified sample with n_samples_per_class from each class.\n",
    "    Uses dataset.targets directly - NO image loading during sampling.\n",
    "    \"\"\"\n",
    "    # CIFAR-10 exposes targets as a list\n",
    "    if hasattr(dataset, 'targets'):\n",
    "        targets = np.array(dataset.targets)\n",
    "    else:\n",
    "        # Fallback for wrapped datasets\n",
    "        targets = np.array([dataset[i][1] for i in range(len(dataset))])\n",
    "    \n",
    "    rng = np.random.RandomState(seed)\n",
    "    \n",
    "    sampled_indices = []\n",
    "    for c in range(10):\n",
    "        # Find all indices for class c\n",
    "        idx_c = np.where(targets == c)[0]\n",
    "        k = min(n_samples_per_class, len(idx_c))\n",
    "        # Sample without replacement\n",
    "        selected = rng.choice(idx_c, size=k, replace=False).tolist()\n",
    "        sampled_indices.extend(selected)\n",
    "    \n",
    "    # Shuffle the combined indices\n",
    "    rng.shuffle(sampled_indices)\n",
    "    return sampled_indices\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Main\n",
    "# ============================================\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"BLOCK 2: QuatNet Training (CIFAR-10, Frozen Preprocessor)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nConfiguration:\")\n",
    "    print(\"  • Dataset: CIFAR-10 (32×32 RGB)\")\n",
    "    print(\"  • Batch size: 128\")\n",
    "    print(\"  • Patience: 10\")\n",
    "    print(\"  • Seeds: [42, 123, 456]\")\n",
    "    print(\"  • Architecture: [frozen 3072→16] → 4 quats → 16 quats → 10\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Load frozen preprocessor from Block 1\n",
    "    print(\"\\nLoading frozen preprocessor from Block 1...\")\n",
    "    try:\n",
    "        realnet_data = torch.load(\"realnet_cifar10_results.pt\", weights_only=False)\n",
    "        preprocessor_state = realnet_data[\"preprocessor_state\"]\n",
    "        print(\"✓ Loaded preprocessor state from realnet_cifar10_results.pt\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"✗ ERROR: realnet_cifar10_results.pt not found!\")\n",
    "        print(\"  Run Block 1 first to train RealNet and save preprocessor.\")\n",
    "        return\n",
    "\n",
    "    # Create frozen preprocessor\n",
    "    shared_preprocessor = SharedPreprocessor(3072, 16).to(device)\n",
    "    shared_preprocessor.load_state_dict(preprocessor_state)\n",
    "    for p in shared_preprocessor.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    preprocessor_params = sum(p.numel() for p in shared_preprocessor.parameters())\n",
    "    print(f\"  Preprocessor frozen with {preprocessor_params:,} params\")\n",
    "\n",
    "    # Load data - CIFAR-10 normalization\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.4914, 0.4822, 0.4465],\n",
    "            std=[0.2470, 0.2435, 0.2616]\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    full_train_ds = datasets.CIFAR10(root=\"./data\", train=True,\n",
    "                                     download=True, transform=transform)\n",
    "    full_test_ds = datasets.CIFAR10(root=\"./data\", train=False,\n",
    "                                    download=True, transform=transform)\n",
    "\n",
    "    # Create stratified samples (fast - uses targets only, no image loading)\n",
    "    print(\"\\nCreating stratified samples...\")\n",
    "    t0 = time.time()\n",
    "    train_indices = stratified_sample_from_targets(full_train_ds, n_samples_per_class=1500, seed=42)\n",
    "    test_indices = stratified_sample_from_targets(full_test_ds, n_samples_per_class=300, seed=42)\n",
    "    print(f\"  Sampling took {time.time()-t0:.2f}s\")\n",
    "    \n",
    "    train_ds = Subset(full_train_ds, train_indices)\n",
    "    test_ds = Subset(full_test_ds, test_indices)\n",
    "    \n",
    "    print(f\"  Train samples: {len(train_ds)} (stratified, 1500 per class)\")\n",
    "    print(f\"  Test samples:  {len(test_ds)} (stratified, 300 per class)\")\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=128,\n",
    "                              shuffle=True, num_workers=4)\n",
    "    test_loader = DataLoader(test_ds, batch_size=256,\n",
    "                             shuffle=False, num_workers=4)\n",
    "\n",
    "    seeds = [42, 123, 456]\n",
    "    all_results = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(f\"SEED {seed}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        set_all_seeds(seed)\n",
    "\n",
    "        print(f\"\\n  Training QuatNet (seed={seed})...\")\n",
    "        quat_model = QuatNet().to(device)\n",
    "        quat_model.preprocessor = shared_preprocessor  # Use frozen preprocessor\n",
    "        \n",
    "        quat_opt = torch.optim.Adam(quat_model.head.parameters(), lr=1e-3)\n",
    "        \n",
    "        result = train_with_early_stopping(\n",
    "            quat_model, train_loader, test_loader, quat_opt, device,\n",
    "            max_epochs=40, patience=10, name=\"Quat\"\n",
    "        )\n",
    "        \n",
    "        result[\"trainable_params\"] = sum(p.numel() for p in quat_model.parameters()\n",
    "                                         if p.requires_grad)\n",
    "        result[\"total_params\"] = sum(p.numel() for p in quat_model.parameters())\n",
    "        result[\"seed\"] = seed\n",
    "        \n",
    "        all_results.append(result)\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"QUATNET SUMMARY (CIFAR-10)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    accs = [r[\"best_acc\"] for r in all_results]\n",
    "    times = [r[\"time\"] for r in all_results]\n",
    "    epochs = [r[\"epochs\"] for r in all_results]\n",
    "    trainable = all_results[0][\"trainable_params\"]\n",
    "    total = all_results[0][\"total_params\"]\n",
    "    \n",
    "    print(f\"\\nAccuracy:   {np.mean(accs):.4f} ± {np.std(accs):.4f}\")\n",
    "    print(f\"Time:       {np.mean(times):.1f}s ± {np.std(times):.1f}s\")\n",
    "    print(f\"Epochs:     {np.mean(epochs):.1f} ± {np.std(epochs):.1f}\")\n",
    "    print(f\"Parameters: {trainable:,} trainable (head), {total:,} total\")\n",
    "    \n",
    "    print(\"\\nPer-seed results:\")\n",
    "    for r in all_results:\n",
    "        print(f\"  Seed {r['seed']}: acc={r['best_acc']:.4f}, \"\n",
    "              f\"time={r['time']:.1f}s, epochs={r['epochs']}\")\n",
    "\n",
    "    # Save results\n",
    "    save_dict = {\n",
    "        \"results\": all_results,\n",
    "        \"summary\": {\n",
    "            \"mean_acc\": np.mean(accs),\n",
    "            \"std_acc\": np.std(accs),\n",
    "            \"mean_time\": np.mean(times),\n",
    "            \"trainable_params\": trainable,\n",
    "            \"total_params\": total\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    torch.save(save_dict, \"quatnet_cifar10_results.pt\")\n",
    "    print(f\"\\n✓ Saved results to: quatnet_cifar10_results.pt\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c266c5-3d08-4449-9f9d-905366d51b71",
   "metadata": {},
   "source": [
    "# Quant No Ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3848f591-dc2d-4283-a3cb-72f381cef549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using lightning.gpu device\n",
      "Using PyTorch device: cuda\n",
      "======================================================================\n",
      "BLOCK 3: Quantum (NO Entanglement) Training (CIFAR-10)\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "  • Dataset: CIFAR-10 (32×32 RGB)\n",
      "  • Batch size: 32\n",
      "  • Patience: 10\n",
      "  • Max epochs: 200\n",
      "  • Seeds: [42, 123, 456]\n",
      "  • Architecture: [frozen 3072→16] → 4 qubits (3 layers, NO ent) → 10\n",
      "  • Quantum device: lightning.gpu\n",
      "======================================================================\n",
      "\n",
      "Loading frozen preprocessor from Block 1...\n",
      "✓ Loaded preprocessor state from realnet_cifar10_results.pt\n",
      "  Preprocessor frozen with 49,168 params\n",
      "\n",
      "Creating stratified samples...\n",
      "  Sampling took 0.00s\n",
      "  Train samples: 15000 (stratified, 1500 per class)\n",
      "  Test samples:  3000 (stratified, 300 per class)\n",
      "\n",
      "======================================================================\n",
      "SEED 42\n",
      "======================================================================\n",
      "\n",
      "  Training QuantumNet (NO entanglement, seed=42)...\n",
      "  [QuantumNoEnt] Epoch 1/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  1 | loss=2.2121 | test_acc=0.2337 | time=457.3s\n",
      "  [QuantumNoEnt] Epoch 2/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  2 | loss=1.9542 | test_acc=0.2880 | time=928.3s\n",
      "  [QuantumNoEnt] Epoch 3/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  3 | loss=1.8018 | test_acc=0.3173 | time=1406.1s\n",
      "  [QuantumNoEnt] Epoch 4/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  4 | loss=1.7247 | test_acc=0.3257 | time=1880.4s\n",
      "  [QuantumNoEnt] Epoch 5/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  5 | loss=1.6892 | test_acc=0.3383 | time=2349.2s\n",
      "  [QuantumNoEnt] Epoch 6/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  6 | loss=1.6711 | test_acc=0.3403 | time=2832.2s\n",
      "  [QuantumNoEnt] Epoch 7/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  7 | loss=1.6590 | test_acc=0.3400 | time=3304.8s\n",
      "  [QuantumNoEnt] Epoch 8/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  8 | loss=1.6520 | test_acc=0.3357 | time=3772.6s\n",
      "  [QuantumNoEnt] Epoch 9/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  9 | loss=1.6459 | test_acc=0.3370 | time=4252.2s\n",
      "  [QuantumNoEnt] Epoch 10/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 10 | loss=1.6410 | test_acc=0.3360 | time=4735.2s\n",
      "  [QuantumNoEnt] Epoch 11/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 11 | loss=1.6382 | test_acc=0.3407 | time=5217.7s\n",
      "  [QuantumNoEnt] Epoch 12/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 12 | loss=1.6349 | test_acc=0.3383 | time=5707.8s\n",
      "  [QuantumNoEnt] Epoch 13/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 13 | loss=1.6331 | test_acc=0.3357 | time=6181.1s\n",
      "  [QuantumNoEnt] Epoch 14/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 14 | loss=1.6313 | test_acc=0.3387 | time=6659.1s\n",
      "  [QuantumNoEnt] Epoch 15/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 15 | loss=1.6295 | test_acc=0.3400 | time=7132.1s\n",
      "  [QuantumNoEnt] Epoch 16/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 16 | loss=1.6281 | test_acc=0.3410 | time=7608.8s\n",
      "  [QuantumNoEnt] Epoch 17/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 17 | loss=1.6279 | test_acc=0.3390 | time=8059.6s\n",
      "  [QuantumNoEnt] Epoch 18/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 18 | loss=1.6262 | test_acc=0.3377 | time=8526.9s\n",
      "  [QuantumNoEnt] Epoch 19/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 19 | loss=1.6256 | test_acc=0.3360 | time=8997.9s\n",
      "  [QuantumNoEnt] Epoch 20/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 20 | loss=1.6242 | test_acc=0.3397 | time=9466.6s\n",
      "  [QuantumNoEnt] Epoch 21/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 21 | loss=1.6238 | test_acc=0.3420 | time=9944.6s\n",
      "  [QuantumNoEnt] Epoch 22/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 22 | loss=1.6237 | test_acc=0.3393 | time=10436.0s\n",
      "  [QuantumNoEnt] Epoch 23/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 23 | loss=1.6226 | test_acc=0.3397 | time=10907.5s\n",
      "  [QuantumNoEnt] Epoch 24/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 24 | loss=1.6223 | test_acc=0.3360 | time=11389.5s\n",
      "  [QuantumNoEnt] Epoch 25/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 25 | loss=1.6218 | test_acc=0.3360 | time=11876.2s\n",
      "  [QuantumNoEnt] Epoch 26/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 26 | loss=1.6207 | test_acc=0.3367 | time=12356.1s\n",
      "  [QuantumNoEnt] Epoch 27/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 27 | loss=1.6198 | test_acc=0.3383 | time=12816.6s\n",
      "  [QuantumNoEnt] Epoch 28/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 28 | loss=1.6202 | test_acc=0.3373 | time=13290.1s\n",
      "  [QuantumNoEnt] Epoch 29/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 29 | loss=1.6197 | test_acc=0.3347 | time=13777.3s\n",
      "  [QuantumNoEnt] Epoch 30/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 30 | loss=1.6191 | test_acc=0.3367 | time=14253.1s\n",
      "  [QuantumNoEnt] Epoch 31/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 31 | loss=1.6184 | test_acc=0.3403 | time=14736.7s\n",
      "  [QuantumNoEnt] Early stop at epoch 31 (no improvement for 10 epochs)\n",
      "\n",
      "======================================================================\n",
      "SEED 123\n",
      "======================================================================\n",
      "\n",
      "  Training QuantumNet (NO entanglement, seed=123)...\n",
      "  [QuantumNoEnt] Epoch 1/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  1 | loss=2.1673 | test_acc=0.2437 | time=468.2s\n",
      "  [QuantumNoEnt] Epoch 2/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  2 | loss=1.9087 | test_acc=0.2820 | time=920.8s\n",
      "  [QuantumNoEnt] Epoch 3/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  3 | loss=1.8104 | test_acc=0.3140 | time=1412.8s\n",
      "  [QuantumNoEnt] Epoch 4/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  4 | loss=1.7525 | test_acc=0.3247 | time=1890.5s\n",
      "  [QuantumNoEnt] Epoch 5/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  5 | loss=1.7127 | test_acc=0.3250 | time=2371.0s\n",
      "  [QuantumNoEnt] Epoch 6/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  6 | loss=1.6875 | test_acc=0.3353 | time=2865.5s\n",
      "  [QuantumNoEnt] Epoch 7/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  7 | loss=1.6707 | test_acc=0.3343 | time=3343.4s\n",
      "  [QuantumNoEnt] Epoch 8/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  8 | loss=1.6590 | test_acc=0.3323 | time=3830.3s\n",
      "  [QuantumNoEnt] Epoch 9/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  9 | loss=1.6502 | test_acc=0.3343 | time=4296.4s\n",
      "  [QuantumNoEnt] Epoch 10/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 10 | loss=1.6427 | test_acc=0.3393 | time=4776.7s\n",
      "  [QuantumNoEnt] Epoch 11/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 11 | loss=1.6355 | test_acc=0.3393 | time=5237.5s\n",
      "  [QuantumNoEnt] Epoch 12/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 12 | loss=1.6299 | test_acc=0.3393 | time=5747.6s\n",
      "  [QuantumNoEnt] Epoch 13/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 13 | loss=1.6242 | test_acc=0.3423 | time=6231.8s\n",
      "  [QuantumNoEnt] Epoch 14/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 14 | loss=1.6192 | test_acc=0.3397 | time=6695.4s\n",
      "  [QuantumNoEnt] Epoch 15/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 15 | loss=1.6153 | test_acc=0.3437 | time=7187.2s\n",
      "  [QuantumNoEnt] Epoch 16/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 16 | loss=1.6140 | test_acc=0.3410 | time=7663.5s\n",
      "  [QuantumNoEnt] Epoch 17/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 17 | loss=1.6113 | test_acc=0.3420 | time=8126.7s\n",
      "  [QuantumNoEnt] Epoch 18/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 18 | loss=1.6082 | test_acc=0.3477 | time=8604.6s\n",
      "  [QuantumNoEnt] Epoch 19/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 19 | loss=1.6065 | test_acc=0.3420 | time=9087.4s\n",
      "  [QuantumNoEnt] Epoch 20/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 20 | loss=1.6062 | test_acc=0.3460 | time=9576.6s\n",
      "  [QuantumNoEnt] Epoch 21/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 21 | loss=1.6039 | test_acc=0.3453 | time=10080.8s\n",
      "  [QuantumNoEnt] Epoch 22/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 22 | loss=1.6015 | test_acc=0.3447 | time=10558.9s\n",
      "  [QuantumNoEnt] Epoch 23/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 23 | loss=1.6010 | test_acc=0.3433 | time=11019.9s\n",
      "  [QuantumNoEnt] Epoch 24/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 24 | loss=1.5999 | test_acc=0.3447 | time=11506.2s\n",
      "  [QuantumNoEnt] Epoch 25/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 25 | loss=1.5996 | test_acc=0.3517 | time=11984.6s\n",
      "  [QuantumNoEnt] Epoch 26/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 26 | loss=1.5988 | test_acc=0.3433 | time=12443.5s\n",
      "  [QuantumNoEnt] Epoch 27/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 27 | loss=1.5975 | test_acc=0.3470 | time=12947.1s\n",
      "  [QuantumNoEnt] Epoch 28/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 28 | loss=1.5968 | test_acc=0.3503 | time=13439.0s\n",
      "  [QuantumNoEnt] Epoch 29/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 29 | loss=1.5959 | test_acc=0.3473 | time=13916.4s\n",
      "  [QuantumNoEnt] Epoch 30/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 30 | loss=1.5952 | test_acc=0.3463 | time=14410.4s\n",
      "  [QuantumNoEnt] Epoch 31/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 31 | loss=1.5949 | test_acc=0.3517 | time=14874.4s\n",
      "  [QuantumNoEnt] Epoch 32/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 32 | loss=1.5946 | test_acc=0.3477 | time=15340.2s\n",
      "  [QuantumNoEnt] Epoch 33/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 33 | loss=1.5944 | test_acc=0.3480 | time=15854.3s\n",
      "  [QuantumNoEnt] Epoch 34/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 34 | loss=1.5931 | test_acc=0.3507 | time=16323.9s\n",
      "  [QuantumNoEnt] Epoch 35/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 35 | loss=1.5937 | test_acc=0.3490 | time=16792.4s\n",
      "  [QuantumNoEnt] Early stop at epoch 35 (no improvement for 10 epochs)\n",
      "\n",
      "======================================================================\n",
      "SEED 456\n",
      "======================================================================\n",
      "\n",
      "  Training QuantumNet (NO entanglement, seed=456)...\n",
      "  [QuantumNoEnt] Epoch 1/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  1 | loss=2.2020 | test_acc=0.2723 | time=498.3s\n",
      "  [QuantumNoEnt] Epoch 2/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  2 | loss=1.8691 | test_acc=0.3410 | time=981.0s\n",
      "  [QuantumNoEnt] Epoch 3/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  3 | loss=1.7320 | test_acc=0.3383 | time=1466.4s\n",
      "  [QuantumNoEnt] Epoch 4/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  4 | loss=1.6882 | test_acc=0.3413 | time=1968.8s\n",
      "  [QuantumNoEnt] Epoch 5/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  5 | loss=1.6676 | test_acc=0.3450 | time=2426.2s\n",
      "  [QuantumNoEnt] Epoch 6/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  6 | loss=1.6555 | test_acc=0.3490 | time=2902.7s\n",
      "  [QuantumNoEnt] Epoch 7/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  7 | loss=1.6467 | test_acc=0.3443 | time=3397.6s\n",
      "  [QuantumNoEnt] Epoch 8/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  8 | loss=1.6397 | test_acc=0.3557 | time=3869.3s\n",
      "  [QuantumNoEnt] Epoch 9/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  9 | loss=1.6344 | test_acc=0.3480 | time=4343.8s\n",
      "  [QuantumNoEnt] Epoch 10/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 10 | loss=1.6310 | test_acc=0.3563 | time=4841.1s\n",
      "  [QuantumNoEnt] Epoch 11/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 11 | loss=1.6273 | test_acc=0.3573 | time=5320.5s\n",
      "  [QuantumNoEnt] Epoch 12/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 12 | loss=1.6238 | test_acc=0.3580 | time=5813.8s\n",
      "  [QuantumNoEnt] Epoch 13/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 13 | loss=1.6214 | test_acc=0.3580 | time=6306.8s\n",
      "  [QuantumNoEnt] Epoch 14/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 14 | loss=1.6187 | test_acc=0.3517 | time=6794.9s\n",
      "  [QuantumNoEnt] Epoch 15/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 15 | loss=1.6177 | test_acc=0.3580 | time=7274.7s\n",
      "  [QuantumNoEnt] Epoch 16/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 16 | loss=1.6156 | test_acc=0.3520 | time=7762.9s\n",
      "  [QuantumNoEnt] Epoch 17/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 17 | loss=1.6147 | test_acc=0.3553 | time=8241.3s\n",
      "  [QuantumNoEnt] Epoch 18/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 18 | loss=1.6128 | test_acc=0.3543 | time=8723.2s\n",
      "  [QuantumNoEnt] Epoch 19/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 19 | loss=1.6116 | test_acc=0.3577 | time=9199.4s\n",
      "  [QuantumNoEnt] Epoch 20/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 20 | loss=1.6111 | test_acc=0.3553 | time=9678.2s\n",
      "  [QuantumNoEnt] Epoch 21/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 21 | loss=1.6100 | test_acc=0.3593 | time=10169.6s\n",
      "  [QuantumNoEnt] Epoch 22/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 22 | loss=1.6085 | test_acc=0.3533 | time=10657.3s\n",
      "  [QuantumNoEnt] Epoch 23/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 23 | loss=1.6079 | test_acc=0.3537 | time=11132.0s\n",
      "  [QuantumNoEnt] Epoch 24/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 24 | loss=1.6083 | test_acc=0.3500 | time=11630.6s\n",
      "  [QuantumNoEnt] Epoch 25/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 25 | loss=1.6075 | test_acc=0.3560 | time=12103.6s\n",
      "  [QuantumNoEnt] Epoch 26/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 26 | loss=1.6058 | test_acc=0.3540 | time=12570.7s\n",
      "  [QuantumNoEnt] Epoch 27/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 27 | loss=1.6062 | test_acc=0.3580 | time=13056.8s\n",
      "  [QuantumNoEnt] Epoch 28/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 28 | loss=1.6054 | test_acc=0.3573 | time=13541.4s\n",
      "  [QuantumNoEnt] Epoch 29/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 29 | loss=1.6051 | test_acc=0.3570 | time=14020.4s\n",
      "  [QuantumNoEnt] Epoch 30/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 30 | loss=1.6037 | test_acc=0.3547 | time=14501.0s\n",
      "  [QuantumNoEnt] Epoch 31/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 31 | loss=1.6031 | test_acc=0.3560 | time=14982.3s\n",
      "  [QuantumNoEnt] Early stop at epoch 31 (no improvement for 10 epochs)\n",
      "\n",
      "======================================================================\n",
      "QUANTUM (NO ENTANGLEMENT) SUMMARY (CIFAR-10)\n",
      "======================================================================\n",
      "\n",
      "Accuracy:   0.3510 ± 0.0071\n",
      "Time:       15503.8s ± 916.7s\n",
      "Epochs:     32.3 ± 1.9\n",
      "Parameters: 162 trainable (head), 49,330 total\n",
      "\n",
      "Per-seed results:\n",
      "  Seed 42: acc=0.3420, time=14736.7s, epochs=31\n",
      "  Seed 123: acc=0.3517, time=16792.4s, epochs=35\n",
      "  Seed 456: acc=0.3593, time=14982.3s, epochs=31\n",
      "\n",
      "✓ Saved results to: quantum_noent_cifar10_results.pt\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Block 3: Quantum (No Entanglement) Training (CIFAR-10)\n",
    "========================================================\n",
    "Loads frozen preprocessor from Block 1 and trains quantum head WITHOUT entanglement.\n",
    "Uses Lightning-GPU acceleration.\n",
    "\n",
    "Requirements:\n",
    "- realnet_cifar10_results.pt (from Block 1)\n",
    "- pennylane, pennylane-lightning-gpu\n",
    "\n",
    "Outputs:\n",
    "- quantum_noent_cifar10_results.pt: Contains quantum (no ent) results\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "try:\n",
    "    import pennylane as qml\n",
    "    QUANTUM_DEVICE = \"lightning.gpu\"\n",
    "    PENNYLANE_AVAILABLE = True\n",
    "    print(\"✓ Using lightning.gpu device\")\n",
    "except ImportError:\n",
    "    PENNYLANE_AVAILABLE = False\n",
    "    print(\"✗ PennyLane not installed\")\n",
    "    exit(1)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "print(f\"Using PyTorch device: {device}\")\n",
    "\n",
    "# ============================================\n",
    "# Comprehensive seed setting for reproducibility\n",
    "# ============================================\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    \"\"\"Set seeds for all RNG sources for reproducibility\"\"\"\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    # Python\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # NumPy\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # PyTorch deterministic mode\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # CuPy (if available)\n",
    "    try:\n",
    "        import cupy as cp\n",
    "        cp.random.seed(seed)\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    # PennyLane (if available)\n",
    "    try:\n",
    "        import pennylane as qml\n",
    "        qml.numpy.random.seed(seed)\n",
    "    except (ImportError, AttributeError):\n",
    "        pass\n",
    "\n",
    "# ============================================\n",
    "# Shared Preprocessor (must match Block 1 - CIFAR-10)\n",
    "# ============================================\n",
    "\n",
    "class SharedPreprocessor(nn.Module):\n",
    "    \"\"\"Shared classical feature extractor: 3072 → 16\"\"\"\n",
    "    def __init__(self, input_dim=3072, bottleneck_dim=16):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, bottleneck_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten to (batch, 3072)\n",
    "        x = torch.tanh(self.fc(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Quantum Head (No Entanglement)\n",
    "# ============================================\n",
    "\n",
    "class QuantumHead(nn.Module):\n",
    "    \"\"\"\n",
    "    VQC with 4 qubits, 3 layers, NO entanglement → 10 classes.\n",
    "    Uses Lightning acceleration (GPU).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_qubits=4, n_layers=3, num_classes=10, device_name=None):\n",
    "        super().__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Map 16 features → n_qubits\n",
    "        self.feature_select = nn.Linear(16, n_qubits)\n",
    "\n",
    "        # Use specified device or default\n",
    "        if device_name is None:\n",
    "            device_name = QUANTUM_DEVICE\n",
    "        \n",
    "        # Quantum device\n",
    "        self.dev = qml.device(device_name, wires=n_qubits)\n",
    "\n",
    "        # Use adjoint differentiation for lightning (much faster)\n",
    "        diff_method = \"adjoint\" if \"lightning\" in device_name else \"parameter-shift\"\n",
    "\n",
    "        @qml.qnode(self.dev, interface=\"torch\", diff_method=diff_method)\n",
    "        def quantum_circuit(inputs, weights):\n",
    "            \"\"\"\n",
    "            Single-sample circuit WITHOUT entanglement.\n",
    "            inputs: (n_qubits,)\n",
    "            weights: (n_layers, n_qubits, 2)\n",
    "            \"\"\"\n",
    "            for layer in range(n_layers):\n",
    "                # Data re-uploading\n",
    "                for i in range(n_qubits):\n",
    "                    qml.RY(inputs[i], wires=i)\n",
    "\n",
    "                # Trainable rotations\n",
    "                for i in range(n_qubits):\n",
    "                    qml.RY(weights[layer, i, 0], wires=i)\n",
    "                    qml.RZ(weights[layer, i, 1], wires=i)\n",
    "\n",
    "                # NO ENTANGLEMENT\n",
    "\n",
    "            return [\n",
    "                qml.expval(qml.PauliZ(0)),\n",
    "                qml.expval(qml.PauliZ(1)),\n",
    "                qml.expval(qml.PauliZ(2)),\n",
    "                qml.expval(qml.PauliZ(3)),\n",
    "                qml.expval(qml.PauliZ(0) @ qml.PauliZ(1)),\n",
    "                qml.expval(qml.PauliZ(2) @ qml.PauliZ(3))\n",
    "            ]\n",
    "\n",
    "        self.quantum_circuit = quantum_circuit\n",
    "\n",
    "        weight_shape = (n_layers, n_qubits, 2)\n",
    "        self.q_weights = nn.Parameter(torch.randn(weight_shape) * 0.1)\n",
    "        self.fc_out = nn.Linear(6, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Process samples with progress tracking.\n",
    "        x: (batch, 16) real bottleneck features\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        x = torch.tanh(self.feature_select(x))\n",
    "\n",
    "        # Process in chunks for memory management\n",
    "        chunk_size = 32\n",
    "        quantum_outputs = []\n",
    "\n",
    "        for start_idx in range(0, batch_size, chunk_size):\n",
    "            end_idx = min(start_idx + chunk_size, batch_size)\n",
    "            chunk = x[start_idx:end_idx]\n",
    "\n",
    "            chunk_outputs = []\n",
    "            for i in range(chunk.size(0)):\n",
    "                q_raw = self.quantum_circuit(chunk[i], self.q_weights)\n",
    "                if isinstance(q_raw, (list, tuple)):\n",
    "                    q_out = torch.stack(q_raw)\n",
    "                else:\n",
    "                    q_out = q_raw\n",
    "                chunk_outputs.append(q_out)\n",
    "\n",
    "            quantum_outputs.extend(chunk_outputs)\n",
    "\n",
    "        # Convert to tensor (cast to float32)\n",
    "        quantum_outputs = torch.stack(quantum_outputs).float()\n",
    "        quantum_outputs = quantum_outputs.to(self.fc_out.weight.dtype)\n",
    "\n",
    "        output = self.fc_out(quantum_outputs)\n",
    "        return output\n",
    "\n",
    "\n",
    "class QuantumNet(nn.Module):\n",
    "    \"\"\"Complete Quantum network: Preprocessor + QuantumHead (no ent)\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.preprocessor = SharedPreprocessor(3072, 16)\n",
    "        self.head = QuantumHead(n_qubits=4, n_layers=3, num_classes=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.preprocessor(x)\n",
    "        return self.head(features)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Training and Evaluation\n",
    "# ============================================\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, show_progress=True):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(loader):\n",
    "        # Keep data on CPU for quantum models\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        total_samples += x.size(0)\n",
    "\n",
    "        if show_progress and batch_idx % 20 == 0:\n",
    "            print(f\"    Batch {batch_idx}/{len(loader)}, samples: {total_samples}\", end=\"\\r\")\n",
    "\n",
    "    if show_progress:\n",
    "        print()\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            # Keep data on CPU\n",
    "            logits = model(x)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += x.size(0)\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "def train_with_early_stopping(model, train_loader, test_loader, optimizer,\n",
    "                              max_epochs=200, patience=10, name=\"Model\"):\n",
    "    best_acc = 0.0\n",
    "    epochs_without_improvement = 0\n",
    "    start = time.time()\n",
    "    last_acc = 0.0\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        print(f\"  [{name}] Epoch {epoch}/{max_epochs}\")\n",
    "        loss = train_one_epoch(model, train_loader, optimizer, show_progress=True)\n",
    "        acc = evaluate(model, test_loader)\n",
    "        last_acc = acc\n",
    "\n",
    "        elapsed = time.time() - start\n",
    "        print(f\"  [{name}] Epoch {epoch:2d} | loss={loss:.4f} \"\n",
    "              f\"| test_acc={acc:.4f} | time={elapsed:.1f}s\")\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"  [{name}] Early stop at epoch {epoch} \"\n",
    "                  f\"(no improvement for {patience} epochs)\")\n",
    "            break\n",
    "\n",
    "    total_time = time.time() - start\n",
    "    return {\n",
    "        \"best_acc\": best_acc,\n",
    "        \"final_acc\": last_acc,\n",
    "        \"time\": total_time,\n",
    "        \"epochs\": epoch,\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Data utilities\n",
    "# ============================================\n",
    "\n",
    "def stratified_sample_from_targets(dataset, n_samples_per_class, seed=42):\n",
    "    \"\"\"\n",
    "    Create a stratified sample with n_samples_per_class from each class.\n",
    "    Uses dataset.targets directly - NO image loading during sampling.\n",
    "    \"\"\"\n",
    "    # CIFAR-10 exposes targets as a list\n",
    "    if hasattr(dataset, 'targets'):\n",
    "        targets = np.array(dataset.targets)\n",
    "    else:\n",
    "        # Fallback for wrapped datasets\n",
    "        targets = np.array([dataset[i][1] for i in range(len(dataset))])\n",
    "    \n",
    "    rng = np.random.RandomState(seed)\n",
    "    \n",
    "    sampled_indices = []\n",
    "    for c in range(10):\n",
    "        # Find all indices for class c\n",
    "        idx_c = np.where(targets == c)[0]\n",
    "        k = min(n_samples_per_class, len(idx_c))\n",
    "        # Sample without replacement\n",
    "        selected = rng.choice(idx_c, size=k, replace=False).tolist()\n",
    "        sampled_indices.extend(selected)\n",
    "    \n",
    "    # Shuffle the combined indices\n",
    "    rng.shuffle(sampled_indices)\n",
    "    return sampled_indices\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Main\n",
    "# ============================================\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"BLOCK 3: Quantum (NO Entanglement) Training (CIFAR-10)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nConfiguration:\")\n",
    "    print(\"  • Dataset: CIFAR-10 (32×32 RGB)\")\n",
    "    print(\"  • Batch size: 32\")\n",
    "    print(\"  • Patience: 10\")\n",
    "    print(\"  • Max epochs: 200\")\n",
    "    print(\"  • Seeds: [42, 123, 456]\")\n",
    "    print(\"  • Architecture: [frozen 3072→16] → 4 qubits (3 layers, NO ent) → 10\")\n",
    "    print(f\"  • Quantum device: {QUANTUM_DEVICE}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Load frozen preprocessor from Block 1\n",
    "    print(\"\\nLoading frozen preprocessor from Block 1...\")\n",
    "    try:\n",
    "        realnet_data = torch.load(\"realnet_cifar10_results.pt\", weights_only=False)\n",
    "        preprocessor_state = realnet_data[\"preprocessor_state\"]\n",
    "        print(\"✓ Loaded preprocessor state from realnet_cifar10_results.pt\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"✗ ERROR: realnet_cifar10_results.pt not found!\")\n",
    "        print(\"  Run Block 1 first to train RealNet and save preprocessor.\")\n",
    "        return\n",
    "\n",
    "    # Create frozen preprocessor (CPU for quantum)\n",
    "    shared_preprocessor = SharedPreprocessor(3072, 16)\n",
    "    shared_preprocessor.load_state_dict(preprocessor_state)\n",
    "    for p in shared_preprocessor.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    preprocessor_params = sum(p.numel() for p in shared_preprocessor.parameters())\n",
    "    print(f\"  Preprocessor frozen with {preprocessor_params:,} params\")\n",
    "\n",
    "    # Load data - CIFAR-10 normalization\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.4914, 0.4822, 0.4465],\n",
    "            std=[0.2470, 0.2435, 0.2616]\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    full_train_ds = datasets.CIFAR10(root=\"./data\", train=True,\n",
    "                                     download=True, transform=transform)\n",
    "    full_test_ds = datasets.CIFAR10(root=\"./data\", train=False,\n",
    "                                    download=True, transform=transform)\n",
    "\n",
    "    # Create stratified samples (fast - uses targets only, no image loading)\n",
    "    print(\"\\nCreating stratified samples...\")\n",
    "    t0 = time.time()\n",
    "    train_indices = stratified_sample_from_targets(full_train_ds, n_samples_per_class=1500, seed=42)\n",
    "    test_indices = stratified_sample_from_targets(full_test_ds, n_samples_per_class=300, seed=42)\n",
    "    print(f\"  Sampling took {time.time()-t0:.2f}s\")\n",
    "    \n",
    "    train_ds = Subset(full_train_ds, train_indices)\n",
    "    test_ds = Subset(full_test_ds, test_indices)\n",
    "    \n",
    "    print(f\"  Train samples: {len(train_ds)} (stratified, 1500 per class)\")\n",
    "    print(f\"  Test samples:  {len(test_ds)} (stratified, 300 per class)\")\n",
    "\n",
    "    # Smaller batches for quantum\n",
    "    train_loader = DataLoader(train_ds, batch_size=32,\n",
    "                              shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_ds, batch_size=64,\n",
    "                             shuffle=False, num_workers=0)\n",
    "\n",
    "    seeds = [42, 123, 456]\n",
    "    all_results = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(f\"SEED {seed}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        set_all_seeds(seed)\n",
    "\n",
    "        print(f\"\\n  Training QuantumNet (NO entanglement, seed={seed})...\")\n",
    "        quantum_model = QuantumNet()\n",
    "        quantum_model.preprocessor = shared_preprocessor  # Use frozen preprocessor\n",
    "        \n",
    "        quantum_opt = torch.optim.Adam(quantum_model.head.parameters(), lr=1e-3)\n",
    "        \n",
    "        result = train_with_early_stopping(\n",
    "            quantum_model, train_loader, test_loader, quantum_opt,\n",
    "            max_epochs=200, patience=10, name=\"QuantumNoEnt\"\n",
    "        )\n",
    "        \n",
    "        result[\"trainable_params\"] = sum(p.numel() for p in quantum_model.parameters()\n",
    "                                         if p.requires_grad)\n",
    "        result[\"total_params\"] = sum(p.numel() for p in quantum_model.parameters())\n",
    "        result[\"seed\"] = seed\n",
    "        \n",
    "        all_results.append(result)\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"QUANTUM (NO ENTANGLEMENT) SUMMARY (CIFAR-10)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    accs = [r[\"best_acc\"] for r in all_results]\n",
    "    times = [r[\"time\"] for r in all_results]\n",
    "    epochs = [r[\"epochs\"] for r in all_results]\n",
    "    trainable = all_results[0][\"trainable_params\"]\n",
    "    total = all_results[0][\"total_params\"]\n",
    "    \n",
    "    print(f\"\\nAccuracy:   {np.mean(accs):.4f} ± {np.std(accs):.4f}\")\n",
    "    print(f\"Time:       {np.mean(times):.1f}s ± {np.std(times):.1f}s\")\n",
    "    print(f\"Epochs:     {np.mean(epochs):.1f} ± {np.std(epochs):.1f}\")\n",
    "    print(f\"Parameters: {trainable:,} trainable (head), {total:,} total\")\n",
    "    \n",
    "    print(\"\\nPer-seed results:\")\n",
    "    for r in all_results:\n",
    "        print(f\"  Seed {r['seed']}: acc={r['best_acc']:.4f}, \"\n",
    "              f\"time={r['time']:.1f}s, epochs={r['epochs']}\")\n",
    "\n",
    "    # Save results\n",
    "    save_dict = {\n",
    "        \"results\": all_results,\n",
    "        \"summary\": {\n",
    "            \"mean_acc\": np.mean(accs),\n",
    "            \"std_acc\": np.std(accs),\n",
    "            \"mean_time\": np.mean(times),\n",
    "            \"trainable_params\": trainable,\n",
    "            \"total_params\": total\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    torch.save(save_dict, \"quantum_noent_cifar10_results.pt\")\n",
    "    print(f\"\\n✓ Saved results to: quantum_noent_cifar10_results.pt\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88f3912-4a8e-4846-b2c1-1d2784a3cde8",
   "metadata": {},
   "source": [
    "# Quant Ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2411fc78-3424-4175-bdd1-79a343ff7f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using lightning.gpu device\n",
      "Using PyTorch device: cuda\n",
      "======================================================================\n",
      "BLOCK 4: Quantum (WITH Entanglement) Training (CIFAR-10)\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "  • Dataset: CIFAR-10 (32×32 RGB)\n",
      "  • Batch size: 32\n",
      "  • Patience: 10\n",
      "  • Max epochs: 200\n",
      "  • Seeds: [42, 123, 456]\n",
      "  • Architecture: [frozen 3072→16] → 4 qubits (3 layers, WITH ent) → 10\n",
      "  • Quantum device: lightning.gpu\n",
      "======================================================================\n",
      "\n",
      "Loading frozen preprocessor from Block 1...\n",
      "✓ Loaded preprocessor state from realnet_cifar10_results.pt\n",
      "  Preprocessor frozen with 49,168 params\n",
      "\n",
      "Creating stratified samples...\n",
      "  Sampling took 0.00s\n",
      "  Train samples: 15000 (stratified, 1500 per class)\n",
      "  Test samples:  3000 (stratified, 300 per class)\n",
      "\n",
      "======================================================================\n",
      "SEED 42\n",
      "======================================================================\n",
      "\n",
      "  Training QuantumNet (WITH entanglement, seed=42)...\n",
      "  [QuantumEnt] Epoch 1/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  1 | loss=2.2983 | test_acc=0.2123 | time=529.6s\n",
      "  [QuantumEnt] Epoch 2/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  2 | loss=2.1265 | test_acc=0.2300 | time=1043.0s\n",
      "  [QuantumEnt] Epoch 3/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  3 | loss=1.9891 | test_acc=0.2437 | time=1553.8s\n",
      "  [QuantumEnt] Epoch 4/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  4 | loss=1.9109 | test_acc=0.2693 | time=2069.1s\n",
      "  [QuantumEnt] Epoch 5/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  5 | loss=1.8665 | test_acc=0.2740 | time=2604.7s\n",
      "  [QuantumEnt] Epoch 6/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  6 | loss=1.8386 | test_acc=0.2723 | time=3133.8s\n",
      "  [QuantumEnt] Epoch 7/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  7 | loss=1.8173 | test_acc=0.2870 | time=3672.8s\n",
      "  [QuantumEnt] Epoch 8/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  8 | loss=1.7996 | test_acc=0.2880 | time=4203.6s\n",
      "  [QuantumEnt] Epoch 9/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  9 | loss=1.7835 | test_acc=0.2880 | time=4721.6s\n",
      "  [QuantumEnt] Epoch 10/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 10 | loss=1.7693 | test_acc=0.2963 | time=5233.4s\n",
      "  [QuantumEnt] Epoch 11/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 11 | loss=1.7560 | test_acc=0.3010 | time=5750.3s\n",
      "  [QuantumEnt] Epoch 12/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 12 | loss=1.7441 | test_acc=0.3013 | time=6270.8s\n",
      "  [QuantumEnt] Epoch 13/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 13 | loss=1.7333 | test_acc=0.3083 | time=6789.5s\n",
      "  [QuantumEnt] Epoch 14/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 14 | loss=1.7237 | test_acc=0.3140 | time=7306.5s\n",
      "  [QuantumEnt] Epoch 15/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 15 | loss=1.7148 | test_acc=0.3207 | time=7826.3s\n",
      "  [QuantumEnt] Epoch 16/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 16 | loss=1.7070 | test_acc=0.3180 | time=8336.6s\n",
      "  [QuantumEnt] Epoch 17/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 17 | loss=1.7006 | test_acc=0.3200 | time=8851.3s\n",
      "  [QuantumEnt] Epoch 18/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 18 | loss=1.6931 | test_acc=0.3190 | time=9390.0s\n",
      "  [QuantumEnt] Epoch 19/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 19 | loss=1.6876 | test_acc=0.3290 | time=9925.7s\n",
      "  [QuantumEnt] Epoch 20/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 20 | loss=1.6814 | test_acc=0.3310 | time=10463.1s\n",
      "  [QuantumEnt] Epoch 21/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 21 | loss=1.6760 | test_acc=0.3333 | time=10996.4s\n",
      "  [QuantumEnt] Epoch 22/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 22 | loss=1.6717 | test_acc=0.3353 | time=11532.4s\n",
      "  [QuantumEnt] Epoch 23/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 23 | loss=1.6669 | test_acc=0.3300 | time=12044.5s\n",
      "  [QuantumEnt] Epoch 24/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 24 | loss=1.6624 | test_acc=0.3327 | time=12550.6s\n",
      "  [QuantumEnt] Epoch 25/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 25 | loss=1.6577 | test_acc=0.3337 | time=13059.4s\n",
      "  [QuantumEnt] Epoch 26/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 26 | loss=1.6537 | test_acc=0.3397 | time=13567.0s\n",
      "  [QuantumEnt] Epoch 27/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 27 | loss=1.6491 | test_acc=0.3393 | time=14079.5s\n",
      "  [QuantumEnt] Epoch 28/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 28 | loss=1.6455 | test_acc=0.3410 | time=14586.5s\n",
      "  [QuantumEnt] Epoch 29/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 29 | loss=1.6425 | test_acc=0.3373 | time=15101.5s\n",
      "  [QuantumEnt] Epoch 30/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 30 | loss=1.6390 | test_acc=0.3380 | time=15613.3s\n",
      "  [QuantumEnt] Epoch 31/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 31 | loss=1.6352 | test_acc=0.3377 | time=16117.2s\n",
      "  [QuantumEnt] Epoch 32/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 32 | loss=1.6334 | test_acc=0.3377 | time=16627.8s\n",
      "  [QuantumEnt] Epoch 33/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 33 | loss=1.6305 | test_acc=0.3440 | time=17136.3s\n",
      "  [QuantumEnt] Epoch 34/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 34 | loss=1.6279 | test_acc=0.3453 | time=17648.4s\n",
      "  [QuantumEnt] Epoch 35/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 35 | loss=1.6258 | test_acc=0.3400 | time=18153.7s\n",
      "  [QuantumEnt] Epoch 36/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 36 | loss=1.6239 | test_acc=0.3413 | time=18665.4s\n",
      "  [QuantumEnt] Epoch 37/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 37 | loss=1.6221 | test_acc=0.3403 | time=19184.2s\n",
      "  [QuantumEnt] Epoch 38/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 38 | loss=1.6199 | test_acc=0.3453 | time=19691.0s\n",
      "  [QuantumEnt] Epoch 39/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 39 | loss=1.6182 | test_acc=0.3353 | time=20205.2s\n",
      "  [QuantumEnt] Epoch 40/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 40 | loss=1.6172 | test_acc=0.3453 | time=20713.3s\n",
      "  [QuantumEnt] Epoch 41/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 41 | loss=1.6153 | test_acc=0.3437 | time=21222.7s\n",
      "  [QuantumEnt] Epoch 42/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 42 | loss=1.6142 | test_acc=0.3463 | time=21740.1s\n",
      "  [QuantumEnt] Epoch 43/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 43 | loss=1.6130 | test_acc=0.3440 | time=22252.6s\n",
      "  [QuantumEnt] Epoch 44/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 44 | loss=1.6124 | test_acc=0.3430 | time=22769.4s\n",
      "  [QuantumEnt] Epoch 45/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 45 | loss=1.6109 | test_acc=0.3407 | time=23297.5s\n",
      "  [QuantumEnt] Epoch 46/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 46 | loss=1.6092 | test_acc=0.3490 | time=23841.8s\n",
      "  [QuantumEnt] Epoch 47/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 47 | loss=1.6088 | test_acc=0.3443 | time=24348.4s\n",
      "  [QuantumEnt] Epoch 48/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 48 | loss=1.6079 | test_acc=0.3403 | time=24861.7s\n",
      "  [QuantumEnt] Epoch 49/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 49 | loss=1.6072 | test_acc=0.3470 | time=25380.1s\n",
      "  [QuantumEnt] Epoch 50/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 50 | loss=1.6060 | test_acc=0.3440 | time=25895.3s\n",
      "  [QuantumEnt] Epoch 51/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 51 | loss=1.6056 | test_acc=0.3457 | time=26422.0s\n",
      "  [QuantumEnt] Epoch 52/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 52 | loss=1.6050 | test_acc=0.3433 | time=26938.9s\n",
      "  [QuantumEnt] Epoch 53/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 53 | loss=1.6042 | test_acc=0.3470 | time=27447.0s\n",
      "  [QuantumEnt] Epoch 54/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 54 | loss=1.6032 | test_acc=0.3373 | time=27953.2s\n",
      "  [QuantumEnt] Epoch 55/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 55 | loss=1.6024 | test_acc=0.3457 | time=28463.0s\n",
      "  [QuantumEnt] Epoch 56/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 56 | loss=1.6015 | test_acc=0.3380 | time=28973.1s\n",
      "  [QuantumEnt] Early stop at epoch 56 (no improvement for 10 epochs)\n",
      "\n",
      "======================================================================\n",
      "SEED 123\n",
      "======================================================================\n",
      "\n",
      "  Training QuantumNet (WITH entanglement, seed=123)...\n",
      "  [QuantumEnt] Epoch 1/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  1 | loss=2.2104 | test_acc=0.1917 | time=532.0s\n",
      "  [QuantumEnt] Epoch 2/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  2 | loss=1.9763 | test_acc=0.2390 | time=1064.6s\n",
      "  [QuantumEnt] Epoch 3/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  3 | loss=1.8803 | test_acc=0.2737 | time=1600.3s\n",
      "  [QuantumEnt] Epoch 4/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  4 | loss=1.8429 | test_acc=0.2847 | time=2133.6s\n",
      "  [QuantumEnt] Epoch 5/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  5 | loss=1.8126 | test_acc=0.2937 | time=2678.6s\n",
      "  [QuantumEnt] Epoch 6/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  6 | loss=1.7800 | test_acc=0.3050 | time=3217.6s\n",
      "  [QuantumEnt] Epoch 7/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  7 | loss=1.7514 | test_acc=0.3077 | time=3746.8s\n",
      "  [QuantumEnt] Epoch 8/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  8 | loss=1.7291 | test_acc=0.3150 | time=4274.0s\n",
      "  [QuantumEnt] Epoch 9/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  9 | loss=1.7117 | test_acc=0.3227 | time=4795.5s\n",
      "  [QuantumEnt] Epoch 10/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 10 | loss=1.6978 | test_acc=0.3253 | time=5313.3s\n",
      "  [QuantumEnt] Epoch 11/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 11 | loss=1.6853 | test_acc=0.3320 | time=5836.8s\n",
      "  [QuantumEnt] Epoch 12/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 12 | loss=1.6755 | test_acc=0.3330 | time=6370.3s\n",
      "  [QuantumEnt] Epoch 13/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 13 | loss=1.6663 | test_acc=0.3283 | time=6891.4s\n",
      "  [QuantumEnt] Epoch 14/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 14 | loss=1.6580 | test_acc=0.3320 | time=7447.1s\n",
      "  [QuantumEnt] Epoch 15/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 15 | loss=1.6515 | test_acc=0.3393 | time=7975.3s\n",
      "  [QuantumEnt] Epoch 16/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 16 | loss=1.6458 | test_acc=0.3370 | time=8546.5s\n",
      "  [QuantumEnt] Epoch 17/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 17 | loss=1.6406 | test_acc=0.3357 | time=9097.8s\n",
      "  [QuantumEnt] Epoch 18/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 18 | loss=1.6351 | test_acc=0.3420 | time=9660.7s\n",
      "  [QuantumEnt] Epoch 19/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 19 | loss=1.6311 | test_acc=0.3393 | time=10200.0s\n",
      "  [QuantumEnt] Epoch 20/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 20 | loss=1.6280 | test_acc=0.3417 | time=10752.4s\n",
      "  [QuantumEnt] Epoch 21/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 21 | loss=1.6241 | test_acc=0.3427 | time=11307.3s\n",
      "  [QuantumEnt] Epoch 22/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 22 | loss=1.6208 | test_acc=0.3383 | time=11856.1s\n",
      "  [QuantumEnt] Epoch 23/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 23 | loss=1.6182 | test_acc=0.3453 | time=12391.6s\n",
      "  [QuantumEnt] Epoch 24/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 24 | loss=1.6153 | test_acc=0.3420 | time=12958.0s\n",
      "  [QuantumEnt] Epoch 25/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 25 | loss=1.6142 | test_acc=0.3447 | time=13492.9s\n",
      "  [QuantumEnt] Epoch 26/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 26 | loss=1.6114 | test_acc=0.3460 | time=14051.5s\n",
      "  [QuantumEnt] Epoch 27/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 27 | loss=1.6093 | test_acc=0.3467 | time=14594.6s\n",
      "  [QuantumEnt] Epoch 28/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 28 | loss=1.6079 | test_acc=0.3480 | time=15148.6s\n",
      "  [QuantumEnt] Epoch 29/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 29 | loss=1.6065 | test_acc=0.3423 | time=15702.8s\n",
      "  [QuantumEnt] Epoch 30/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 30 | loss=1.6051 | test_acc=0.3453 | time=16252.5s\n",
      "  [QuantumEnt] Epoch 31/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 31 | loss=1.6043 | test_acc=0.3477 | time=16809.4s\n",
      "  [QuantumEnt] Epoch 32/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 32 | loss=1.6034 | test_acc=0.3457 | time=17340.1s\n",
      "  [QuantumEnt] Epoch 33/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 33 | loss=1.6019 | test_acc=0.3460 | time=17899.3s\n",
      "  [QuantumEnt] Epoch 34/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 34 | loss=1.6007 | test_acc=0.3453 | time=18434.6s\n",
      "  [QuantumEnt] Epoch 35/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 35 | loss=1.5999 | test_acc=0.3510 | time=19000.5s\n",
      "  [QuantumEnt] Epoch 36/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 36 | loss=1.5995 | test_acc=0.3477 | time=19535.6s\n",
      "  [QuantumEnt] Epoch 37/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 37 | loss=1.5986 | test_acc=0.3413 | time=20097.3s\n",
      "  [QuantumEnt] Epoch 38/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 38 | loss=1.5984 | test_acc=0.3420 | time=20652.3s\n",
      "  [QuantumEnt] Epoch 39/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 39 | loss=1.5967 | test_acc=0.3470 | time=21184.8s\n",
      "  [QuantumEnt] Epoch 40/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 40 | loss=1.5969 | test_acc=0.3467 | time=21731.2s\n",
      "  [QuantumEnt] Epoch 41/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 41 | loss=1.5961 | test_acc=0.3520 | time=22301.3s\n",
      "  [QuantumEnt] Epoch 42/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 42 | loss=1.5964 | test_acc=0.3450 | time=22842.6s\n",
      "  [QuantumEnt] Epoch 43/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 43 | loss=1.5953 | test_acc=0.3413 | time=23397.8s\n",
      "  [QuantumEnt] Epoch 44/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 44 | loss=1.5941 | test_acc=0.3477 | time=23946.7s\n",
      "  [QuantumEnt] Epoch 45/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 45 | loss=1.5949 | test_acc=0.3467 | time=24507.0s\n",
      "  [QuantumEnt] Epoch 46/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 46 | loss=1.5949 | test_acc=0.3483 | time=25077.1s\n",
      "  [QuantumEnt] Epoch 47/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 47 | loss=1.5941 | test_acc=0.3480 | time=25621.5s\n",
      "  [QuantumEnt] Epoch 48/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 48 | loss=1.5942 | test_acc=0.3447 | time=26197.2s\n",
      "  [QuantumEnt] Epoch 49/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 49 | loss=1.5934 | test_acc=0.3440 | time=26730.8s\n",
      "  [QuantumEnt] Epoch 50/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 50 | loss=1.5930 | test_acc=0.3480 | time=27293.7s\n",
      "  [QuantumEnt] Epoch 51/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 51 | loss=1.5927 | test_acc=0.3463 | time=27844.7s\n",
      "  [QuantumEnt] Early stop at epoch 51 (no improvement for 10 epochs)\n",
      "\n",
      "======================================================================\n",
      "SEED 456\n",
      "======================================================================\n",
      "\n",
      "  Training QuantumNet (WITH entanglement, seed=456)...\n",
      "  [QuantumEnt] Epoch 1/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  1 | loss=2.1980 | test_acc=0.2560 | time=561.7s\n",
      "  [QuantumEnt] Epoch 2/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  2 | loss=1.9772 | test_acc=0.2733 | time=1100.2s\n",
      "  [QuantumEnt] Epoch 3/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  3 | loss=1.8926 | test_acc=0.2883 | time=1658.8s\n",
      "  [QuantumEnt] Epoch 4/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  4 | loss=1.8409 | test_acc=0.2950 | time=2193.5s\n",
      "  [QuantumEnt] Epoch 5/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  5 | loss=1.8034 | test_acc=0.3057 | time=2749.6s\n",
      "  [QuantumEnt] Epoch 6/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  6 | loss=1.7705 | test_acc=0.3213 | time=3287.1s\n",
      "  [QuantumEnt] Epoch 7/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  7 | loss=1.7416 | test_acc=0.3263 | time=3837.5s\n",
      "  [QuantumEnt] Epoch 8/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  8 | loss=1.7185 | test_acc=0.3337 | time=4360.3s\n",
      "  [QuantumEnt] Epoch 9/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  9 | loss=1.7005 | test_acc=0.3370 | time=4897.0s\n",
      "  [QuantumEnt] Epoch 10/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 10 | loss=1.6882 | test_acc=0.3413 | time=5450.1s\n",
      "  [QuantumEnt] Epoch 11/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 11 | loss=1.6775 | test_acc=0.3377 | time=6018.7s\n",
      "  [QuantumEnt] Epoch 12/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 12 | loss=1.6693 | test_acc=0.3410 | time=6551.4s\n",
      "  [QuantumEnt] Epoch 13/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 13 | loss=1.6624 | test_acc=0.3467 | time=7092.7s\n",
      "  [QuantumEnt] Epoch 14/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 14 | loss=1.6562 | test_acc=0.3410 | time=7652.2s\n",
      "  [QuantumEnt] Epoch 15/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 15 | loss=1.6520 | test_acc=0.3417 | time=8197.1s\n",
      "  [QuantumEnt] Epoch 16/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 16 | loss=1.6472 | test_acc=0.3370 | time=8735.4s\n",
      "  [QuantumEnt] Epoch 17/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 17 | loss=1.6439 | test_acc=0.3410 | time=9278.7s\n",
      "  [QuantumEnt] Epoch 18/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 18 | loss=1.6406 | test_acc=0.3443 | time=9843.7s\n",
      "  [QuantumEnt] Epoch 19/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 19 | loss=1.6375 | test_acc=0.3423 | time=10375.2s\n",
      "  [QuantumEnt] Epoch 20/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 20 | loss=1.6357 | test_acc=0.3407 | time=10902.9s\n",
      "  [QuantumEnt] Epoch 21/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 21 | loss=1.6325 | test_acc=0.3387 | time=11453.2s\n",
      "  [QuantumEnt] Epoch 22/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 22 | loss=1.6296 | test_acc=0.3407 | time=12010.7s\n",
      "  [QuantumEnt] Epoch 23/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 23 | loss=1.6281 | test_acc=0.3413 | time=12534.2s\n",
      "  [QuantumEnt] Early stop at epoch 23 (no improvement for 10 epochs)\n",
      "\n",
      "======================================================================\n",
      "QUANTUM (WITH ENTANGLEMENT) SUMMARY (CIFAR-10)\n",
      "======================================================================\n",
      "\n",
      "Accuracy:   0.3492 ± 0.0022\n",
      "Time:       23117.3s ± 7497.6s\n",
      "Epochs:     43.3 ± 14.5\n",
      "Parameters: 162 trainable (head), 49,330 total\n",
      "\n",
      "Per-seed results:\n",
      "  Seed 42: acc=0.3490, time=28973.1s, epochs=56\n",
      "  Seed 123: acc=0.3520, time=27844.7s, epochs=51\n",
      "  Seed 456: acc=0.3467, time=12534.2s, epochs=23\n",
      "\n",
      "✓ Saved results to: quantum_ent_cifar10_results.pt\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Block 4: Quantum (WITH Entanglement) Training (CIFAR-10)\n",
    "==========================================================\n",
    "Loads frozen preprocessor from Block 1 and trains quantum head WITH entanglement.\n",
    "Uses Lightning-GPU acceleration.\n",
    "\n",
    "Requirements:\n",
    "- realnet_cifar10_results.pt (from Block 1)\n",
    "- pennylane, pennylane-lightning-gpu\n",
    "\n",
    "Outputs:\n",
    "- quantum_ent_cifar10_results.pt: Contains quantum (with ent) results\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "try:\n",
    "    import pennylane as qml\n",
    "    QUANTUM_DEVICE = \"lightning.gpu\"\n",
    "    PENNYLANE_AVAILABLE = True\n",
    "    print(\"✓ Using lightning.gpu device\")\n",
    "except ImportError:\n",
    "    PENNYLANE_AVAILABLE = False\n",
    "    print(\"✗ PennyLane not installed\")\n",
    "    exit(1)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "print(f\"Using PyTorch device: {device}\")\n",
    "\n",
    "# ============================================\n",
    "# Comprehensive seed setting for reproducibility\n",
    "# ============================================\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    \"\"\"Set seeds for all RNG sources for reproducibility\"\"\"\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    # Python\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # NumPy\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # PyTorch deterministic mode\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # CuPy (if available)\n",
    "    try:\n",
    "        import cupy as cp\n",
    "        cp.random.seed(seed)\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    # PennyLane (if available)\n",
    "    try:\n",
    "        import pennylane as qml\n",
    "        qml.numpy.random.seed(seed)\n",
    "    except (ImportError, AttributeError):\n",
    "        pass\n",
    "\n",
    "# ============================================\n",
    "# Shared Preprocessor (must match Block 1 - CIFAR-10)\n",
    "# ============================================\n",
    "\n",
    "class SharedPreprocessor(nn.Module):\n",
    "    \"\"\"Shared classical feature extractor: 3072 → 16\"\"\"\n",
    "    def __init__(self, input_dim=3072, bottleneck_dim=16):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, bottleneck_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten to (batch, 3072)\n",
    "        x = torch.tanh(self.fc(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Quantum Head (WITH Entanglement)\n",
    "# ============================================\n",
    "\n",
    "class QuantumHead(nn.Module):\n",
    "    \"\"\"\n",
    "    VQC with 4 qubits, 3 layers, WITH entanglement → 10 classes.\n",
    "    Uses Lightning acceleration (GPU).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_qubits=4, n_layers=3, num_classes=10, device_name=None):\n",
    "        super().__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Map 16 features → n_qubits\n",
    "        self.feature_select = nn.Linear(16, n_qubits)\n",
    "\n",
    "        # Use specified device or default\n",
    "        if device_name is None:\n",
    "            device_name = QUANTUM_DEVICE\n",
    "        \n",
    "        # Quantum device\n",
    "        self.dev = qml.device(device_name, wires=n_qubits)\n",
    "\n",
    "        # Use adjoint differentiation for lightning (much faster)\n",
    "        diff_method = \"adjoint\" if \"lightning\" in device_name else \"parameter-shift\"\n",
    "\n",
    "        @qml.qnode(self.dev, interface=\"torch\", diff_method=diff_method)\n",
    "        def quantum_circuit(inputs, weights):\n",
    "            \"\"\"\n",
    "            Single-sample circuit WITH entanglement.\n",
    "            inputs: (n_qubits,)\n",
    "            weights: (n_layers, n_qubits, 2)\n",
    "            \"\"\"\n",
    "            for layer in range(n_layers):\n",
    "                # Data re-uploading\n",
    "                for i in range(n_qubits):\n",
    "                    qml.RY(inputs[i], wires=i)\n",
    "\n",
    "                # Trainable rotations\n",
    "                for i in range(n_qubits):\n",
    "                    qml.RY(weights[layer, i, 0], wires=i)\n",
    "                    qml.RZ(weights[layer, i, 1], wires=i)\n",
    "\n",
    "                # ENTANGLEMENT: CNOT ring\n",
    "                for i in range(n_qubits - 1):\n",
    "                    qml.CNOT(wires=[i, i + 1])\n",
    "                if n_qubits > 2:\n",
    "                    qml.CNOT(wires=[n_qubits - 1, 0])\n",
    "\n",
    "            return [\n",
    "                qml.expval(qml.PauliZ(0)),\n",
    "                qml.expval(qml.PauliZ(1)),\n",
    "                qml.expval(qml.PauliZ(2)),\n",
    "                qml.expval(qml.PauliZ(3)),\n",
    "                qml.expval(qml.PauliZ(0) @ qml.PauliZ(1)),\n",
    "                qml.expval(qml.PauliZ(2) @ qml.PauliZ(3))\n",
    "            ]\n",
    "\n",
    "        self.quantum_circuit = quantum_circuit\n",
    "\n",
    "        weight_shape = (n_layers, n_qubits, 2)\n",
    "        self.q_weights = nn.Parameter(torch.randn(weight_shape) * 0.1)\n",
    "        self.fc_out = nn.Linear(6, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Process samples with progress tracking.\n",
    "        x: (batch, 16) real bottleneck features\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        x = torch.tanh(self.feature_select(x))\n",
    "\n",
    "        # Process in chunks for memory management\n",
    "        chunk_size = 32\n",
    "        quantum_outputs = []\n",
    "\n",
    "        for start_idx in range(0, batch_size, chunk_size):\n",
    "            end_idx = min(start_idx + chunk_size, batch_size)\n",
    "            chunk = x[start_idx:end_idx]\n",
    "\n",
    "            chunk_outputs = []\n",
    "            for i in range(chunk.size(0)):\n",
    "                q_raw = self.quantum_circuit(chunk[i], self.q_weights)\n",
    "                if isinstance(q_raw, (list, tuple)):\n",
    "                    q_out = torch.stack(q_raw)\n",
    "                else:\n",
    "                    q_out = q_raw\n",
    "                chunk_outputs.append(q_out)\n",
    "\n",
    "            quantum_outputs.extend(chunk_outputs)\n",
    "\n",
    "        # Convert to tensor (cast to float32)\n",
    "        quantum_outputs = torch.stack(quantum_outputs).float()\n",
    "        quantum_outputs = quantum_outputs.to(self.fc_out.weight.dtype)\n",
    "\n",
    "        output = self.fc_out(quantum_outputs)\n",
    "        return output\n",
    "\n",
    "\n",
    "class QuantumNet(nn.Module):\n",
    "    \"\"\"Complete Quantum network: Preprocessor + QuantumHead (with ent)\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.preprocessor = SharedPreprocessor(3072, 16)\n",
    "        self.head = QuantumHead(n_qubits=4, n_layers=3, num_classes=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.preprocessor(x)\n",
    "        return self.head(features)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Training and Evaluation\n",
    "# ============================================\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, show_progress=True):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(loader):\n",
    "        # Keep data on CPU for quantum models\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        total_samples += x.size(0)\n",
    "\n",
    "        if show_progress and batch_idx % 20 == 0:\n",
    "            print(f\"    Batch {batch_idx}/{len(loader)}, samples: {total_samples}\", end=\"\\r\")\n",
    "\n",
    "    if show_progress:\n",
    "        print()\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            # Keep data on CPU\n",
    "            logits = model(x)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += x.size(0)\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "def train_with_early_stopping(model, train_loader, test_loader, optimizer,\n",
    "                              max_epochs=200, patience=10, name=\"Model\"):\n",
    "    best_acc = 0.0\n",
    "    epochs_without_improvement = 0\n",
    "    start = time.time()\n",
    "    last_acc = 0.0\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        print(f\"  [{name}] Epoch {epoch}/{max_epochs}\")\n",
    "        loss = train_one_epoch(model, train_loader, optimizer, show_progress=True)\n",
    "        acc = evaluate(model, test_loader)\n",
    "        last_acc = acc\n",
    "\n",
    "        elapsed = time.time() - start\n",
    "        print(f\"  [{name}] Epoch {epoch:2d} | loss={loss:.4f} \"\n",
    "              f\"| test_acc={acc:.4f} | time={elapsed:.1f}s\")\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"  [{name}] Early stop at epoch {epoch} \"\n",
    "                  f\"(no improvement for {patience} epochs)\")\n",
    "            break\n",
    "\n",
    "    total_time = time.time() - start\n",
    "    return {\n",
    "        \"best_acc\": best_acc,\n",
    "        \"final_acc\": last_acc,\n",
    "        \"time\": total_time,\n",
    "        \"epochs\": epoch,\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Data utilities\n",
    "# ============================================\n",
    "\n",
    "def stratified_sample_from_targets(dataset, n_samples_per_class, seed=42):\n",
    "    \"\"\"\n",
    "    Create a stratified sample with n_samples_per_class from each class.\n",
    "    Uses dataset.targets directly - NO image loading during sampling.\n",
    "    \"\"\"\n",
    "    # CIFAR-10 exposes targets as a list\n",
    "    if hasattr(dataset, 'targets'):\n",
    "        targets = np.array(dataset.targets)\n",
    "    else:\n",
    "        # Fallback for wrapped datasets\n",
    "        targets = np.array([dataset[i][1] for i in range(len(dataset))])\n",
    "    \n",
    "    rng = np.random.RandomState(seed)\n",
    "    \n",
    "    sampled_indices = []\n",
    "    for c in range(10):\n",
    "        # Find all indices for class c\n",
    "        idx_c = np.where(targets == c)[0]\n",
    "        k = min(n_samples_per_class, len(idx_c))\n",
    "        # Sample without replacement\n",
    "        selected = rng.choice(idx_c, size=k, replace=False).tolist()\n",
    "        sampled_indices.extend(selected)\n",
    "    \n",
    "    # Shuffle the combined indices\n",
    "    rng.shuffle(sampled_indices)\n",
    "    return sampled_indices\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Main\n",
    "# ============================================\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"BLOCK 4: Quantum (WITH Entanglement) Training (CIFAR-10)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nConfiguration:\")\n",
    "    print(\"  • Dataset: CIFAR-10 (32×32 RGB)\")\n",
    "    print(\"  • Batch size: 32\")\n",
    "    print(\"  • Patience: 10\")\n",
    "    print(\"  • Max epochs: 200\")\n",
    "    print(\"  • Seeds: [42, 123, 456]\")\n",
    "    print(\"  • Architecture: [frozen 3072→16] → 4 qubits (3 layers, WITH ent) → 10\")\n",
    "    print(f\"  • Quantum device: {QUANTUM_DEVICE}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Load frozen preprocessor from Block 1\n",
    "    print(\"\\nLoading frozen preprocessor from Block 1...\")\n",
    "    try:\n",
    "        realnet_data = torch.load(\"realnet_cifar10_results.pt\", weights_only=False)\n",
    "        preprocessor_state = realnet_data[\"preprocessor_state\"]\n",
    "        print(\"✓ Loaded preprocessor state from realnet_cifar10_results.pt\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"✗ ERROR: realnet_cifar10_results.pt not found!\")\n",
    "        print(\"  Run Block 1 first to train RealNet and save preprocessor.\")\n",
    "        return\n",
    "\n",
    "    # Create frozen preprocessor (CPU for quantum)\n",
    "    shared_preprocessor = SharedPreprocessor(3072, 16)\n",
    "    shared_preprocessor.load_state_dict(preprocessor_state)\n",
    "    for p in shared_preprocessor.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    preprocessor_params = sum(p.numel() for p in shared_preprocessor.parameters())\n",
    "    print(f\"  Preprocessor frozen with {preprocessor_params:,} params\")\n",
    "\n",
    "    # Load data - CIFAR-10 normalization\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.4914, 0.4822, 0.4465],\n",
    "            std=[0.2470, 0.2435, 0.2616]\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    full_train_ds = datasets.CIFAR10(root=\"./data\", train=True,\n",
    "                                     download=True, transform=transform)\n",
    "    full_test_ds = datasets.CIFAR10(root=\"./data\", train=False,\n",
    "                                    download=True, transform=transform)\n",
    "\n",
    "    # Create stratified samples (fast - uses targets only, no image loading)\n",
    "    print(\"\\nCreating stratified samples...\")\n",
    "    t0 = time.time()\n",
    "    train_indices = stratified_sample_from_targets(full_train_ds, n_samples_per_class=1500, seed=42)\n",
    "    test_indices = stratified_sample_from_targets(full_test_ds, n_samples_per_class=300, seed=42)\n",
    "    print(f\"  Sampling took {time.time()-t0:.2f}s\")\n",
    "    \n",
    "    train_ds = Subset(full_train_ds, train_indices)\n",
    "    test_ds = Subset(full_test_ds, test_indices)\n",
    "    \n",
    "    print(f\"  Train samples: {len(train_ds)} (stratified, 1500 per class)\")\n",
    "    print(f\"  Test samples:  {len(test_ds)} (stratified, 300 per class)\")\n",
    "\n",
    "    # Smaller batches for quantum\n",
    "    train_loader = DataLoader(train_ds, batch_size=32,\n",
    "                              shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_ds, batch_size=64,\n",
    "                             shuffle=False, num_workers=0)\n",
    "\n",
    "    seeds = [42, 123, 456]\n",
    "    all_results = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(f\"SEED {seed}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        set_all_seeds(seed)\n",
    "\n",
    "        print(f\"\\n  Training QuantumNet (WITH entanglement, seed={seed})...\")\n",
    "        quantum_model = QuantumNet()\n",
    "        quantum_model.preprocessor = shared_preprocessor  # Use frozen preprocessor\n",
    "        \n",
    "        quantum_opt = torch.optim.Adam(quantum_model.head.parameters(), lr=1e-3)\n",
    "        \n",
    "        result = train_with_early_stopping(\n",
    "            quantum_model, train_loader, test_loader, quantum_opt,\n",
    "            max_epochs=200, patience=10, name=\"QuantumEnt\"\n",
    "        )\n",
    "        \n",
    "        result[\"trainable_params\"] = sum(p.numel() for p in quantum_model.parameters()\n",
    "                                         if p.requires_grad)\n",
    "        result[\"total_params\"] = sum(p.numel() for p in quantum_model.parameters())\n",
    "        result[\"seed\"] = seed\n",
    "        \n",
    "        all_results.append(result)\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"QUANTUM (WITH ENTANGLEMENT) SUMMARY (CIFAR-10)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    accs = [r[\"best_acc\"] for r in all_results]\n",
    "    times = [r[\"time\"] for r in all_results]\n",
    "    epochs = [r[\"epochs\"] for r in all_results]\n",
    "    trainable = all_results[0][\"trainable_params\"]\n",
    "    total = all_results[0][\"total_params\"]\n",
    "    \n",
    "    print(f\"\\nAccuracy:   {np.mean(accs):.4f} ± {np.std(accs):.4f}\")\n",
    "    print(f\"Time:       {np.mean(times):.1f}s ± {np.std(times):.1f}s\")\n",
    "    print(f\"Epochs:     {np.mean(epochs):.1f} ± {np.std(epochs):.1f}\")\n",
    "    print(f\"Parameters: {trainable:,} trainable (head), {total:,} total\")\n",
    "    \n",
    "    print(\"\\nPer-seed results:\")\n",
    "    for r in all_results:\n",
    "        print(f\"  Seed {r['seed']}: acc={r['best_acc']:.4f}, \"\n",
    "              f\"time={r['time']:.1f}s, epochs={r['epochs']}\")\n",
    "\n",
    "    # Save results\n",
    "    save_dict = {\n",
    "        \"results\": all_results,\n",
    "        \"summary\": {\n",
    "            \"mean_acc\": np.mean(accs),\n",
    "            \"std_acc\": np.std(accs),\n",
    "            \"mean_time\": np.mean(times),\n",
    "            \"trainable_params\": trainable,\n",
    "            \"total_params\": total\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    torch.save(save_dict, \"quantum_ent_cifar10_results.pt\")\n",
    "    print(f\"\\n✓ Saved results to: quantum_ent_cifar10_results.pt\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46aeb1c-b285-4f93-88e7-722dca4278b8",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7dcb002-3456-4f65-85c7-dcfc2ebb7c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "BLOCK 5: AGGREGATE RESULTS AND COMPARATIVE ANALYSIS (CIFAR-10)\n",
      "==========================================================================================\n",
      "✓ Loaded RealNet (CIFAR-10) results\n",
      "✓ Loaded QuatNet (CIFAR-10) results\n",
      "✓ Loaded Quantum (no ent) (CIFAR-10) results\n",
      "✓ Loaded Quantum (with ent) (CIFAR-10) results\n",
      "\n",
      "==========================================================================================\n",
      "AGGREGATED RESULTS (CIFAR-10) (mean ± std over 3 seeds)\n",
      "==========================================================================================\n",
      "Model           Accuracy             Time (s)             Epochs          Parameters          \n",
      "------------------------------------------------------------------------------------------\n",
      "Real            0.4023 ± 0.0044      11.0 ± 1.7           24.0 ± 3.6      50,906              \n",
      "Quat            0.3792 ± 0.0033      24.3 ± 2.7           36.0 ± 5.7      1,000 (head)        \n",
      "QNoEnt          0.3510 ± 0.0071      15503.8 ± 916.7      32.3 ± 1.9      162 (head)          \n",
      "QEnt            0.3492 ± 0.0022      23117.3 ± 7497.6     43.3 ± 14.5     162 (head)          \n",
      "==========================================================================================\n",
      "\n",
      "======================================================================\n",
      "PER-SEED RESULTS (CIFAR-10)\n",
      "======================================================================\n",
      "\n",
      "Seed 42:\n",
      "Model           Accuracy     Time (s)     Epochs    \n",
      "--------------------------------------------------\n",
      "Real            0.3973       13.3         29        \n",
      "Quat            0.3753       20.4         28        \n",
      "QNoEnt          0.3420       14736.7      31        \n",
      "QEnt            0.3490       28973.1      56        \n",
      "\n",
      "Seed 123:\n",
      "Model           Accuracy     Time (s)     Epochs    \n",
      "--------------------------------------------------\n",
      "Real            0.4080       9.3          21        \n",
      "Quat            0.3790       26.3         40        \n",
      "QNoEnt          0.3517       16792.4      35        \n",
      "QEnt            0.3520       27844.7      51        \n",
      "\n",
      "Seed 456:\n",
      "Model           Accuracy     Time (s)     Epochs    \n",
      "--------------------------------------------------\n",
      "Real            0.4017       10.5         22        \n",
      "Quat            0.3833       26.1         40        \n",
      "QNoEnt          0.3593       14982.3      31        \n",
      "QEnt            0.3467       12534.2      23        \n",
      "\n",
      "==========================================================================================\n",
      "COMPARATIVE ANALYSIS (CIFAR-10): Answering Research Questions\n",
      "==========================================================================================\n",
      "\n",
      "1. QUATERNION vs REAL MLP:\n",
      "   ----------------------------------------------------------------------\n",
      "   Real MLP accuracy:        0.4023 ± 0.0044\n",
      "   Quaternion accuracy:      0.3792 ± 0.0033\n",
      "   Gap:                      2.31 percentage points\n",
      "   Performance retention:    94.3%\n",
      "\n",
      "   → Classical SU(2) (quaternions) captures 94.3% of\n",
      "     standard MLP performance with structured algebraic constraints on CIFAR-10.\n",
      "\n",
      "2. QUANTUM (no entanglement) vs REAL MLP:\n",
      "   ----------------------------------------------------------------------\n",
      "   Real MLP accuracy:        0.4023 ± 0.0044\n",
      "   Quantum (no ent) accuracy:0.3510 ± 0.0071\n",
      "   Gap:                      5.13 percentage points\n",
      "   Performance retention:    87.2%\n",
      "\n",
      "   → Quantum circuits WITHOUT entanglement capture 87.2%\n",
      "     of MLP performance, suggesting limited benefit over classical rotation gates\n",
      "     on CIFAR-10's color images.\n",
      "\n",
      "3. QUANTUM (with entanglement) vs REAL MLP:\n",
      "   ----------------------------------------------------------------------\n",
      "   Real MLP accuracy:        0.4023 ± 0.0044\n",
      "   Quantum (with ent) accuracy:0.3492 ± 0.0022\n",
      "   Gap:                      5.31 percentage points\n",
      "   Performance retention:    86.8%\n",
      "\n",
      "   → Quantum circuits WITH entanglement capture 86.8%\n",
      "     of MLP performance on CIFAR-10.\n",
      "\n",
      "4. ENTANGLEMENT EFFECT:\n",
      "   ----------------------------------------------------------------------\n",
      "   Quantum (no ent) accuracy:0.3510 ± 0.0071\n",
      "   Quantum (with ent) accuracy:0.3492 ± 0.0022\n",
      "   Improvement:              -0.18 percentage points\n",
      "\n",
      "   → Entanglement DOES NOT improve performance on CIFAR-10\n",
      "     → Task difficulty may mask entanglement benefit\n",
      "\n",
      "5. QUATERNION vs QUANTUM (no entanglement) - Core Research Question:\n",
      "   ----------------------------------------------------------------------\n",
      "   Quaternion accuracy:      0.3792 ± 0.0033\n",
      "   Quantum (no ent) accuracy:0.3510 ± 0.0071\n",
      "   Gap:                      2.82 percentage points\n",
      "\n",
      "   → Quaternion networks OUTPERFORM quantum circuits without entanglement\n",
      "     by 2.82 percentage points.\n",
      "\n",
      "   → Classical SU(2) (quaternions) SUFFICES for learning tasks addressable\n",
      "     by product-state quantum circuits on CIFAR-10, consistent with\n",
      "     the paper's findings on FashionMNIST (~6pp advantage).\n",
      "\n",
      "6. QUATERNION vs QUANTUM (with entanglement):\n",
      "   ----------------------------------------------------------------------\n",
      "   Quaternion accuracy:      0.3792 ± 0.0033\n",
      "   Quantum (with ent) accuracy:0.3492 ± 0.0022\n",
      "   Gap:                      3.00 percentage points\n",
      "\n",
      "   → Quaternions match or exceed entangled quantum performance on CIFAR-10,\n",
      "     suggesting entanglement provides limited benefit on this harder task.\n",
      "     → Measurement loss + optimization challenges may dominate on complex data.\n",
      "\n",
      "==========================================================================================\n",
      "KEY TAKEAWAYS FOR PAPER (CIFAR-10)\n",
      "==========================================================================================\n",
      "\n",
      "✓ EMPIRICAL FINDINGS:\n",
      "  1. Quaternion networks achieve 94.3% of Real MLP performance\n",
      "     with structured SU(2) algebraic constraints on CIFAR-10\n",
      "     → Note: Lower absolute accuracy (~37.9%) reflects\n",
      "       16-D bottleneck limitation, not quaternion geometry weakness\n",
      "\n",
      "  2. Quaternions vs Quantum (no ent): 2.82 percentage point gap\n",
      "     → Classical SU(2) OUTPERFORMS quantum product-state circuits\n",
      "       (consistent with paper: quaternions >> quantum on FashionMNIST)\n",
      "\n",
      "  3. Entanglement provides minimal/no benefit (-0.5%) on CIFAR-10\n",
      "     → Task difficulty may mask entanglement advantage\n",
      "\n",
      "✓ IMPLICATIONS:\n",
      "  • CIFAR-10 results test generalization of paper's findings to harder tasks\n",
      "  • If quaternions still outperform quantum (like FashionMNIST):\n",
      "    → Strengthens claim that classical SU(2) suffices for product-state VQCs\n",
      "  • If entanglement benefit persists (~0.5-1pp):\n",
      "    → Confirms entanglement as quantum boundary across task difficulties\n",
      "  • Lower absolute accuracies (~30-40%) reflect bottleneck constraint,\n",
      "    NOT failure of any specific architecture\n",
      "\n",
      "✓ COMPUTATIONAL EFFICIENCY:\n",
      "  • Real MLP:  11.0s (baseline)\n",
      "  • Quaternion: 24.3s (2.20x Real)\n",
      "  • Quantum (no ent): 15503.8s (1404.6x Real)\n",
      "    → Even with lightning.gpu, quantum is 638.3x slower than quaternions\n",
      "    → Pattern consistent across datasets (FashionMNIST: ~500x, CIFAR-10: ~638x)\n",
      "\n",
      "✓ CONTROLLED EXPERIMENTAL DESIGN:\n",
      "  • All models use IDENTICAL frozen 3072→16 preprocessor\n",
      "  • Performance differences reflect HEAD ARCHITECTURE ONLY\n",
      "  • Validates paper's methodology on second dataset (CIFAR-10)\n",
      "==========================================================================================\n",
      "\n",
      "==========================================================================================\n",
      "ANALYSIS COMPLETE (CIFAR-10)\n",
      "==========================================================================================\n",
      "\n",
      "All results saved in:\n",
      "  • realnet_cifar10_results.pt\n",
      "  • quatnet_cifar10_results.pt\n",
      "  • quantum_noent_cifar10_results.pt\n",
      "  • quantum_ent_cifar10_results.pt\n",
      "\n",
      "==========================================================================================\n",
      "COMPARISON TO PAPER (FashionMNIST)\n",
      "==========================================================================================\n",
      "\n",
      "Paper's key findings on FashionMNIST:\n",
      "  • Quaternions ≈ Real MLP (within 0.2pp)\n",
      "  • Quaternions >> Quantum-NoEnt (by ~2.4pp)\n",
      "  • Entanglement helps modestly (0.6pp gain)\n",
      "\n",
      "CIFAR-10 tests whether these patterns generalize to:\n",
      "  • Harder task (color images, more complex)\n",
      "  • Same frozen bottleneck constraint (16-D)\n",
      "  • Same architectural comparison (controlled design)\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Block 5: Aggregate Results and Comparative Analysis (CIFAR-10)\n",
    "===============================================================\n",
    "Loads results from Blocks 1-4 and performs comprehensive comparison.\n",
    "\n",
    "Requirements:\n",
    "- realnet_cifar10_results.pt (from Block 1)\n",
    "- quatnet_cifar10_results.pt (from Block 2)\n",
    "- quantum_noent_cifar10_results.pt (from Block 3)\n",
    "- quantum_ent_cifar10_results.pt (from Block 4)\n",
    "\n",
    "Outputs:\n",
    "- Comprehensive comparison tables\n",
    "- Statistical analysis\n",
    "- Research question answers\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def load_results():\n",
    "    \"\"\"Load all results files\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    try:\n",
    "        real_data = torch.load(\"realnet_cifar10_results.pt\", weights_only=False)\n",
    "        results[\"Real\"] = real_data[\"results\"]\n",
    "        print(\"✓ Loaded RealNet (CIFAR-10) results\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"✗ realnet_cifar10_results.pt not found\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        quat_data = torch.load(\"quatnet_cifar10_results.pt\", weights_only=False)\n",
    "        results[\"Quat\"] = quat_data[\"results\"]\n",
    "        print(\"✓ Loaded QuatNet (CIFAR-10) results\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"✗ quatnet_cifar10_results.pt not found\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        qno_data = torch.load(\"quantum_noent_cifar10_results.pt\", weights_only=False)\n",
    "        results[\"QNoEnt\"] = qno_data[\"results\"]\n",
    "        print(\"✓ Loaded Quantum (no ent) (CIFAR-10) results\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"⚠ quantum_noent_cifar10_results.pt not found (skipping)\")\n",
    "        results[\"QNoEnt\"] = []\n",
    "    \n",
    "    try:\n",
    "        qent_data = torch.load(\"quantum_ent_cifar10_results.pt\", weights_only=False)\n",
    "        results[\"QEnt\"] = qent_data[\"results\"]\n",
    "        print(\"✓ Loaded Quantum (with ent) (CIFAR-10) results\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"⚠ quantum_ent_cifar10_results.pt not found (skipping)\")\n",
    "        results[\"QEnt\"] = []\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def print_summary_table(results):\n",
    "    \"\"\"Print aggregated summary table\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(\"AGGREGATED RESULTS (CIFAR-10) (mean ± std over 3 seeds)\")\n",
    "    print(\"=\" * 90)\n",
    "    print(f\"{'Model':<15} {'Accuracy':<20} {'Time (s)':<20} {'Epochs':<15} {'Parameters':<20}\")\n",
    "    print(\"-\" * 90)\n",
    "    \n",
    "    for name in [\"Real\", \"Quat\", \"QNoEnt\", \"QEnt\"]:\n",
    "        if not results[name]:\n",
    "            continue\n",
    "            \n",
    "        accs = [r[\"best_acc\"] for r in results[name]]\n",
    "        times = [r[\"time\"] for r in results[name]]\n",
    "        epochs = [r[\"epochs\"] for r in results[name]]\n",
    "        \n",
    "        acc_str = f\"{np.mean(accs):.4f} ± {np.std(accs):.4f}\"\n",
    "        time_str = f\"{np.mean(times):.1f} ± {np.std(times):.1f}\"\n",
    "        epoch_str = f\"{np.mean(epochs):.1f} ± {np.std(epochs):.1f}\"\n",
    "        \n",
    "        if name == \"Real\":\n",
    "            params = results[name][0][\"params\"]\n",
    "            param_str = f\"{params:,}\"\n",
    "        else:\n",
    "            trainable = results[name][0][\"trainable_params\"]\n",
    "            total = results[name][0][\"total_params\"]\n",
    "            param_str = f\"{trainable:,} (head)\"\n",
    "        \n",
    "        print(f\"{name:<15} {acc_str:<20} {time_str:<20} {epoch_str:<15} {param_str:<20}\")\n",
    "    \n",
    "    print(\"=\" * 90)\n",
    "\n",
    "\n",
    "def print_per_seed_table(results):\n",
    "    \"\"\"Print detailed per-seed results\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"PER-SEED RESULTS (CIFAR-10)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    seeds = [42, 123, 456]\n",
    "    \n",
    "    for seed_idx, seed in enumerate(seeds):\n",
    "        print(f\"\\nSeed {seed}:\")\n",
    "        print(f\"{'Model':<15} {'Accuracy':<12} {'Time (s)':<12} {'Epochs':<10}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for name in [\"Real\", \"Quat\", \"QNoEnt\", \"QEnt\"]:\n",
    "            if not results[name]:\n",
    "                continue\n",
    "            \n",
    "            r = results[name][seed_idx]\n",
    "            print(f\"{name:<15} {r['best_acc']:<12.4f} {r['time']:<12.1f} {r['epochs']:<10}\")\n",
    "\n",
    "\n",
    "def comparative_analysis(results):\n",
    "    \"\"\"Perform comparative analysis answering research questions\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(\"COMPARATIVE ANALYSIS (CIFAR-10): Answering Research Questions\")\n",
    "    print(\"=\" * 90)\n",
    "    \n",
    "    real_accs = [r[\"best_acc\"] for r in results[\"Real\"]]\n",
    "    quat_accs = [r[\"best_acc\"] for r in results[\"Quat\"]]\n",
    "    \n",
    "    def pct_gap(a, b):\n",
    "        \"\"\"Percentage point gap (a - b)\"\"\"\n",
    "        return (np.mean(a) - np.mean(b)) * 100.0\n",
    "    \n",
    "    def retention(a, b):\n",
    "        \"\"\"Percentage of performance retained\"\"\"\n",
    "        return (np.mean(a) / np.mean(b)) * 100.0\n",
    "    \n",
    "    print(\"\\n1. QUATERNION vs REAL MLP:\")\n",
    "    print(\"   \" + \"-\" * 70)\n",
    "    print(f\"   Real MLP accuracy:        {np.mean(real_accs):.4f} ± {np.std(real_accs):.4f}\")\n",
    "    print(f\"   Quaternion accuracy:      {np.mean(quat_accs):.4f} ± {np.std(quat_accs):.4f}\")\n",
    "    print(f\"   Gap:                      {pct_gap(real_accs, quat_accs):.2f} percentage points\")\n",
    "    print(f\"   Performance retention:    {retention(quat_accs, real_accs):.1f}%\")\n",
    "    print(f\"\\n   → Classical SU(2) (quaternions) captures {retention(quat_accs, real_accs):.1f}% of\")\n",
    "    print(f\"     standard MLP performance with structured algebraic constraints on CIFAR-10.\")\n",
    "    \n",
    "    if results[\"QNoEnt\"]:\n",
    "        qno_accs = [r[\"best_acc\"] for r in results[\"QNoEnt\"]]\n",
    "        \n",
    "        print(\"\\n2. QUANTUM (no entanglement) vs REAL MLP:\")\n",
    "        print(\"   \" + \"-\" * 70)\n",
    "        print(f\"   Real MLP accuracy:        {np.mean(real_accs):.4f} ± {np.std(real_accs):.4f}\")\n",
    "        print(f\"   Quantum (no ent) accuracy:{np.mean(qno_accs):.4f} ± {np.std(qno_accs):.4f}\")\n",
    "        print(f\"   Gap:                      {pct_gap(real_accs, qno_accs):.2f} percentage points\")\n",
    "        print(f\"   Performance retention:    {retention(qno_accs, real_accs):.1f}%\")\n",
    "        print(f\"\\n   → Quantum circuits WITHOUT entanglement capture {retention(qno_accs, real_accs):.1f}%\")\n",
    "        print(f\"     of MLP performance, suggesting limited benefit over classical rotation gates\")\n",
    "        print(f\"     on CIFAR-10's color images.\")\n",
    "    \n",
    "    if results[\"QEnt\"]:\n",
    "        qent_accs = [r[\"best_acc\"] for r in results[\"QEnt\"]]\n",
    "        \n",
    "        print(\"\\n3. QUANTUM (with entanglement) vs REAL MLP:\")\n",
    "        print(\"   \" + \"-\" * 70)\n",
    "        print(f\"   Real MLP accuracy:        {np.mean(real_accs):.4f} ± {np.std(real_accs):.4f}\")\n",
    "        print(f\"   Quantum (with ent) accuracy:{np.mean(qent_accs):.4f} ± {np.std(qent_accs):.4f}\")\n",
    "        print(f\"   Gap:                      {pct_gap(real_accs, qent_accs):.2f} percentage points\")\n",
    "        print(f\"   Performance retention:    {retention(qent_accs, real_accs):.1f}%\")\n",
    "        print(f\"\\n   → Quantum circuits WITH entanglement capture {retention(qent_accs, real_accs):.1f}%\")\n",
    "        print(f\"     of MLP performance on CIFAR-10.\")\n",
    "    \n",
    "    if results[\"QNoEnt\"] and results[\"QEnt\"]:\n",
    "        print(\"\\n4. ENTANGLEMENT EFFECT:\")\n",
    "        print(\"   \" + \"-\" * 70)\n",
    "        print(f\"   Quantum (no ent) accuracy:{np.mean(qno_accs):.4f} ± {np.std(qno_accs):.4f}\")\n",
    "        print(f\"   Quantum (with ent) accuracy:{np.mean(qent_accs):.4f} ± {np.std(qent_accs):.4f}\")\n",
    "        print(f\"   Improvement:              {pct_gap(qent_accs, qno_accs):.2f} percentage points\")\n",
    "        \n",
    "        if np.mean(qent_accs) > np.mean(qno_accs):\n",
    "            improvement_pct = ((np.mean(qent_accs) - np.mean(qno_accs)) / np.mean(qno_accs)) * 100\n",
    "            print(f\"\\n   → Entanglement IMPROVES performance by {improvement_pct:.1f}%\")\n",
    "            print(f\"     (relative improvement over non-entangled baseline)\")\n",
    "            print(f\"     → Pattern consistent with paper findings (~0.6pp gain on FashionMNIST)\")\n",
    "        else:\n",
    "            print(f\"\\n   → Entanglement DOES NOT improve performance on CIFAR-10\")\n",
    "            print(f\"     → Task difficulty may mask entanglement benefit\")\n",
    "    \n",
    "    if results[\"QNoEnt\"]:\n",
    "        print(\"\\n5. QUATERNION vs QUANTUM (no entanglement) - Core Research Question:\")\n",
    "        print(\"   \" + \"-\" * 70)\n",
    "        print(f\"   Quaternion accuracy:      {np.mean(quat_accs):.4f} ± {np.std(quat_accs):.4f}\")\n",
    "        print(f\"   Quantum (no ent) accuracy:{np.mean(qno_accs):.4f} ± {np.std(qno_accs):.4f}\")\n",
    "        print(f\"   Gap:                      {pct_gap(quat_accs, qno_accs):.2f} percentage points\")\n",
    "        \n",
    "        gap_abs = abs(pct_gap(quat_accs, qno_accs))\n",
    "        if gap_abs < 2.0:\n",
    "            print(f\"\\n   → Quaternion networks (classical SU(2)) CLOSELY APPROXIMATE quantum circuits\")\n",
    "            print(f\"     without entanglement (quantum SU(2)). Gap < 2 percentage points.\")\n",
    "            print(f\"\\n   → This supports the hypothesis that classical quaternion algebra can serve\")\n",
    "            print(f\"     as an effective implementation of SU(2) geometry on CIFAR-10.\")\n",
    "        elif np.mean(quat_accs) > np.mean(qno_accs):\n",
    "            print(f\"\\n   → Quaternion networks OUTPERFORM quantum circuits without entanglement\")\n",
    "            print(f\"     by {gap_abs:.2f} percentage points.\")\n",
    "            print(f\"\\n   → Classical SU(2) (quaternions) SUFFICES for learning tasks addressable\")\n",
    "            print(f\"     by product-state quantum circuits on CIFAR-10, consistent with\")\n",
    "            print(f\"     the paper's findings on FashionMNIST (~6pp advantage).\")\n",
    "        else:\n",
    "            print(f\"\\n   → Quantum circuits without entanglement OUTPERFORM quaternions\")\n",
    "            print(f\"     by {gap_abs:.2f} percentage points on CIFAR-10.\")\n",
    "    \n",
    "    if results[\"QEnt\"]:\n",
    "        print(\"\\n6. QUATERNION vs QUANTUM (with entanglement):\")\n",
    "        print(\"   \" + \"-\" * 70)\n",
    "        print(f\"   Quaternion accuracy:      {np.mean(quat_accs):.4f} ± {np.std(quat_accs):.4f}\")\n",
    "        print(f\"   Quantum (with ent) accuracy:{np.mean(qent_accs):.4f} ± {np.std(qent_accs):.4f}\")\n",
    "        print(f\"   Gap:                      {pct_gap(quat_accs, qent_accs):.2f} percentage points\")\n",
    "        \n",
    "        if np.mean(qent_accs) > np.mean(quat_accs):\n",
    "            print(f\"\\n   → Entangled quantum circuits outperform quaternions, demonstrating\")\n",
    "            print(f\"     the value of quantum correlations beyond classical SU(2) rotations.\")\n",
    "        else:\n",
    "            print(f\"\\n   → Quaternions match or exceed entangled quantum performance on CIFAR-10,\")\n",
    "            print(f\"     suggesting entanglement provides limited benefit on this harder task.\")\n",
    "            print(f\"     → Measurement loss + optimization challenges may dominate on complex data.\")\n",
    "\n",
    "\n",
    "def key_takeaways(results):\n",
    "    \"\"\"Summarize key takeaways for paper\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(\"KEY TAKEAWAYS FOR PAPER (CIFAR-10)\")\n",
    "    print(\"=\" * 90)\n",
    "    \n",
    "    real_accs = [r[\"best_acc\"] for r in results[\"Real\"]]\n",
    "    quat_accs = [r[\"best_acc\"] for r in results[\"Quat\"]]\n",
    "    \n",
    "    def retention(a, b):\n",
    "        return (np.mean(a) / np.mean(b)) * 100.0\n",
    "    \n",
    "    print(\"\\n✓ EMPIRICAL FINDINGS:\")\n",
    "    print(f\"  1. Quaternion networks achieve {retention(quat_accs, real_accs):.1f}% of Real MLP performance\")\n",
    "    print(f\"     with structured SU(2) algebraic constraints on CIFAR-10\")\n",
    "    print(f\"     → Note: Lower absolute accuracy (~{np.mean(quat_accs)*100:.1f}%) reflects\")\n",
    "    print(f\"       16-D bottleneck limitation, not quaternion geometry weakness\")\n",
    "    \n",
    "    if results[\"QNoEnt\"]:\n",
    "        qno_accs = [r[\"best_acc\"] for r in results[\"QNoEnt\"]]\n",
    "        gap = abs((np.mean(quat_accs) - np.mean(qno_accs)) * 100.0)\n",
    "        print(f\"\\n  2. Quaternions vs Quantum (no ent): {gap:.2f} percentage point gap\")\n",
    "        if gap < 2.0:\n",
    "            print(f\"     → Classical SU(2) effectively approximates quantum SU(2) without entanglement\")\n",
    "        elif np.mean(quat_accs) > np.mean(qno_accs):\n",
    "            print(f\"     → Classical SU(2) OUTPERFORMS quantum product-state circuits\")\n",
    "            print(f\"       (consistent with paper: quaternions >> quantum on FashionMNIST)\")\n",
    "        else:\n",
    "            print(f\"     → Quantum product-state circuits outperform classical SU(2)\")\n",
    "            print(f\"       (different pattern from FashionMNIST)\")\n",
    "    \n",
    "    if results[\"QEnt\"] and results[\"QNoEnt\"]:\n",
    "        qent_accs = [r[\"best_acc\"] for r in results[\"QEnt\"]]\n",
    "        qno_accs = [r[\"best_acc\"] for r in results[\"QNoEnt\"]]\n",
    "        ent_improvement = ((np.mean(qent_accs) - np.mean(qno_accs)) / np.mean(qno_accs)) * 100\n",
    "        if ent_improvement > 0.5:\n",
    "            print(f\"\\n  3. Entanglement improves quantum performance by {ent_improvement:.1f}%\")\n",
    "            print(f\"     → Demonstrates measurable value of quantum correlations on CIFAR-10\")\n",
    "            print(f\"     → Pattern consistent with paper (~0.6pp gain on FashionMNIST)\")\n",
    "        else:\n",
    "            print(f\"\\n  3. Entanglement provides minimal/no benefit ({ent_improvement:.1f}%) on CIFAR-10\")\n",
    "            print(f\"     → Task difficulty may mask entanglement advantage\")\n",
    "    \n",
    "    print(\"\\n✓ IMPLICATIONS:\")\n",
    "    print(\"  • CIFAR-10 results test generalization of paper's findings to harder tasks\")\n",
    "    print(\"  • If quaternions still outperform quantum (like FashionMNIST):\")\n",
    "    print(\"    → Strengthens claim that classical SU(2) suffices for product-state VQCs\")\n",
    "    print(\"  • If entanglement benefit persists (~0.5-1pp):\")\n",
    "    print(\"    → Confirms entanglement as quantum boundary across task difficulties\")\n",
    "    print(\"  • Lower absolute accuracies (~30-40%) reflect bottleneck constraint,\")\n",
    "    print(\"    NOT failure of any specific architecture\")\n",
    "    \n",
    "    print(\"\\n✓ COMPUTATIONAL EFFICIENCY:\")\n",
    "    real_time = np.mean([r[\"time\"] for r in results[\"Real\"]])\n",
    "    quat_time = np.mean([r[\"time\"] for r in results[\"Quat\"]])\n",
    "    print(f\"  • Real MLP:  {real_time:.1f}s (baseline)\")\n",
    "    print(f\"  • Quaternion: {quat_time:.1f}s ({quat_time/real_time:.2f}x Real)\")\n",
    "    \n",
    "    if results[\"QNoEnt\"]:\n",
    "        qno_time = np.mean([r[\"time\"] for r in results[\"QNoEnt\"]])\n",
    "        print(f\"  • Quantum (no ent): {qno_time:.1f}s ({qno_time/real_time:.1f}x Real)\")\n",
    "        print(f\"    → Even with lightning.gpu, quantum is {qno_time/quat_time:.1f}x slower than quaternions\")\n",
    "        print(f\"    → Pattern consistent across datasets (FashionMNIST: ~500x, CIFAR-10: ~{qno_time/quat_time:.0f}x)\")\n",
    "    \n",
    "    print(\"\\n✓ CONTROLLED EXPERIMENTAL DESIGN:\")\n",
    "    print(\"  • All models use IDENTICAL frozen 3072→16 preprocessor\")\n",
    "    print(\"  • Performance differences reflect HEAD ARCHITECTURE ONLY\")\n",
    "    print(\"  • Validates paper's methodology on second dataset (CIFAR-10)\")\n",
    "    \n",
    "    print(\"=\" * 90)\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 90)\n",
    "    print(\"BLOCK 5: AGGREGATE RESULTS AND COMPARATIVE ANALYSIS (CIFAR-10)\")\n",
    "    print(\"=\" * 90)\n",
    "    \n",
    "    results = load_results()\n",
    "    \n",
    "    if results is None:\n",
    "        print(\"\\n✗ ERROR: Required results files not found.\")\n",
    "        print(\"Run Blocks 1 and 2 at minimum (Real and Quat) on CIFAR-10.\")\n",
    "        return\n",
    "    \n",
    "    # Summary table\n",
    "    print_summary_table(results)\n",
    "    \n",
    "    # Per-seed details\n",
    "    print_per_seed_table(results)\n",
    "    \n",
    "    # Comparative analysis\n",
    "    comparative_analysis(results)\n",
    "    \n",
    "    # Key takeaways\n",
    "    key_takeaways(results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(\"ANALYSIS COMPLETE (CIFAR-10)\")\n",
    "    print(\"=\" * 90)\n",
    "    print(\"\\nAll results saved in:\")\n",
    "    print(\"  • realnet_cifar10_results.pt\")\n",
    "    print(\"  • quatnet_cifar10_results.pt\")\n",
    "    if results[\"QNoEnt\"]:\n",
    "        print(\"  • quantum_noent_cifar10_results.pt\")\n",
    "    if results[\"QEnt\"]:\n",
    "        print(\"  • quantum_ent_cifar10_results.pt\")\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(\"COMPARISON TO PAPER (FashionMNIST)\")\n",
    "    print(\"=\" * 90)\n",
    "    print(\"\\nPaper's key findings on FashionMNIST:\")\n",
    "    print(\"  • Quaternions ≈ Real MLP (within 0.2pp)\")\n",
    "    print(\"  • Quaternions >> Quantum-NoEnt (by ~2.4pp)\")\n",
    "    print(\"  • Entanglement helps modestly (0.6pp gain)\")\n",
    "    print(\"\\nCIFAR-10 tests whether these patterns generalize to:\")\n",
    "    print(\"  • Harder task (color images, more complex)\")\n",
    "    print(\"  • Same frozen bottleneck constraint (16-D)\")\n",
    "    print(\"  • Same architectural comparison (controlled design)\")\n",
    "    print(\"=\" * 90)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3e3e5c-7bff-46c7-a0a7-91a313e86b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch-env-clone)",
   "language": "python",
   "name": "torch-env-clone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
