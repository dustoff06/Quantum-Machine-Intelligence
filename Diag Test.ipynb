{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04995331-5941-4864-aa60-87cfbe32bdda",
   "metadata": {},
   "source": [
    "# RealNet MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0d604c6-4390-4022-81c1-4d1568d54146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch device: cuda\n",
      "======================================================================\n",
      "BLOCK 1: RealNet Training\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "  • Batch size: 32\n",
      "  • Patience: 10\n",
      "  • Seeds: [42, 123, 456]\n",
      "  • Architecture: 784 → 16 → 64 → 10\n",
      "======================================================================\n",
      "\n",
      "Creating stratified samples...\n",
      "  Train samples: 15000 (stratified, 1500 per class)\n",
      "  Test samples:  3000 (stratified, 300 per class)\n",
      "\n",
      "======================================================================\n",
      "SEED 42\n",
      "======================================================================\n",
      "\n",
      "  Training RealNet (seed=42)...\n",
      "  [Real] Epoch  1 | loss=0.7188 | test_acc=0.8977 | time=1.3s\n",
      "  [Real] Epoch  2 | loss=0.2981 | test_acc=0.9090 | time=2.6s\n",
      "  [Real] Epoch  3 | loss=0.2502 | test_acc=0.9157 | time=4.0s\n",
      "  [Real] Epoch  4 | loss=0.2143 | test_acc=0.9173 | time=5.2s\n",
      "  [Real] Epoch  5 | loss=0.1910 | test_acc=0.9293 | time=6.5s\n",
      "  [Real] Epoch  6 | loss=0.1768 | test_acc=0.9210 | time=7.9s\n",
      "  [Real] Epoch  7 | loss=0.1631 | test_acc=0.9253 | time=9.2s\n",
      "  [Real] Epoch  8 | loss=0.1486 | test_acc=0.9290 | time=10.6s\n",
      "  [Real] Epoch  9 | loss=0.1428 | test_acc=0.9260 | time=11.9s\n",
      "  [Real] Epoch 10 | loss=0.1289 | test_acc=0.9250 | time=13.2s\n",
      "  [Real] Epoch 11 | loss=0.1239 | test_acc=0.9237 | time=14.5s\n",
      "  [Real] Epoch 12 | loss=0.1204 | test_acc=0.9270 | time=15.8s\n",
      "  [Real] Epoch 13 | loss=0.1106 | test_acc=0.9193 | time=17.1s\n",
      "  [Real] Epoch 14 | loss=0.1045 | test_acc=0.9237 | time=18.4s\n",
      "  [Real] Epoch 15 | loss=0.1019 | test_acc=0.9267 | time=19.6s\n",
      "  [Real] Early stop at epoch 15 (no improvement for 10 epochs)\n",
      "\n",
      "  → Saved preprocessor state from seed 42\n",
      "\n",
      "======================================================================\n",
      "SEED 123\n",
      "======================================================================\n",
      "\n",
      "  Training RealNet (seed=123)...\n",
      "  [Real] Epoch  1 | loss=0.7262 | test_acc=0.9087 | time=1.3s\n",
      "  [Real] Epoch  2 | loss=0.2878 | test_acc=0.9247 | time=2.6s\n",
      "  [Real] Epoch  3 | loss=0.2327 | test_acc=0.9323 | time=3.9s\n",
      "  [Real] Epoch  4 | loss=0.2044 | test_acc=0.9287 | time=5.2s\n",
      "  [Real] Epoch  5 | loss=0.1806 | test_acc=0.9290 | time=6.5s\n",
      "  [Real] Epoch  6 | loss=0.1628 | test_acc=0.9370 | time=7.8s\n",
      "  [Real] Epoch  7 | loss=0.1496 | test_acc=0.9337 | time=9.1s\n",
      "  [Real] Epoch  8 | loss=0.1420 | test_acc=0.9387 | time=10.3s\n",
      "  [Real] Epoch  9 | loss=0.1306 | test_acc=0.9337 | time=11.6s\n",
      "  [Real] Epoch 10 | loss=0.1194 | test_acc=0.9333 | time=12.8s\n",
      "  [Real] Epoch 11 | loss=0.1158 | test_acc=0.9313 | time=14.2s\n",
      "  [Real] Epoch 12 | loss=0.1122 | test_acc=0.9347 | time=15.6s\n",
      "  [Real] Epoch 13 | loss=0.1006 | test_acc=0.9360 | time=16.9s\n",
      "  [Real] Epoch 14 | loss=0.0952 | test_acc=0.9420 | time=18.3s\n",
      "  [Real] Epoch 15 | loss=0.0905 | test_acc=0.9273 | time=19.7s\n",
      "  [Real] Epoch 16 | loss=0.0869 | test_acc=0.9327 | time=21.1s\n",
      "  [Real] Epoch 17 | loss=0.0821 | test_acc=0.9387 | time=22.5s\n",
      "  [Real] Epoch 18 | loss=0.0840 | test_acc=0.9387 | time=23.9s\n",
      "  [Real] Epoch 19 | loss=0.0712 | test_acc=0.9330 | time=25.2s\n",
      "  [Real] Epoch 20 | loss=0.0794 | test_acc=0.9343 | time=26.6s\n",
      "  [Real] Epoch 21 | loss=0.0744 | test_acc=0.9363 | time=28.0s\n",
      "  [Real] Epoch 22 | loss=0.0712 | test_acc=0.9357 | time=29.3s\n",
      "  [Real] Epoch 23 | loss=0.0612 | test_acc=0.9367 | time=30.7s\n",
      "  [Real] Epoch 24 | loss=0.0595 | test_acc=0.9273 | time=32.1s\n",
      "  [Real] Early stop at epoch 24 (no improvement for 10 epochs)\n",
      "\n",
      "======================================================================\n",
      "SEED 456\n",
      "======================================================================\n",
      "\n",
      "  Training RealNet (seed=456)...\n",
      "  [Real] Epoch  1 | loss=0.7188 | test_acc=0.9053 | time=1.5s\n",
      "  [Real] Epoch  2 | loss=0.3021 | test_acc=0.9193 | time=2.8s\n",
      "  [Real] Epoch  3 | loss=0.2458 | test_acc=0.9263 | time=4.2s\n",
      "  [Real] Epoch  4 | loss=0.2096 | test_acc=0.9207 | time=5.6s\n",
      "  [Real] Epoch  5 | loss=0.1902 | test_acc=0.9230 | time=7.0s\n",
      "  [Real] Epoch  6 | loss=0.1754 | test_acc=0.9287 | time=8.3s\n",
      "  [Real] Epoch  7 | loss=0.1599 | test_acc=0.9293 | time=9.7s\n",
      "  [Real] Epoch  8 | loss=0.1456 | test_acc=0.9327 | time=11.0s\n",
      "  [Real] Epoch  9 | loss=0.1389 | test_acc=0.9240 | time=12.4s\n",
      "  [Real] Epoch 10 | loss=0.1278 | test_acc=0.9267 | time=13.8s\n",
      "  [Real] Epoch 11 | loss=0.1196 | test_acc=0.9240 | time=15.1s\n",
      "  [Real] Epoch 12 | loss=0.1129 | test_acc=0.9267 | time=16.4s\n",
      "  [Real] Epoch 13 | loss=0.1103 | test_acc=0.9233 | time=17.7s\n",
      "  [Real] Epoch 14 | loss=0.1083 | test_acc=0.9247 | time=19.1s\n",
      "  [Real] Epoch 15 | loss=0.1050 | test_acc=0.9350 | time=20.4s\n",
      "  [Real] Epoch 16 | loss=0.0894 | test_acc=0.9340 | time=21.7s\n",
      "  [Real] Epoch 17 | loss=0.0888 | test_acc=0.9263 | time=23.0s\n",
      "  [Real] Epoch 18 | loss=0.0921 | test_acc=0.9317 | time=24.3s\n",
      "  [Real] Epoch 19 | loss=0.0816 | test_acc=0.9270 | time=25.6s\n",
      "  [Real] Epoch 20 | loss=0.0793 | test_acc=0.9347 | time=26.9s\n",
      "  [Real] Epoch 21 | loss=0.0739 | test_acc=0.9277 | time=28.3s\n",
      "  [Real] Epoch 22 | loss=0.0765 | test_acc=0.9317 | time=29.6s\n",
      "  [Real] Epoch 23 | loss=0.0713 | test_acc=0.9250 | time=30.9s\n",
      "  [Real] Epoch 24 | loss=0.0655 | test_acc=0.9290 | time=32.3s\n",
      "  [Real] Epoch 25 | loss=0.0633 | test_acc=0.9297 | time=33.7s\n",
      "  [Real] Early stop at epoch 25 (no improvement for 10 epochs)\n",
      "\n",
      "======================================================================\n",
      "REALNET SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Accuracy:   0.9354 ± 0.0052\n",
      "Time:       28.5s ± 6.3s\n",
      "Epochs:     21.3 ± 4.5\n",
      "Parameters: 14,298\n",
      "\n",
      "Per-seed results:\n",
      "  Seed 42: acc=0.9293, time=19.6s, epochs=15\n",
      "  Seed 123: acc=0.9420, time=32.1s, epochs=24\n",
      "  Seed 456: acc=0.9350, time=33.7s, epochs=25\n",
      "\n",
      "✓ Saved results to: realnet_results.pt\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Block 1: RealNet Training\n",
    "==========================\n",
    "Trains the baseline Real MLP on 3 seeds.\n",
    "Saves the trained preprocessor for reuse in subsequent blocks.\n",
    "\n",
    "Outputs:\n",
    "- realnet_results.pt: Contains results dict and trained preprocessor state\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from collections import defaultdict\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "print(f\"Using PyTorch device: {device}\")\n",
    "\n",
    "# ============================================\n",
    "# Comprehensive seed setting for reproducibility\n",
    "# ============================================\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    \"\"\"Set seeds for all RNG sources for reproducibility\"\"\"\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    # Python\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # NumPy\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # PyTorch deterministic mode\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # CuPy (if available)\n",
    "    try:\n",
    "        import cupy as cp\n",
    "        cp.random.seed(seed)\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    # PennyLane (if available)\n",
    "    try:\n",
    "        import pennylane as qml\n",
    "        qml.numpy.random.seed(seed)\n",
    "    except (ImportError, AttributeError):\n",
    "        pass\n",
    "\n",
    "# ============================================\n",
    "# Shared Bottleneck Preprocessor: 784 → 16\n",
    "# ============================================\n",
    "\n",
    "class SharedPreprocessor(nn.Module):\n",
    "    \"\"\"Shared classical feature extractor: 784 → 16\"\"\"\n",
    "    def __init__(self, input_dim=784, bottleneck_dim=16):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, bottleneck_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.tanh(self.fc(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Real-valued Head and Network\n",
    "# ============================================\n",
    "\n",
    "class RealHead(nn.Module):\n",
    "    \"\"\"Standard MLP: 16 → 64 → 10\"\"\"\n",
    "    def __init__(self, bottleneck_dim=16, hidden_dim=64, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(bottleneck_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RealNet(nn.Module):\n",
    "    \"\"\"Complete Real network: Preprocessor + RealHead\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.preprocessor = SharedPreprocessor(784, 16)\n",
    "        self.head = RealHead(16, 64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.preprocessor(x)\n",
    "        return self.head(features)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Training and Evaluation\n",
    "# ============================================\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, device, show_progress=False):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        total_samples += x.size(0)\n",
    "\n",
    "        if show_progress and batch_idx % 50 == 0:\n",
    "            print(f\"    Batch {batch_idx}/{len(loader)}, samples: {total_samples}\", end=\"\\r\")\n",
    "\n",
    "    if show_progress:\n",
    "        print()\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += x.size(0)\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "def train_with_early_stopping(model, train_loader, test_loader, optimizer,\n",
    "                              device, max_epochs=40, patience=10, name=\"Model\"):\n",
    "    if isinstance(device, str):\n",
    "        device = torch.device(device)\n",
    "\n",
    "    best_acc = 0.0\n",
    "    epochs_without_improvement = 0\n",
    "    start = time.time()\n",
    "    last_acc = 0.0\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        loss = train_one_epoch(model, train_loader, optimizer, device, show_progress=False)\n",
    "        acc = evaluate(model, test_loader, device)\n",
    "        last_acc = acc\n",
    "\n",
    "        elapsed = time.time() - start\n",
    "        print(f\"  [{name}] Epoch {epoch:2d} | loss={loss:.4f} \"\n",
    "              f\"| test_acc={acc:.4f} | time={elapsed:.1f}s\")\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"  [{name}] Early stop at epoch {epoch} \"\n",
    "                  f\"(no improvement for {patience} epochs)\")\n",
    "            break\n",
    "\n",
    "    total_time = time.time() - start\n",
    "    return {\n",
    "        \"best_acc\": best_acc,\n",
    "        \"final_acc\": last_acc,\n",
    "        \"time\": total_time,\n",
    "        \"epochs\": epoch,\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Data utilities\n",
    "# ============================================\n",
    "\n",
    "def stratified_sample(dataset, n_samples_per_class):\n",
    "    \"\"\"\n",
    "    Create a stratified sample with n_samples_per_class from each class.\n",
    "    Maintains class balance.\n",
    "    \"\"\"\n",
    "    # Group indices by class\n",
    "    class_indices = defaultdict(list)\n",
    "    for idx, (_, label) in enumerate(dataset):\n",
    "        class_indices[label].append(idx)\n",
    "    \n",
    "    # Sample from each class\n",
    "    sampled_indices = []\n",
    "    for class_label in sorted(class_indices.keys()):\n",
    "        indices = class_indices[class_label]\n",
    "        # Use fixed random seed for reproducibility\n",
    "        rng = np.random.RandomState(42)\n",
    "        selected = rng.choice(indices, size=min(n_samples_per_class, len(indices)), \n",
    "                             replace=False)\n",
    "        sampled_indices.extend(selected)\n",
    "    \n",
    "    return sampled_indices\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Main\n",
    "# ============================================\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"BLOCK 1: RealNet Training\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nConfiguration:\")\n",
    "    print(\"  • Batch size: 32\")\n",
    "    print(\"  • Patience: 10\")\n",
    "    print(\"  • Seeds: [42, 123, 456]\")\n",
    "    print(\"  • Architecture: 784 → 16 → 64 → 10\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    # Load full datasets\n",
    "    full_train_ds = datasets.MNIST(root=\"./data\", train=True,\n",
    "                                   download=True, transform=transform)\n",
    "    full_test_ds = datasets.MNIST(root=\"./data\", train=False,\n",
    "                                  download=True, transform=transform)\n",
    "\n",
    "    # Create stratified samples\n",
    "    print(\"\\nCreating stratified samples...\")\n",
    "    train_indices = stratified_sample(full_train_ds, n_samples_per_class=1500)  # 15K total\n",
    "    test_indices = stratified_sample(full_test_ds, n_samples_per_class=300)     # 3K total\n",
    "    \n",
    "    train_ds = Subset(full_train_ds, train_indices)\n",
    "    test_ds = Subset(full_test_ds, test_indices)\n",
    "    \n",
    "    print(f\"  Train samples: {len(train_ds)} (stratified, 1500 per class)\")\n",
    "    print(f\"  Test samples:  {len(test_ds)} (stratified, 300 per class)\")\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=32,\n",
    "                              shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_ds, batch_size=64,\n",
    "                             shuffle=False, num_workers=0)\n",
    "\n",
    "    seeds = [42, 123, 456]\n",
    "    all_results = []\n",
    "    trained_preprocessor_state = None\n",
    "\n",
    "    for seed in seeds:\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(f\"SEED {seed}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        set_all_seeds(seed)\n",
    "\n",
    "        print(f\"\\n  Training RealNet (seed={seed})...\")\n",
    "        real_model = RealNet().to(device)\n",
    "        real_opt = torch.optim.Adam(real_model.parameters(), lr=1e-3)\n",
    "        \n",
    "        result = train_with_early_stopping(\n",
    "            real_model, train_loader, test_loader, real_opt, device,\n",
    "            max_epochs=40, patience=10, name=\"Real\"\n",
    "        )\n",
    "        result[\"params\"] = sum(p.numel() for p in real_model.parameters())\n",
    "        result[\"seed\"] = seed\n",
    "        \n",
    "        all_results.append(result)\n",
    "        \n",
    "        # Save preprocessor from first seed\n",
    "        if trained_preprocessor_state is None:\n",
    "            trained_preprocessor_state = real_model.preprocessor.state_dict()\n",
    "            print(f\"\\n  → Saved preprocessor state from seed {seed}\")\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"REALNET SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    accs = [r[\"best_acc\"] for r in all_results]\n",
    "    times = [r[\"time\"] for r in all_results]\n",
    "    epochs = [r[\"epochs\"] for r in all_results]\n",
    "    params = all_results[0][\"params\"]\n",
    "    \n",
    "    print(f\"\\nAccuracy:   {np.mean(accs):.4f} ± {np.std(accs):.4f}\")\n",
    "    print(f\"Time:       {np.mean(times):.1f}s ± {np.std(times):.1f}s\")\n",
    "    print(f\"Epochs:     {np.mean(epochs):.1f} ± {np.std(epochs):.1f}\")\n",
    "    print(f\"Parameters: {params:,}\")\n",
    "    \n",
    "    print(\"\\nPer-seed results:\")\n",
    "    for r in all_results:\n",
    "        print(f\"  Seed {r['seed']}: acc={r['best_acc']:.4f}, \"\n",
    "              f\"time={r['time']:.1f}s, epochs={r['epochs']}\")\n",
    "\n",
    "    # Save results and preprocessor\n",
    "    save_dict = {\n",
    "        \"results\": all_results,\n",
    "        \"preprocessor_state\": trained_preprocessor_state,\n",
    "        \"summary\": {\n",
    "            \"mean_acc\": np.mean(accs),\n",
    "            \"std_acc\": np.std(accs),\n",
    "            \"mean_time\": np.mean(times),\n",
    "            \"params\": params\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    torch.save(save_dict, \"realnet_results.pt\")\n",
    "    print(f\"\\n✓ Saved results to: realnet_results.pt\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d828c706-896f-4266-b262-3fc97bfeb308",
   "metadata": {},
   "source": [
    "# Diagnostic MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "340c152b-476c-42c1-bb0d-1e4c8990cbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[device] backend=lightning.gpu, wires=5 (aux_wire=4)\n",
      "\n",
      "[Euclid vs Adam vs FS-NG] DATASET=MNIST | backend=lightning.gpu\n",
      "  layers=2 | batch=8 | minibatches=10 | steps=10\n",
      "  ETA=0.01 | LAM=0.001 | alpha_shrink=1.0 | Adam(lr=0.001, betas=(0.9, 0.999))\n",
      "\n",
      "\n",
      "======================================================================\n",
      "SEED 42\n",
      "======================================================================\n",
      "  minibatch  1/10: full_lossK=2.2681 | diag_lossK=2.2690 | adam_lossK=2.2700\n",
      "  minibatch 10/10: full_lossK=2.3659 | diag_lossK=2.3660 | adam_lossK=2.3659\n",
      "\n",
      "======================================================================\n",
      "SEED 123\n",
      "======================================================================\n",
      "  minibatch  1/10: full_lossK=2.2654 | diag_lossK=2.2655 | adam_lossK=2.2655\n",
      "  minibatch 10/10: full_lossK=2.3312 | diag_lossK=2.3310 | adam_lossK=2.3307\n",
      "\n",
      "======================================================================\n",
      "SEED 456\n",
      "======================================================================\n",
      "  minibatch  1/10: full_lossK=2.2671 | diag_lossK=2.2671 | adam_lossK=2.2668\n",
      "  minibatch 10/10: full_lossK=2.3362 | diag_lossK=2.3362 | adam_lossK=2.3360\n",
      "\n",
      "======================================================================\n",
      "AGGREGATE RESULTS (across 3 seeds × 10 minibatches = 30 runs per method)\n",
      "======================================================================\n",
      "  euclid   : final_loss=2.3163 ± 0.0389 | time=6.04s ± 0.36s | cos(step1 vs full)=0.940 ± 0.047\n",
      "  adam     : final_loss=2.3154 ± 0.0389 | time=5.97s ± 0.40s | cos(step1 vs full)=0.728 ± 0.073\n",
      "  diag     : final_loss=2.3155 ± 0.0389 | time=7.33s ± 0.59s | cos(step1 vs full)=0.961 ± 0.030\n",
      "  block-diag: final_loss=2.3155 ± 0.0389 | time=7.44s ± 0.62s | cos(step1 vs full)=0.961 ± 0.030\n",
      "  full     : final_loss=2.3154 ± 0.0389 | time=18.11s ± 1.16s | cos(step1 vs full)=1.000\n",
      "\n",
      "Notes:\n",
      "  • Cosines compare the *applied step vector* at step 1 (not the raw gradient).\n",
      "  • Adam’s delta is its actual applied update on θ (per minibatch, fresh state).\n",
      "  • If you want Adam to carry momentum across minibatches, move AdamState outside run_one_minibatch().\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Mini-script: Euclid vs Adam vs FS-NG covariance test (euclid vs adam vs diag vs block-diag vs full)\n",
    "=================================================================================================\n",
    "\n",
    "What this does\n",
    "--------------\n",
    "Runs K update steps on *multiple fixed minibatches* (10 minibatches) for each of 3 seeds,\n",
    "updating ONLY the circuit weights θ, while holding the preprocessor + feature mapping + readout fixed.\n",
    "\n",
    "We compare five update rules:\n",
    "\n",
    "  1) euclid     : plain Euclidean gradient descent        θ <- θ - η * g\n",
    "  2) adam       : Adam optimizer on θ (PyTorch-style)     θ <- AdamStep(g)\n",
    "  3) diag       : FS/QFI natural gradient (metric_tensor approx=\"diag\")\n",
    "  4) block-diag : FS/QFI natural gradient (approx=\"block-diag\")\n",
    "  5) full       : FS/QFI natural gradient (approx=None, needs aux wire)\n",
    "\n",
    "Optional hybrid (“shrinkage” FS metric toward Euclidean)\n",
    "--------------------------------------------------------\n",
    "Instead of pure FS-NG, you can blend the metric toward identity:\n",
    "\n",
    "    G_eff = α * G_bar + (1-α) * I\n",
    "    step  = (G_eff + λ I)^(-1) g\n",
    "\n",
    "Set ALPHA_SHRINK in (0,1]. If you want *pure* FS-NG, set ALPHA_SHRINK=1.0.\n",
    "\n",
    "Goal\n",
    "----\n",
    "- See whether FS covariance structure helps over Euclidean GD and Adam\n",
    "- Quantify directional difference (cosine similarity vs full step direction)\n",
    "- Keep runtime low: small minibatches, 10 minibatches, K steps each, 3 seeds\n",
    "\n",
    "Requires\n",
    "--------\n",
    "- realnet_results.pt (from Block 1; contains frozen preprocessor weights)\n",
    "- pennylane\n",
    "- torchvision\n",
    "- optional: pennylane-lightning-gpu (for lightning.gpu)\n",
    "\n",
    "Notes\n",
    "-----\n",
    "- FULL metric_tensor (approx=None) requires an auxiliary wire on many devices.\n",
    "  We allocate N_QUBITS + 1 wires and reserve the last wire as aux.\n",
    "- Adam is implemented in a lightweight, deterministic “PyTorch Adam” way on θ only,\n",
    "  so you can compare it without rerunning multi-day experiments.\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as pnp\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Config (tune for speed)\n",
    "# ---------------------------\n",
    "SEEDS = [42, 123, 456]\n",
    "\n",
    "DATASET = \"MNIST\"          # fixed per your request\n",
    "N_QUBITS = 4\n",
    "N_LAYERS = 2\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "N_MINIBATCHES = 10         # 10 minibatches per seed\n",
    "K_STEPS = 10               # steps per minibatch (kept from prior diagnostic)\n",
    "\n",
    "ETA = 0.01                 # Euclid/FS learning rate\n",
    "LAM = 1e-3                 # FS damping\n",
    "\n",
    "# Shrinkage: α=1 => pure FS, α<1 => blend toward Euclidean identity metric\n",
    "ALPHA_SHRINK = 1.0         # try 0.8 or 0.5 for “Euclid+FS” blend\n",
    "\n",
    "# Adam hyperparams (match your prior analysis spirit)\n",
    "ADAM_LR = 1e-3\n",
    "ADAM_BETAS = (0.9, 0.999)\n",
    "ADAM_EPS = 1e-8\n",
    "ADAM_WEIGHT_DECAY = 0.0\n",
    "\n",
    "PREFERRED_DEVICE = \"lightning.gpu\"\n",
    "FALLBACK_DEVICE = \"default.qubit\"\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Reproducibility\n",
    "# ---------------------------\n",
    "def set_all_seeds(seed: int):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    try:\n",
    "        qml.numpy.random.seed(seed)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Shared Preprocessor (Block 1)\n",
    "# ---------------------------\n",
    "class SharedPreprocessor(nn.Module):\n",
    "    \"\"\"Shared classical feature extractor: 784 → 16\"\"\"\n",
    "    def __init__(self, input_dim=784, bottleneck_dim=16):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, bottleneck_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return torch.tanh(self.fc(x))\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Device factory (+1 aux wire for full metric)\n",
    "# ---------------------------\n",
    "def make_device(n_qubits: int):\n",
    "    n_wires_total = n_qubits + 1\n",
    "    aux_wire = n_qubits\n",
    "    try:\n",
    "        dev = qml.device(PREFERRED_DEVICE, wires=n_wires_total)\n",
    "        backend = PREFERRED_DEVICE\n",
    "    except Exception as e:\n",
    "        dev = qml.device(FALLBACK_DEVICE, wires=n_wires_total)\n",
    "        backend = FALLBACK_DEVICE\n",
    "        print(f\"[device] Could not load {PREFERRED_DEVICE} ({type(e).__name__}: {e}). Using {FALLBACK_DEVICE}.\")\n",
    "    print(f\"[device] backend={backend}, wires={n_wires_total} (aux_wire={aux_wire})\")\n",
    "    return dev, backend, aux_wire\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Helpers\n",
    "# ---------------------------\n",
    "def _cosine(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    na = np.linalg.norm(a) + 1e-12\n",
    "    nb = np.linalg.norm(b) + 1e-12\n",
    "    return float(np.dot(a, b) / (na * nb))\n",
    "\n",
    "\n",
    "def _flat(theta) -> pnp.ndarray:\n",
    "    return pnp.reshape(theta, (-1,))\n",
    "\n",
    "\n",
    "def _unflat(v, shape) -> pnp.ndarray:\n",
    "    return pnp.reshape(v, shape)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Build circuit + metric fns\n",
    "# ---------------------------\n",
    "dev, backend, aux_wire = make_device(N_QUBITS)\n",
    "\n",
    "@qml.qnode(dev, interface=\"autograd\", diff_method=\"parameter-shift\")\n",
    "def circuit(inputs, weights):\n",
    "    # circuit uses ONLY wires 0..N_QUBITS-1; last wire is aux-reserved\n",
    "    for layer in range(N_LAYERS):\n",
    "        for i in range(N_QUBITS):\n",
    "            qml.RY(inputs[i], wires=i)\n",
    "        for i in range(N_QUBITS):\n",
    "            qml.RY(weights[layer, i, 0], wires=i)\n",
    "            qml.RZ(weights[layer, i, 1], wires=i)\n",
    "\n",
    "        for i in range(N_QUBITS - 1):\n",
    "            qml.CNOT(wires=[i, i + 1])\n",
    "        if N_QUBITS > 2:\n",
    "            qml.CNOT(wires=[N_QUBITS - 1, 0])\n",
    "\n",
    "    return (\n",
    "        qml.expval(qml.PauliZ(0)),\n",
    "        qml.expval(qml.PauliZ(1)),\n",
    "        qml.expval(qml.PauliZ(2)),\n",
    "        qml.expval(qml.PauliZ(3)),\n",
    "        qml.expval(qml.PauliZ(0) @ qml.PauliZ(1)),\n",
    "        qml.expval(qml.PauliZ(2) @ qml.PauliZ(3)),\n",
    "    )\n",
    "\n",
    "metric_full  = qml.metric_tensor(circuit, approx=None,       aux_wire=aux_wire)\n",
    "metric_diag  = qml.metric_tensor(circuit, approx=\"diag\")\n",
    "metric_block = qml.metric_tensor(circuit, approx=\"block-diag\")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Load frozen preprocessor from Block 1\n",
    "# ---------------------------\n",
    "realnet = torch.load(\"realnet_results.pt\", weights_only=False)\n",
    "pre_state = realnet[\"preprocessor_state\"]\n",
    "\n",
    "pre = SharedPreprocessor(784, 16)\n",
    "pre.load_state_dict(pre_state)\n",
    "for p in pre.parameters():\n",
    "    p.requires_grad = False\n",
    "pre.eval()\n",
    "\n",
    "# frozen feature_select (16 -> 4)\n",
    "feature_select = nn.Linear(16, N_QUBITS)\n",
    "for p in feature_select.parameters():\n",
    "    p.requires_grad = False\n",
    "feature_select.eval()\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Dataset: MNIST (fixed)\n",
    "# ---------------------------\n",
    "norm_mean, norm_std = (0.1307,), (0.3081,)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std),\n",
    "])\n",
    "train_ds = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Fixed readout (constant) to isolate θ update behavior\n",
    "# ---------------------------\n",
    "C = 10\n",
    "W = pnp.array(np.random.randn(C, 6).astype(np.float64) * 0.1)  # (10,6)\n",
    "b = pnp.array(np.zeros((C,), dtype=np.float64))\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Loss factory (depends on current minibatch)\n",
    "# ---------------------------\n",
    "def make_batch_ce_loss(x_np, y_np):\n",
    "    def batch_ce_loss(theta):\n",
    "        losses = []\n",
    "        for i in range(len(y_np)):\n",
    "            q_raw = circuit(x_np[i], theta)\n",
    "            q = pnp.stack(q_raw)  # (6,)\n",
    "            logits = W @ q + b\n",
    "            logits = logits - pnp.max(logits)\n",
    "            ex = pnp.exp(logits)\n",
    "            probs = ex / pnp.sum(ex)\n",
    "            losses.append(-pnp.log(probs[int(y_np[i])] + 1e-12))\n",
    "        return pnp.mean(pnp.stack(losses))\n",
    "    return batch_ce_loss\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# FS-NG / Euclid delta\n",
    "# ---------------------------\n",
    "def fsng_or_euclid_delta(theta, approx: str, batch_ce_loss, x_np):\n",
    "    \"\"\"\n",
    "    approx in {\"euclid\",\"diag\",\"block-diag\",\"full\"}.\n",
    "\n",
    "    - euclid: delta = g\n",
    "    - others: delta = (G_eff + λI)^(-1) g\n",
    "              with optional shrinkage: G_eff = α G_bar + (1-α) I\n",
    "    \"\"\"\n",
    "    theta_shape = theta.shape\n",
    "    P = int(np.prod(theta_shape))\n",
    "    I = pnp.eye(P, dtype=pnp.float64)\n",
    "\n",
    "    g = qml.grad(batch_ce_loss)(theta)\n",
    "    g_flat = pnp.reshape(g, (P,))\n",
    "\n",
    "    if approx == \"euclid\":\n",
    "        return np.array(g_flat, dtype=np.float64)\n",
    "\n",
    "    if approx == \"diag\":\n",
    "        metric_fn = metric_diag\n",
    "    elif approx == \"block-diag\":\n",
    "        metric_fn = metric_block\n",
    "    elif approx == \"full\":\n",
    "        metric_fn = metric_full\n",
    "    else:\n",
    "        raise ValueError(\"approx must be one of: 'euclid','diag','block-diag','full'\")\n",
    "\n",
    "    G_sum = pnp.zeros((P, P), dtype=pnp.float64)\n",
    "    for i in range(x_np.shape[0]):\n",
    "        Gi = metric_fn(x_np[i], theta)\n",
    "        Gi = pnp.reshape(Gi, (P, P))\n",
    "        G_sum = G_sum + Gi\n",
    "    G_bar = G_sum / float(x_np.shape[0])\n",
    "\n",
    "    alpha = float(ALPHA_SHRINK)\n",
    "    G_eff = alpha * G_bar + (1.0 - alpha) * I\n",
    "    G_reg = G_eff + LAM * I\n",
    "\n",
    "    delta = pnp.linalg.solve(G_reg, g_flat)\n",
    "    return np.array(delta, dtype=np.float64)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Adam (PyTorch-style) on θ only\n",
    "# ---------------------------\n",
    "class AdamState:\n",
    "    def __init__(self, P, betas=(0.9, 0.999)):\n",
    "        self.t = 0\n",
    "        self.m = np.zeros((P,), dtype=np.float64)\n",
    "        self.v = np.zeros((P,), dtype=np.float64)\n",
    "        self.beta1, self.beta2 = betas\n",
    "\n",
    "def adam_step(theta, grad_flat, state: AdamState, lr=1e-3, eps=1e-8, weight_decay=0.0):\n",
    "    \"\"\"\n",
    "    Deterministic Adam update on a *flat* theta vector.\n",
    "    Returns updated flat theta and the applied update direction (delta).\n",
    "    \"\"\"\n",
    "    state.t += 1\n",
    "\n",
    "    g = grad_flat.astype(np.float64)\n",
    "    if weight_decay != 0.0:\n",
    "        g = g + weight_decay * theta\n",
    "\n",
    "    b1, b2 = state.beta1, state.beta2\n",
    "    state.m = b1 * state.m + (1.0 - b1) * g\n",
    "    state.v = b2 * state.v + (1.0 - b2) * (g * g)\n",
    "\n",
    "    mhat = state.m / (1.0 - b1 ** state.t)\n",
    "    vhat = state.v / (1.0 - b2 ** state.t)\n",
    "\n",
    "    step = lr * mhat / (np.sqrt(vhat) + eps)   # this is the update magnitude\n",
    "    theta_new = theta - step\n",
    "    return theta_new, step\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# One minibatch run for one method\n",
    "# ---------------------------\n",
    "def run_one_minibatch(theta0, method, x_np, y_np):\n",
    "    \"\"\"\n",
    "    Runs K_STEPS on a single minibatch and returns:\n",
    "      losses: (K_STEPS,)\n",
    "      delta1: first-step update direction (flat, numpy)\n",
    "      elapsed\n",
    "    \"\"\"\n",
    "    theta = pnp.array(theta0, requires_grad=True)\n",
    "    theta_shape = theta.shape\n",
    "    P = int(np.prod(theta_shape))\n",
    "\n",
    "    batch_ce_loss = make_batch_ce_loss(x_np, y_np)\n",
    "\n",
    "    # Adam needs state per minibatch (matches your “diagnostic” intent)\n",
    "    adam_state = AdamState(P, betas=ADAM_BETAS) if method == \"adam\" else None\n",
    "\n",
    "    losses = []\n",
    "    delta1 = None\n",
    "\n",
    "    t0 = time.time()\n",
    "    for k in range(K_STEPS):\n",
    "        loss_before = float(batch_ce_loss(theta))\n",
    "\n",
    "        if method == \"adam\":\n",
    "            g = qml.grad(batch_ce_loss)(theta)\n",
    "            g_flat = np.array(pnp.reshape(g, (P,)), dtype=np.float64)\n",
    "\n",
    "            theta_flat = np.array(pnp.reshape(theta, (P,)), dtype=np.float64)\n",
    "            theta_flat_new, step_vec = adam_step(\n",
    "                theta_flat, g_flat, adam_state,\n",
    "                lr=ADAM_LR, eps=ADAM_EPS, weight_decay=ADAM_WEIGHT_DECAY\n",
    "            )\n",
    "            # For cosine comparisons, treat Adam's *applied update* as delta\n",
    "            delta = step_vec / (ADAM_LR + 1e-12)  # normalized-ish direction (optional)\n",
    "            theta = _unflat(theta_flat_new, theta_shape)\n",
    "\n",
    "        else:\n",
    "            delta = fsng_or_euclid_delta(theta, method, batch_ce_loss, x_np)\n",
    "            theta_flat = np.array(pnp.reshape(theta, (P,)), dtype=np.float64)\n",
    "            theta = _unflat(theta_flat - ETA * delta, theta_shape)\n",
    "\n",
    "        loss_after = float(batch_ce_loss(theta))\n",
    "        losses.append(loss_after)\n",
    "\n",
    "        if k == 0:\n",
    "            # Store first-step applied direction in comparable units:\n",
    "            # - for FS/euclid: delta is the preconditioned direction (so update is ETA*delta)\n",
    "            # - for Adam: delta above is roughly direction; we’ll store the *applied step* too\n",
    "            if method == \"adam\":\n",
    "                delta1 = np.array(step_vec, dtype=np.float64)  # actual applied update\n",
    "            else:\n",
    "                delta1 = np.array(ETA * delta, dtype=np.float64)  # applied update\n",
    "\n",
    "    return np.array(losses), delta1, time.time() - t0\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Collect minibatches (deterministic but seed-dependent)\n",
    "# ---------------------------\n",
    "def get_minibatches(seed, n_minibatches, batch_size):\n",
    "    \"\"\"\n",
    "    Returns list of (x_np, y_np) minibatches.\n",
    "    Uses a deterministic RNG over indices so we don't depend on DataLoader nondeterminism.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    batches = []\n",
    "    for _ in range(n_minibatches):\n",
    "        idx = rng.choice(len(train_ds), size=batch_size, replace=False)\n",
    "        x0, y0 = zip(*[train_ds[i] for i in idx])\n",
    "        x0 = torch.stack(list(x0), dim=0)\n",
    "        y0 = torch.tensor(list(y0), dtype=torch.long)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            feats16 = pre(x0)\n",
    "            xq = torch.tanh(feature_select(feats16))\n",
    "\n",
    "        batches.append((xq.cpu().numpy(), y0.cpu().numpy()))\n",
    "    return batches\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Main experiment loop\n",
    "# ---------------------------\n",
    "METHODS = [\"euclid\", \"adam\", \"diag\", \"block-diag\", \"full\"]\n",
    "\n",
    "def summarize(arr):\n",
    "    return float(np.mean(arr)), float(np.std(arr))\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(f\"\\n[Euclid vs Adam vs FS-NG] DATASET={DATASET} | backend={backend}\")\n",
    "    print(f\"  layers={N_LAYERS} | batch={BATCH_SIZE} | minibatches={N_MINIBATCHES} | steps={K_STEPS}\")\n",
    "    print(f\"  ETA={ETA} | LAM={LAM} | alpha_shrink={ALPHA_SHRINK} | Adam(lr={ADAM_LR}, betas={ADAM_BETAS})\\n\")\n",
    "\n",
    "    # Store per-method results across (seed, minibatch)\n",
    "    final_losses = {m: [] for m in METHODS}\n",
    "    times = {m: [] for m in METHODS}\n",
    "    cos_vs_full = {m: [] for m in METHODS if m != \"full\"}  # cosine of step1 update vs full step1 update\n",
    "\n",
    "    for seed in SEEDS:\n",
    "        set_all_seeds(seed)\n",
    "        print(f\"\\n{'='*70}\\nSEED {seed}\\n{'='*70}\")\n",
    "\n",
    "        # same initial theta per seed across methods\n",
    "        theta0 = pnp.array(np.random.randn(N_LAYERS, N_QUBITS, 2) * 0.1, requires_grad=True)\n",
    "\n",
    "        minibatches = get_minibatches(seed, N_MINIBATCHES, BATCH_SIZE)\n",
    "\n",
    "        for mb_i, (x_np, y_np) in enumerate(minibatches, start=1):\n",
    "            # run full first for reference direction cosine\n",
    "            full_losses, full_delta1, full_t = run_one_minibatch(theta0, \"full\", x_np, y_np)\n",
    "            final_losses[\"full\"].append(full_losses[-1])\n",
    "            times[\"full\"].append(full_t)\n",
    "\n",
    "            # run others\n",
    "            for m in [\"euclid\", \"adam\", \"diag\", \"block-diag\"]:\n",
    "                losses, delta1, tsec = run_one_minibatch(theta0, m, x_np, y_np)\n",
    "                final_losses[m].append(losses[-1])\n",
    "                times[m].append(tsec)\n",
    "\n",
    "                # cosine on step-1 APPLIED update (so compare apples-to-apples)\n",
    "                cos_vs_full[m].append(_cosine(delta1, full_delta1))\n",
    "\n",
    "            if mb_i in (1, N_MINIBATCHES):\n",
    "                print(f\"  minibatch {mb_i:2d}/{N_MINIBATCHES}: \"\n",
    "                      f\"full_lossK={full_losses[-1]:.4f} | \"\n",
    "                      f\"diag_lossK={final_losses['diag'][-1]:.4f} | \"\n",
    "                      f\"adam_lossK={final_losses['adam'][-1]:.4f}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"AGGREGATE RESULTS (across 3 seeds × 10 minibatches = 30 runs per method)\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Report: mean±std final loss, mean±std time, mean cosine vs full\n",
    "    for m in METHODS:\n",
    "        muL, sdL = summarize(final_losses[m])\n",
    "        muT, sdT = summarize(times[m])\n",
    "        if m == \"full\":\n",
    "            print(f\"  {m:9s}: final_loss={muL:.4f} ± {sdL:.4f} | time={muT:.2f}s ± {sdT:.2f}s | cos(step1 vs full)=1.000\")\n",
    "        else:\n",
    "            muC, sdC = summarize(cos_vs_full[m])\n",
    "            print(f\"  {m:9s}: final_loss={muL:.4f} ± {sdL:.4f} | time={muT:.2f}s ± {sdT:.2f}s | cos(step1 vs full)={muC:.3f} ± {sdC:.3f}\")\n",
    "\n",
    "    print(\"\\nNotes:\")\n",
    "    print(\"  • Cosines compare the *applied step vector* at step 1 (not the raw gradient).\")\n",
    "    print(\"  • Adam’s delta is its actual applied update on θ (per minibatch, fresh state).\")\n",
    "    print(\"  • If you want Adam to carry momentum across minibatches, move AdamState outside run_one_minibatch().\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47c54da-5eae-42c1-8c44-b8639ff72062",
   "metadata": {},
   "source": [
    "# RealNet FashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13a23d81-00fe-476d-8186-a512664cf773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch device: cuda\n",
      "======================================================================\n",
      "BLOCK 1: RealNet Training\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "  • Batch size: 32\n",
      "  • Patience: 10\n",
      "  • Seeds: [42, 123, 456]\n",
      "  • Architecture: 784 → 16 → 64 → 10\n",
      "======================================================================\n",
      "\n",
      "Creating stratified samples...\n",
      "  Sampling took 0.01s\n",
      "  Train samples: 15000 (stratified, 1500 per class)\n",
      "  Test samples:  3000 (stratified, 300 per class)\n",
      "\n",
      "======================================================================\n",
      "SEED 42\n",
      "======================================================================\n",
      "\n",
      "  Training RealNet (seed=42)...\n",
      "  [Real] Epoch  1 | loss=0.8276 | test_acc=0.7903 | time=1.3s\n",
      "  [Real] Epoch  2 | loss=0.4974 | test_acc=0.8260 | time=2.6s\n",
      "  [Real] Epoch  3 | loss=0.4442 | test_acc=0.8333 | time=3.9s\n",
      "  [Real] Epoch  4 | loss=0.4149 | test_acc=0.8270 | time=5.2s\n",
      "  [Real] Epoch  5 | loss=0.3965 | test_acc=0.8373 | time=6.6s\n",
      "  [Real] Epoch  6 | loss=0.3760 | test_acc=0.8383 | time=8.1s\n",
      "  [Real] Epoch  7 | loss=0.3641 | test_acc=0.8303 | time=9.5s\n",
      "  [Real] Epoch  8 | loss=0.3509 | test_acc=0.8377 | time=10.9s\n",
      "  [Real] Epoch  9 | loss=0.3429 | test_acc=0.8390 | time=12.3s\n",
      "  [Real] Epoch 10 | loss=0.3271 | test_acc=0.8440 | time=13.6s\n",
      "  [Real] Epoch 11 | loss=0.3186 | test_acc=0.8397 | time=14.9s\n",
      "  [Real] Epoch 12 | loss=0.3115 | test_acc=0.8313 | time=16.2s\n",
      "  [Real] Epoch 13 | loss=0.3069 | test_acc=0.8327 | time=17.5s\n",
      "  [Real] Epoch 14 | loss=0.3008 | test_acc=0.8453 | time=18.8s\n",
      "  [Real] Epoch 15 | loss=0.3007 | test_acc=0.8420 | time=20.1s\n",
      "  [Real] Epoch 16 | loss=0.2891 | test_acc=0.8383 | time=21.4s\n",
      "  [Real] Epoch 17 | loss=0.2797 | test_acc=0.8373 | time=22.7s\n",
      "  [Real] Epoch 18 | loss=0.2733 | test_acc=0.8270 | time=24.0s\n",
      "  [Real] Epoch 19 | loss=0.2711 | test_acc=0.8343 | time=25.3s\n",
      "  [Real] Epoch 20 | loss=0.2649 | test_acc=0.8357 | time=26.6s\n",
      "  [Real] Epoch 21 | loss=0.2621 | test_acc=0.8410 | time=27.9s\n",
      "  [Real] Epoch 22 | loss=0.2619 | test_acc=0.8383 | time=29.2s\n",
      "  [Real] Epoch 23 | loss=0.2506 | test_acc=0.8390 | time=30.5s\n",
      "  [Real] Epoch 24 | loss=0.2493 | test_acc=0.8393 | time=31.8s\n",
      "  [Real] Early stop at epoch 24 (no improvement for 10 epochs)\n",
      "\n",
      "  → Saved preprocessor state from seed 42\n",
      "\n",
      "======================================================================\n",
      "SEED 123\n",
      "======================================================================\n",
      "\n",
      "  Training RealNet (seed=123)...\n",
      "  [Real] Epoch  1 | loss=0.8076 | test_acc=0.8080 | time=1.3s\n",
      "  [Real] Epoch  2 | loss=0.4696 | test_acc=0.8267 | time=2.7s\n",
      "  [Real] Epoch  3 | loss=0.4312 | test_acc=0.8317 | time=4.1s\n",
      "  [Real] Epoch  4 | loss=0.4032 | test_acc=0.8373 | time=5.4s\n",
      "  [Real] Epoch  5 | loss=0.3861 | test_acc=0.8373 | time=6.7s\n",
      "  [Real] Epoch  6 | loss=0.3691 | test_acc=0.8343 | time=8.0s\n",
      "  [Real] Epoch  7 | loss=0.3598 | test_acc=0.8343 | time=9.4s\n",
      "  [Real] Epoch  8 | loss=0.3523 | test_acc=0.8453 | time=10.7s\n",
      "  [Real] Epoch  9 | loss=0.3355 | test_acc=0.8430 | time=11.9s\n",
      "  [Real] Epoch 10 | loss=0.3230 | test_acc=0.8323 | time=13.3s\n",
      "  [Real] Epoch 11 | loss=0.3165 | test_acc=0.8373 | time=14.6s\n",
      "  [Real] Epoch 12 | loss=0.3143 | test_acc=0.8427 | time=15.8s\n",
      "  [Real] Epoch 13 | loss=0.3049 | test_acc=0.8440 | time=17.1s\n",
      "  [Real] Epoch 14 | loss=0.2938 | test_acc=0.8380 | time=18.4s\n",
      "  [Real] Epoch 15 | loss=0.2918 | test_acc=0.8477 | time=19.7s\n",
      "  [Real] Epoch 16 | loss=0.2835 | test_acc=0.8457 | time=21.0s\n",
      "  [Real] Epoch 17 | loss=0.2795 | test_acc=0.8437 | time=22.4s\n",
      "  [Real] Epoch 18 | loss=0.2771 | test_acc=0.8413 | time=23.7s\n",
      "  [Real] Epoch 19 | loss=0.2728 | test_acc=0.8413 | time=25.0s\n",
      "  [Real] Epoch 20 | loss=0.2681 | test_acc=0.8380 | time=26.3s\n",
      "  [Real] Epoch 21 | loss=0.2642 | test_acc=0.8420 | time=27.5s\n",
      "  [Real] Epoch 22 | loss=0.2676 | test_acc=0.8340 | time=28.9s\n",
      "  [Real] Epoch 23 | loss=0.2504 | test_acc=0.8447 | time=30.1s\n",
      "  [Real] Epoch 24 | loss=0.2487 | test_acc=0.8347 | time=31.4s\n",
      "  [Real] Epoch 25 | loss=0.2438 | test_acc=0.8403 | time=32.8s\n",
      "  [Real] Early stop at epoch 25 (no improvement for 10 epochs)\n",
      "\n",
      "======================================================================\n",
      "SEED 456\n",
      "======================================================================\n",
      "\n",
      "  Training RealNet (seed=456)...\n",
      "  [Real] Epoch  1 | loss=0.8361 | test_acc=0.8007 | time=1.4s\n",
      "  [Real] Epoch  2 | loss=0.5010 | test_acc=0.8153 | time=2.8s\n",
      "  [Real] Epoch  3 | loss=0.4518 | test_acc=0.8187 | time=4.3s\n",
      "  [Real] Epoch  4 | loss=0.4273 | test_acc=0.8337 | time=5.7s\n",
      "  [Real] Epoch  5 | loss=0.4044 | test_acc=0.8357 | time=7.2s\n",
      "  [Real] Epoch  6 | loss=0.3824 | test_acc=0.8267 | time=8.6s\n",
      "  [Real] Epoch  7 | loss=0.3673 | test_acc=0.8277 | time=10.0s\n",
      "  [Real] Epoch  8 | loss=0.3606 | test_acc=0.8323 | time=11.4s\n",
      "  [Real] Epoch  9 | loss=0.3492 | test_acc=0.8377 | time=12.9s\n",
      "  [Real] Epoch 10 | loss=0.3366 | test_acc=0.8450 | time=14.3s\n",
      "  [Real] Epoch 11 | loss=0.3289 | test_acc=0.8440 | time=15.7s\n",
      "  [Real] Epoch 12 | loss=0.3256 | test_acc=0.8400 | time=17.1s\n",
      "  [Real] Epoch 13 | loss=0.3118 | test_acc=0.8387 | time=18.5s\n",
      "  [Real] Epoch 14 | loss=0.3115 | test_acc=0.8367 | time=19.9s\n",
      "  [Real] Epoch 15 | loss=0.3035 | test_acc=0.8403 | time=21.3s\n",
      "  [Real] Epoch 16 | loss=0.2936 | test_acc=0.8393 | time=22.8s\n",
      "  [Real] Epoch 17 | loss=0.2924 | test_acc=0.8387 | time=24.2s\n",
      "  [Real] Epoch 18 | loss=0.2804 | test_acc=0.8350 | time=25.7s\n",
      "  [Real] Epoch 19 | loss=0.2789 | test_acc=0.8327 | time=27.1s\n",
      "  [Real] Epoch 20 | loss=0.2802 | test_acc=0.8430 | time=28.5s\n",
      "  [Real] Early stop at epoch 20 (no improvement for 10 epochs)\n",
      "\n",
      "======================================================================\n",
      "REALNET SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Accuracy:   0.8460 ± 0.0012\n",
      "Time:       31.0s ± 1.8s\n",
      "Epochs:     23.0 ± 2.2\n",
      "Parameters: 14,298\n",
      "\n",
      "Per-seed results:\n",
      "  Seed 42: acc=0.8453, time=31.8s, epochs=24\n",
      "  Seed 123: acc=0.8477, time=32.8s, epochs=25\n",
      "  Seed 456: acc=0.8450, time=28.5s, epochs=20\n",
      "\n",
      "✓ Saved results to: realnet_results.pt\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Block 1: RealNet Training\n",
    "==========================\n",
    "Trains the baseline Real MLP on 3 seeds.\n",
    "Saves the trained preprocessor for reuse in subsequent blocks.\n",
    "\n",
    "Outputs:\n",
    "- realnet_results.pt: Contains results dict and trained preprocessor state\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "print(f\"Using PyTorch device: {device}\")\n",
    "\n",
    "# ============================================\n",
    "# Comprehensive seed setting for reproducibility\n",
    "# ============================================\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    \"\"\"Set seeds for all RNG sources for reproducibility\"\"\"\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    # Python\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # NumPy\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # PyTorch deterministic mode\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # CuPy (if available)\n",
    "    try:\n",
    "        import cupy as cp\n",
    "        cp.random.seed(seed)\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    # PennyLane (if available)\n",
    "    try:\n",
    "        import pennylane as qml\n",
    "        qml.numpy.random.seed(seed)\n",
    "    except (ImportError, AttributeError):\n",
    "        pass\n",
    "\n",
    "# ============================================\n",
    "# Shared Bottleneck Preprocessor: 784 → 16\n",
    "# ============================================\n",
    "\n",
    "class SharedPreprocessor(nn.Module):\n",
    "    \"\"\"Shared classical feature extractor: 784 → 16\"\"\"\n",
    "    def __init__(self, input_dim=784, bottleneck_dim=16):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, bottleneck_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.tanh(self.fc(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Real-valued Head and Network\n",
    "# ============================================\n",
    "\n",
    "class RealHead(nn.Module):\n",
    "    \"\"\"Standard MLP: 16 → 64 → 10\"\"\"\n",
    "    def __init__(self, bottleneck_dim=16, hidden_dim=64, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(bottleneck_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RealNet(nn.Module):\n",
    "    \"\"\"Complete Real network: Preprocessor + RealHead\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.preprocessor = SharedPreprocessor(784, 16)\n",
    "        self.head = RealHead(16, 64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.preprocessor(x)\n",
    "        return self.head(features)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Training and Evaluation\n",
    "# ============================================\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, device, show_progress=False):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        total_samples += x.size(0)\n",
    "\n",
    "        if show_progress and batch_idx % 50 == 0:\n",
    "            print(f\"    Batch {batch_idx}/{len(loader)}, samples: {total_samples}\", end=\"\\r\")\n",
    "\n",
    "    if show_progress:\n",
    "        print()\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += x.size(0)\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "def train_with_early_stopping(model, train_loader, test_loader, optimizer,\n",
    "                              device, max_epochs=40, patience=10, name=\"Model\"):\n",
    "    if isinstance(device, str):\n",
    "        device = torch.device(device)\n",
    "\n",
    "    best_acc = 0.0\n",
    "    epochs_without_improvement = 0\n",
    "    start = time.time()\n",
    "    last_acc = 0.0\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        loss = train_one_epoch(model, train_loader, optimizer, device, show_progress=False)\n",
    "        acc = evaluate(model, test_loader, device)\n",
    "        last_acc = acc\n",
    "\n",
    "        elapsed = time.time() - start\n",
    "        print(f\"  [{name}] Epoch {epoch:2d} | loss={loss:.4f} \"\n",
    "              f\"| test_acc={acc:.4f} | time={elapsed:.1f}s\")\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"  [{name}] Early stop at epoch {epoch} \"\n",
    "                  f\"(no improvement for {patience} epochs)\")\n",
    "            break\n",
    "\n",
    "    total_time = time.time() - start\n",
    "    return {\n",
    "        \"best_acc\": best_acc,\n",
    "        \"final_acc\": last_acc,\n",
    "        \"time\": total_time,\n",
    "        \"epochs\": epoch,\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Data utilities\n",
    "# ============================================\n",
    "\n",
    "def stratified_sample_from_targets(dataset, n_samples_per_class, seed=42):\n",
    "    \"\"\"\n",
    "    Create a stratified sample with n_samples_per_class from each class.\n",
    "    Uses dataset.targets directly - NO image loading during sampling.\n",
    "    \"\"\"\n",
    "    # FashionMNIST and MNIST expose targets as a tensor\n",
    "    targets = dataset.targets\n",
    "    rng = np.random.RandomState(seed)\n",
    "    \n",
    "    sampled_indices = []\n",
    "    for c in range(10):\n",
    "        # Find all indices for class c\n",
    "        idx_c = (targets == c).nonzero(as_tuple=False).view(-1).cpu().numpy()\n",
    "        k = min(n_samples_per_class, len(idx_c))\n",
    "        # Sample without replacement\n",
    "        selected = rng.choice(idx_c, size=k, replace=False).tolist()\n",
    "        sampled_indices.extend(selected)\n",
    "    \n",
    "    # Shuffle the combined indices\n",
    "    rng.shuffle(sampled_indices)\n",
    "    return sampled_indices\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Main\n",
    "# ============================================\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"BLOCK 1: RealNet Training\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nConfiguration:\")\n",
    "    print(\"  • Batch size: 32\")\n",
    "    print(\"  • Patience: 10\")\n",
    "    print(\"  • Seeds: [42, 123, 456]\")\n",
    "    print(\"  • Architecture: 784 → 16 → 64 → 10\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.2860,), (0.3530,))\n",
    "    ])\n",
    "\n",
    "    # Load full datasets\n",
    "    full_train_ds = datasets.FashionMNIST(root=\"./data\", train=True,\n",
    "                                         download=True, transform=transform)\n",
    "    full_test_ds = datasets.FashionMNIST(root=\"./data\", train=False,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "    # Create stratified samples (fast - uses targets only, no image loading)\n",
    "    print(\"\\nCreating stratified samples...\")\n",
    "    t0 = time.time()\n",
    "    train_indices = stratified_sample_from_targets(full_train_ds, n_samples_per_class=1500, seed=42)\n",
    "    test_indices = stratified_sample_from_targets(full_test_ds, n_samples_per_class=300, seed=42)\n",
    "    print(f\"  Sampling took {time.time()-t0:.2f}s\")\n",
    "    \n",
    "    train_ds = Subset(full_train_ds, train_indices)\n",
    "    test_ds = Subset(full_test_ds, test_indices)\n",
    "    \n",
    "    print(f\"  Train samples: {len(train_ds)} (stratified, 1500 per class)\")\n",
    "    print(f\"  Test samples:  {len(test_ds)} (stratified, 300 per class)\")\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=32,\n",
    "                              shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_ds, batch_size=64,\n",
    "                             shuffle=False, num_workers=0)\n",
    "\n",
    "    seeds = [42, 123, 456]\n",
    "    all_results = []\n",
    "    trained_preprocessor_state = None\n",
    "\n",
    "    for seed in seeds:\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(f\"SEED {seed}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        set_all_seeds(seed)\n",
    "\n",
    "        print(f\"\\n  Training RealNet (seed={seed})...\")\n",
    "        real_model = RealNet().to(device)\n",
    "        real_opt = torch.optim.Adam(real_model.parameters(), lr=1e-3)\n",
    "        \n",
    "        result = train_with_early_stopping(\n",
    "            real_model, train_loader, test_loader, real_opt, device,\n",
    "            max_epochs=40, patience=10, name=\"Real\"\n",
    "        )\n",
    "        result[\"params\"] = sum(p.numel() for p in real_model.parameters())\n",
    "        result[\"seed\"] = seed\n",
    "        \n",
    "        all_results.append(result)\n",
    "        \n",
    "        # Save preprocessor from first seed\n",
    "        if trained_preprocessor_state is None:\n",
    "            trained_preprocessor_state = real_model.preprocessor.state_dict()\n",
    "            print(f\"\\n  → Saved preprocessor state from seed {seed}\")\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"REALNET SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    accs = [r[\"best_acc\"] for r in all_results]\n",
    "    times = [r[\"time\"] for r in all_results]\n",
    "    epochs = [r[\"epochs\"] for r in all_results]\n",
    "    params = all_results[0][\"params\"]\n",
    "    \n",
    "    print(f\"\\nAccuracy:   {np.mean(accs):.4f} ± {np.std(accs):.4f}\")\n",
    "    print(f\"Time:       {np.mean(times):.1f}s ± {np.std(times):.1f}s\")\n",
    "    print(f\"Epochs:     {np.mean(epochs):.1f} ± {np.std(epochs):.1f}\")\n",
    "    print(f\"Parameters: {params:,}\")\n",
    "    \n",
    "    print(\"\\nPer-seed results:\")\n",
    "    for r in all_results:\n",
    "        print(f\"  Seed {r['seed']}: acc={r['best_acc']:.4f}, \"\n",
    "              f\"time={r['time']:.1f}s, epochs={r['epochs']}\")\n",
    "\n",
    "    # Save results and preprocessor\n",
    "    save_dict = {\n",
    "        \"results\": all_results,\n",
    "        \"preprocessor_state\": trained_preprocessor_state,\n",
    "        \"summary\": {\n",
    "            \"mean_acc\": np.mean(accs),\n",
    "            \"std_acc\": np.std(accs),\n",
    "            \"mean_time\": np.mean(times),\n",
    "            \"params\": params\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    torch.save(save_dict, \"realnet_results1.pt\")\n",
    "    print(f\"\\n✓ Saved results to: realnet_results.pt\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ef98d5-4a2f-4c5b-9ca0-54fafa18975e",
   "metadata": {},
   "source": [
    "# Diagnostic Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b284e76-85d3-484d-9db0-64f1fdb0001a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[device] backend=lightning.gpu, wires=5 (aux_wire=4)\n",
      "\n",
      "[Euclid vs Adam vs FS-NG] DATASET=FashionMNIST | backend=lightning.gpu\n",
      "  layers=2 | batch=8 | minibatches=10 | steps=10\n",
      "  ETA=0.01 | LAM=0.001 | alpha_shrink=1.0 | Adam(lr=0.001, betas=(0.9, 0.999))\n",
      "\n",
      "\n",
      "======================================================================\n",
      "SEED 42\n",
      "======================================================================\n",
      "  minibatch  1/10: full_lossK=2.2397 | diag_lossK=2.2400 | adam_lossK=2.2405\n",
      "  minibatch 10/10: full_lossK=2.3050 | diag_lossK=2.3050 | adam_lossK=2.3046\n",
      "\n",
      "======================================================================\n",
      "SEED 123\n",
      "======================================================================\n",
      "  minibatch  1/10: full_lossK=2.4305 | diag_lossK=2.4305 | adam_lossK=2.4304\n",
      "  minibatch 10/10: full_lossK=2.2562 | diag_lossK=2.2560 | adam_lossK=2.2568\n",
      "\n",
      "======================================================================\n",
      "SEED 456\n",
      "======================================================================\n",
      "  minibatch  1/10: full_lossK=2.2706 | diag_lossK=2.2707 | adam_lossK=2.2705\n",
      "  minibatch 10/10: full_lossK=2.2765 | diag_lossK=2.2765 | adam_lossK=2.2762\n",
      "\n",
      "======================================================================\n",
      "AGGREGATE RESULTS (across 3 seeds × 10 minibatches = 30 runs per method)\n",
      "======================================================================\n",
      "  euclid   : final_loss=2.3014 ± 0.0516 | time=5.95s ± 0.41s | cos(step1 vs full)=0.937 ± 0.074\n",
      "  adam     : final_loss=2.3005 ± 0.0515 | time=5.92s ± 0.47s | cos(step1 vs full)=0.725 ± 0.067\n",
      "  diag     : final_loss=2.3005 ± 0.0515 | time=7.32s ± 0.68s | cos(step1 vs full)=0.959 ± 0.041\n",
      "  block-diag: final_loss=2.3005 ± 0.0515 | time=7.47s ± 0.72s | cos(step1 vs full)=0.959 ± 0.041\n",
      "  full     : final_loss=2.3005 ± 0.0516 | time=18.46s ± 1.51s | cos(step1 vs full)=1.000\n",
      "\n",
      "Notes:\n",
      "  • Cosines compare the *applied step vector* at step 1 (not the raw gradient).\n",
      "  • Adam’s delta is its actual applied update on θ (per minibatch, fresh state).\n",
      "  • If you want Adam to carry momentum across minibatches, move AdamState outside run_one_minibatch().\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Mini-script: Euclid vs Adam vs FS-NG covariance test (euclid vs adam vs diag vs block-diag vs full)\n",
    "=================================================================================================\n",
    "\n",
    "What this does\n",
    "--------------\n",
    "Runs K update steps on *multiple fixed minibatches* (10 minibatches) for each of 3 seeds,\n",
    "updating ONLY the circuit weights θ, while holding the preprocessor + feature mapping + readout fixed.\n",
    "\n",
    "We compare five update rules:\n",
    "\n",
    "  1) euclid     : plain Euclidean gradient descent        θ <- θ - η * g\n",
    "  2) adam       : Adam optimizer on θ (PyTorch-style)     θ <- AdamStep(g)\n",
    "  3) diag       : FS/QFI natural gradient (metric_tensor approx=\"diag\")\n",
    "  4) block-diag : FS/QFI natural gradient (approx=\"block-diag\")\n",
    "  5) full       : FS/QFI natural gradient (approx=None, needs aux wire)\n",
    "\n",
    "Optional hybrid (“shrinkage” FS metric toward Euclidean)\n",
    "--------------------------------------------------------\n",
    "Instead of pure FS-NG, you can blend the metric toward identity:\n",
    "\n",
    "    G_eff = α * G_bar + (1-α) * I\n",
    "    step  = (G_eff + λ I)^(-1) g\n",
    "\n",
    "Set ALPHA_SHRINK in (0,1]. If you want *pure* FS-NG, set ALPHA_SHRINK=1.0.\n",
    "\n",
    "Goal\n",
    "----\n",
    "- See whether FS covariance structure helps over Euclidean GD and Adam\n",
    "- Quantify directional difference (cosine similarity vs full step direction)\n",
    "- Keep runtime low: small minibatches, 10 minibatches, K steps each, 3 seeds\n",
    "\n",
    "Requires\n",
    "--------\n",
    "- realnet_results.pt (from Block 1; contains frozen preprocessor weights)\n",
    "- pennylane\n",
    "- torchvision\n",
    "- optional: pennylane-lightning-gpu (for lightning.gpu)\n",
    "\n",
    "Notes\n",
    "-----\n",
    "- FULL metric_tensor (approx=None) requires an auxiliary wire on many devices.\n",
    "  We allocate N_QUBITS + 1 wires and reserve the last wire as aux.\n",
    "- Adam is implemented in a lightweight, deterministic “PyTorch Adam” way on θ only,\n",
    "  so you can compare it without rerunning multi-day experiments.\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as pnp\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Config (tune for speed)\n",
    "# ---------------------------\n",
    "SEEDS = [42, 123, 456]\n",
    "\n",
    "DATASET = \"FashionMNIST\"          # fixed per your request\n",
    "N_QUBITS = 4\n",
    "N_LAYERS = 2\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "N_MINIBATCHES = 10         # 10 minibatches per seed\n",
    "K_STEPS = 10               # steps per minibatch (kept from prior diagnostic)\n",
    "\n",
    "ETA = 0.01                 # Euclid/FS learning rate\n",
    "LAM = 1e-3                 # FS damping\n",
    "\n",
    "# Shrinkage: α=1 => pure FS, α<1 => blend toward Euclidean identity metric\n",
    "ALPHA_SHRINK = 1.0         # try 0.8 or 0.5 for “Euclid+FS” blend\n",
    "\n",
    "# Adam hyperparams (match your prior analysis spirit)\n",
    "ADAM_LR = 1e-3\n",
    "ADAM_BETAS = (0.9, 0.999)\n",
    "ADAM_EPS = 1e-8\n",
    "ADAM_WEIGHT_DECAY = 0.0\n",
    "\n",
    "PREFERRED_DEVICE = \"lightning.gpu\"\n",
    "FALLBACK_DEVICE = \"default.qubit\"\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Reproducibility\n",
    "# ---------------------------\n",
    "def set_all_seeds(seed: int):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    try:\n",
    "        qml.numpy.random.seed(seed)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Shared Preprocessor (Block 1)\n",
    "# ---------------------------\n",
    "class SharedPreprocessor(nn.Module):\n",
    "    \"\"\"Shared classical feature extractor: 784 → 16\"\"\"\n",
    "    def __init__(self, input_dim=784, bottleneck_dim=16):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, bottleneck_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return torch.tanh(self.fc(x))\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Device factory (+1 aux wire for full metric)\n",
    "# ---------------------------\n",
    "def make_device(n_qubits: int):\n",
    "    n_wires_total = n_qubits + 1\n",
    "    aux_wire = n_qubits\n",
    "    try:\n",
    "        dev = qml.device(PREFERRED_DEVICE, wires=n_wires_total)\n",
    "        backend = PREFERRED_DEVICE\n",
    "    except Exception as e:\n",
    "        dev = qml.device(FALLBACK_DEVICE, wires=n_wires_total)\n",
    "        backend = FALLBACK_DEVICE\n",
    "        print(f\"[device] Could not load {PREFERRED_DEVICE} ({type(e).__name__}: {e}). Using {FALLBACK_DEVICE}.\")\n",
    "    print(f\"[device] backend={backend}, wires={n_wires_total} (aux_wire={aux_wire})\")\n",
    "    return dev, backend, aux_wire\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Helpers\n",
    "# ---------------------------\n",
    "def _cosine(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    na = np.linalg.norm(a) + 1e-12\n",
    "    nb = np.linalg.norm(b) + 1e-12\n",
    "    return float(np.dot(a, b) / (na * nb))\n",
    "\n",
    "\n",
    "def _flat(theta) -> pnp.ndarray:\n",
    "    return pnp.reshape(theta, (-1,))\n",
    "\n",
    "\n",
    "def _unflat(v, shape) -> pnp.ndarray:\n",
    "    return pnp.reshape(v, shape)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Build circuit + metric fns\n",
    "# ---------------------------\n",
    "dev, backend, aux_wire = make_device(N_QUBITS)\n",
    "\n",
    "@qml.qnode(dev, interface=\"autograd\", diff_method=\"parameter-shift\")\n",
    "def circuit(inputs, weights):\n",
    "    # circuit uses ONLY wires 0..N_QUBITS-1; last wire is aux-reserved\n",
    "    for layer in range(N_LAYERS):\n",
    "        for i in range(N_QUBITS):\n",
    "            qml.RY(inputs[i], wires=i)\n",
    "        for i in range(N_QUBITS):\n",
    "            qml.RY(weights[layer, i, 0], wires=i)\n",
    "            qml.RZ(weights[layer, i, 1], wires=i)\n",
    "\n",
    "        for i in range(N_QUBITS - 1):\n",
    "            qml.CNOT(wires=[i, i + 1])\n",
    "        if N_QUBITS > 2:\n",
    "            qml.CNOT(wires=[N_QUBITS - 1, 0])\n",
    "\n",
    "    return (\n",
    "        qml.expval(qml.PauliZ(0)),\n",
    "        qml.expval(qml.PauliZ(1)),\n",
    "        qml.expval(qml.PauliZ(2)),\n",
    "        qml.expval(qml.PauliZ(3)),\n",
    "        qml.expval(qml.PauliZ(0) @ qml.PauliZ(1)),\n",
    "        qml.expval(qml.PauliZ(2) @ qml.PauliZ(3)),\n",
    "    )\n",
    "\n",
    "metric_full  = qml.metric_tensor(circuit, approx=None,       aux_wire=aux_wire)\n",
    "metric_diag  = qml.metric_tensor(circuit, approx=\"diag\")\n",
    "metric_block = qml.metric_tensor(circuit, approx=\"block-diag\")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Load frozen preprocessor from Block 1\n",
    "# ---------------------------\n",
    "realnet = torch.load(\"realnet_results1.pt\", weights_only=False)\n",
    "pre_state = realnet[\"preprocessor_state\"]\n",
    "\n",
    "pre = SharedPreprocessor(784, 16)\n",
    "pre.load_state_dict(pre_state)\n",
    "for p in pre.parameters():\n",
    "    p.requires_grad = False\n",
    "pre.eval()\n",
    "\n",
    "# frozen feature_select (16 -> 4)\n",
    "feature_select = nn.Linear(16, N_QUBITS)\n",
    "for p in feature_select.parameters():\n",
    "    p.requires_grad = False\n",
    "feature_select.eval()\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Dataset: MNIST (fixed)\n",
    "# ---------------------------\n",
    "norm_mean, norm_std = (0.1307,), (0.3081,)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std),\n",
    "])\n",
    "train_ds = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Fixed readout (constant) to isolate θ update behavior\n",
    "# ---------------------------\n",
    "C = 10\n",
    "W = pnp.array(np.random.randn(C, 6).astype(np.float64) * 0.1)  # (10,6)\n",
    "b = pnp.array(np.zeros((C,), dtype=np.float64))\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Loss factory (depends on current minibatch)\n",
    "# ---------------------------\n",
    "def make_batch_ce_loss(x_np, y_np):\n",
    "    def batch_ce_loss(theta):\n",
    "        losses = []\n",
    "        for i in range(len(y_np)):\n",
    "            q_raw = circuit(x_np[i], theta)\n",
    "            q = pnp.stack(q_raw)  # (6,)\n",
    "            logits = W @ q + b\n",
    "            logits = logits - pnp.max(logits)\n",
    "            ex = pnp.exp(logits)\n",
    "            probs = ex / pnp.sum(ex)\n",
    "            losses.append(-pnp.log(probs[int(y_np[i])] + 1e-12))\n",
    "        return pnp.mean(pnp.stack(losses))\n",
    "    return batch_ce_loss\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# FS-NG / Euclid delta\n",
    "# ---------------------------\n",
    "def fsng_or_euclid_delta(theta, approx: str, batch_ce_loss, x_np):\n",
    "    \"\"\"\n",
    "    approx in {\"euclid\",\"diag\",\"block-diag\",\"full\"}.\n",
    "\n",
    "    - euclid: delta = g\n",
    "    - others: delta = (G_eff + λI)^(-1) g\n",
    "              with optional shrinkage: G_eff = α G_bar + (1-α) I\n",
    "    \"\"\"\n",
    "    theta_shape = theta.shape\n",
    "    P = int(np.prod(theta_shape))\n",
    "    I = pnp.eye(P, dtype=pnp.float64)\n",
    "\n",
    "    g = qml.grad(batch_ce_loss)(theta)\n",
    "    g_flat = pnp.reshape(g, (P,))\n",
    "\n",
    "    if approx == \"euclid\":\n",
    "        return np.array(g_flat, dtype=np.float64)\n",
    "\n",
    "    if approx == \"diag\":\n",
    "        metric_fn = metric_diag\n",
    "    elif approx == \"block-diag\":\n",
    "        metric_fn = metric_block\n",
    "    elif approx == \"full\":\n",
    "        metric_fn = metric_full\n",
    "    else:\n",
    "        raise ValueError(\"approx must be one of: 'euclid','diag','block-diag','full'\")\n",
    "\n",
    "    G_sum = pnp.zeros((P, P), dtype=pnp.float64)\n",
    "    for i in range(x_np.shape[0]):\n",
    "        Gi = metric_fn(x_np[i], theta)\n",
    "        Gi = pnp.reshape(Gi, (P, P))\n",
    "        G_sum = G_sum + Gi\n",
    "    G_bar = G_sum / float(x_np.shape[0])\n",
    "\n",
    "    alpha = float(ALPHA_SHRINK)\n",
    "    G_eff = alpha * G_bar + (1.0 - alpha) * I\n",
    "    G_reg = G_eff + LAM * I\n",
    "\n",
    "    delta = pnp.linalg.solve(G_reg, g_flat)\n",
    "    return np.array(delta, dtype=np.float64)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Adam (PyTorch-style) on θ only\n",
    "# ---------------------------\n",
    "class AdamState:\n",
    "    def __init__(self, P, betas=(0.9, 0.999)):\n",
    "        self.t = 0\n",
    "        self.m = np.zeros((P,), dtype=np.float64)\n",
    "        self.v = np.zeros((P,), dtype=np.float64)\n",
    "        self.beta1, self.beta2 = betas\n",
    "\n",
    "def adam_step(theta, grad_flat, state: AdamState, lr=1e-3, eps=1e-8, weight_decay=0.0):\n",
    "    \"\"\"\n",
    "    Deterministic Adam update on a *flat* theta vector.\n",
    "    Returns updated flat theta and the applied update direction (delta).\n",
    "    \"\"\"\n",
    "    state.t += 1\n",
    "\n",
    "    g = grad_flat.astype(np.float64)\n",
    "    if weight_decay != 0.0:\n",
    "        g = g + weight_decay * theta\n",
    "\n",
    "    b1, b2 = state.beta1, state.beta2\n",
    "    state.m = b1 * state.m + (1.0 - b1) * g\n",
    "    state.v = b2 * state.v + (1.0 - b2) * (g * g)\n",
    "\n",
    "    mhat = state.m / (1.0 - b1 ** state.t)\n",
    "    vhat = state.v / (1.0 - b2 ** state.t)\n",
    "\n",
    "    step = lr * mhat / (np.sqrt(vhat) + eps)   # this is the update magnitude\n",
    "    theta_new = theta - step\n",
    "    return theta_new, step\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# One minibatch run for one method\n",
    "# ---------------------------\n",
    "def run_one_minibatch(theta0, method, x_np, y_np):\n",
    "    \"\"\"\n",
    "    Runs K_STEPS on a single minibatch and returns:\n",
    "      losses: (K_STEPS,)\n",
    "      delta1: first-step update direction (flat, numpy)\n",
    "      elapsed\n",
    "    \"\"\"\n",
    "    theta = pnp.array(theta0, requires_grad=True)\n",
    "    theta_shape = theta.shape\n",
    "    P = int(np.prod(theta_shape))\n",
    "\n",
    "    batch_ce_loss = make_batch_ce_loss(x_np, y_np)\n",
    "\n",
    "    # Adam needs state per minibatch (matches your “diagnostic” intent)\n",
    "    adam_state = AdamState(P, betas=ADAM_BETAS) if method == \"adam\" else None\n",
    "\n",
    "    losses = []\n",
    "    delta1 = None\n",
    "\n",
    "    t0 = time.time()\n",
    "    for k in range(K_STEPS):\n",
    "        loss_before = float(batch_ce_loss(theta))\n",
    "\n",
    "        if method == \"adam\":\n",
    "            g = qml.grad(batch_ce_loss)(theta)\n",
    "            g_flat = np.array(pnp.reshape(g, (P,)), dtype=np.float64)\n",
    "\n",
    "            theta_flat = np.array(pnp.reshape(theta, (P,)), dtype=np.float64)\n",
    "            theta_flat_new, step_vec = adam_step(\n",
    "                theta_flat, g_flat, adam_state,\n",
    "                lr=ADAM_LR, eps=ADAM_EPS, weight_decay=ADAM_WEIGHT_DECAY\n",
    "            )\n",
    "            # For cosine comparisons, treat Adam's *applied update* as delta\n",
    "            delta = step_vec / (ADAM_LR + 1e-12)  # normalized-ish direction (optional)\n",
    "            theta = _unflat(theta_flat_new, theta_shape)\n",
    "\n",
    "        else:\n",
    "            delta = fsng_or_euclid_delta(theta, method, batch_ce_loss, x_np)\n",
    "            theta_flat = np.array(pnp.reshape(theta, (P,)), dtype=np.float64)\n",
    "            theta = _unflat(theta_flat - ETA * delta, theta_shape)\n",
    "\n",
    "        loss_after = float(batch_ce_loss(theta))\n",
    "        losses.append(loss_after)\n",
    "\n",
    "        if k == 0:\n",
    "            # Store first-step applied direction in comparable units:\n",
    "            # - for FS/euclid: delta is the preconditioned direction (so update is ETA*delta)\n",
    "            # - for Adam: delta above is roughly direction; we’ll store the *applied step* too\n",
    "            if method == \"adam\":\n",
    "                delta1 = np.array(step_vec, dtype=np.float64)  # actual applied update\n",
    "            else:\n",
    "                delta1 = np.array(ETA * delta, dtype=np.float64)  # applied update\n",
    "\n",
    "    return np.array(losses), delta1, time.time() - t0\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Collect minibatches (deterministic but seed-dependent)\n",
    "# ---------------------------\n",
    "def get_minibatches(seed, n_minibatches, batch_size):\n",
    "    \"\"\"\n",
    "    Returns list of (x_np, y_np) minibatches.\n",
    "    Uses a deterministic RNG over indices so we don't depend on DataLoader nondeterminism.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    batches = []\n",
    "    for _ in range(n_minibatches):\n",
    "        idx = rng.choice(len(train_ds), size=batch_size, replace=False)\n",
    "        x0, y0 = zip(*[train_ds[i] for i in idx])\n",
    "        x0 = torch.stack(list(x0), dim=0)\n",
    "        y0 = torch.tensor(list(y0), dtype=torch.long)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            feats16 = pre(x0)\n",
    "            xq = torch.tanh(feature_select(feats16))\n",
    "\n",
    "        batches.append((xq.cpu().numpy(), y0.cpu().numpy()))\n",
    "    return batches\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Main experiment loop\n",
    "# ---------------------------\n",
    "METHODS = [\"euclid\", \"adam\", \"diag\", \"block-diag\", \"full\"]\n",
    "\n",
    "def summarize(arr):\n",
    "    return float(np.mean(arr)), float(np.std(arr))\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(f\"\\n[Euclid vs Adam vs FS-NG] DATASET={DATASET} | backend={backend}\")\n",
    "    print(f\"  layers={N_LAYERS} | batch={BATCH_SIZE} | minibatches={N_MINIBATCHES} | steps={K_STEPS}\")\n",
    "    print(f\"  ETA={ETA} | LAM={LAM} | alpha_shrink={ALPHA_SHRINK} | Adam(lr={ADAM_LR}, betas={ADAM_BETAS})\\n\")\n",
    "\n",
    "    # Store per-method results across (seed, minibatch)\n",
    "    final_losses = {m: [] for m in METHODS}\n",
    "    times = {m: [] for m in METHODS}\n",
    "    cos_vs_full = {m: [] for m in METHODS if m != \"full\"}  # cosine of step1 update vs full step1 update\n",
    "\n",
    "    for seed in SEEDS:\n",
    "        set_all_seeds(seed)\n",
    "        print(f\"\\n{'='*70}\\nSEED {seed}\\n{'='*70}\")\n",
    "\n",
    "        # same initial theta per seed across methods\n",
    "        theta0 = pnp.array(np.random.randn(N_LAYERS, N_QUBITS, 2) * 0.1, requires_grad=True)\n",
    "\n",
    "        minibatches = get_minibatches(seed, N_MINIBATCHES, BATCH_SIZE)\n",
    "\n",
    "        for mb_i, (x_np, y_np) in enumerate(minibatches, start=1):\n",
    "            # run full first for reference direction cosine\n",
    "            full_losses, full_delta1, full_t = run_one_minibatch(theta0, \"full\", x_np, y_np)\n",
    "            final_losses[\"full\"].append(full_losses[-1])\n",
    "            times[\"full\"].append(full_t)\n",
    "\n",
    "            # run others\n",
    "            for m in [\"euclid\", \"adam\", \"diag\", \"block-diag\"]:\n",
    "                losses, delta1, tsec = run_one_minibatch(theta0, m, x_np, y_np)\n",
    "                final_losses[m].append(losses[-1])\n",
    "                times[m].append(tsec)\n",
    "\n",
    "                # cosine on step-1 APPLIED update (so compare apples-to-apples)\n",
    "                cos_vs_full[m].append(_cosine(delta1, full_delta1))\n",
    "\n",
    "            if mb_i in (1, N_MINIBATCHES):\n",
    "                print(f\"  minibatch {mb_i:2d}/{N_MINIBATCHES}: \"\n",
    "                      f\"full_lossK={full_losses[-1]:.4f} | \"\n",
    "                      f\"diag_lossK={final_losses['diag'][-1]:.4f} | \"\n",
    "                      f\"adam_lossK={final_losses['adam'][-1]:.4f}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"AGGREGATE RESULTS (across 3 seeds × 10 minibatches = 30 runs per method)\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Report: mean±std final loss, mean±std time, mean cosine vs full\n",
    "    for m in METHODS:\n",
    "        muL, sdL = summarize(final_losses[m])\n",
    "        muT, sdT = summarize(times[m])\n",
    "        if m == \"full\":\n",
    "            print(f\"  {m:9s}: final_loss={muL:.4f} ± {sdL:.4f} | time={muT:.2f}s ± {sdT:.2f}s | cos(step1 vs full)=1.000\")\n",
    "        else:\n",
    "            muC, sdC = summarize(cos_vs_full[m])\n",
    "            print(f\"  {m:9s}: final_loss={muL:.4f} ± {sdL:.4f} | time={muT:.2f}s ± {sdT:.2f}s | cos(step1 vs full)={muC:.3f} ± {sdC:.3f}\")\n",
    "\n",
    "    print(\"\\nNotes:\")\n",
    "    print(\"  • Cosines compare the *applied step vector* at step 1 (not the raw gradient).\")\n",
    "    print(\"  • Adam’s delta is its actual applied update on θ (per minibatch, fresh state).\")\n",
    "    print(\"  • If you want Adam to carry momentum across minibatches, move AdamState outside run_one_minibatch().\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c4c8f0-1304-43c3-a5e9-f6e469df4df1",
   "metadata": {},
   "source": [
    "# RealNet CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db697d06-3969-4098-abd4-543aa067f949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch device: cuda\n",
      "======================================================================\n",
      "BLOCK 1: RealNet Training (CIFAR-10)\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "  • Dataset: CIFAR-10 (32×32 RGB)\n",
      "  • Input dimension: 3072 (32×32×3 flattened)\n",
      "  • Batch size: 128\n",
      "  • Patience: 10\n",
      "  • Seeds: [42, 123, 456]\n",
      "  • Architecture: 3072 → 16 → 64 → 10\n",
      "======================================================================\n",
      "\n",
      "Creating stratified samples...\n",
      "  Sampling took 0.00s\n",
      "  Train samples: 15000 (stratified, 1500 per class)\n",
      "  Test samples:  3000 (stratified, 300 per class)\n",
      "\n",
      "======================================================================\n",
      "SEED 42\n",
      "======================================================================\n",
      "\n",
      "  Training RealNet (seed=42)...\n",
      "  [Real] Epoch  1 | loss=2.0209 | test_acc=0.3273 | time=0.5s\n",
      "  [Real] Epoch  2 | loss=1.8281 | test_acc=0.3470 | time=0.9s\n",
      "  [Real] Epoch  3 | loss=1.7635 | test_acc=0.3577 | time=1.4s\n",
      "  [Real] Epoch  4 | loss=1.7250 | test_acc=0.3617 | time=1.8s\n",
      "  [Real] Epoch  5 | loss=1.6946 | test_acc=0.3710 | time=2.3s\n",
      "  [Real] Epoch  6 | loss=1.6776 | test_acc=0.3710 | time=2.7s\n",
      "  [Real] Epoch  7 | loss=1.6463 | test_acc=0.3630 | time=3.2s\n",
      "  [Real] Epoch  8 | loss=1.6154 | test_acc=0.3730 | time=3.6s\n",
      "  [Real] Epoch  9 | loss=1.6088 | test_acc=0.3753 | time=4.1s\n",
      "  [Real] Epoch 10 | loss=1.5864 | test_acc=0.3817 | time=4.5s\n",
      "  [Real] Epoch 11 | loss=1.5683 | test_acc=0.3740 | time=5.0s\n",
      "  [Real] Epoch 12 | loss=1.5486 | test_acc=0.3860 | time=5.5s\n",
      "  [Real] Epoch 13 | loss=1.5296 | test_acc=0.3843 | time=5.9s\n",
      "  [Real] Epoch 14 | loss=1.5265 | test_acc=0.3827 | time=6.4s\n",
      "  [Real] Epoch 15 | loss=1.4951 | test_acc=0.3813 | time=6.8s\n",
      "  [Real] Epoch 16 | loss=1.4895 | test_acc=0.3793 | time=7.3s\n",
      "  [Real] Epoch 17 | loss=1.4768 | test_acc=0.3917 | time=7.7s\n",
      "  [Real] Epoch 18 | loss=1.4662 | test_acc=0.3953 | time=8.2s\n",
      "  [Real] Epoch 19 | loss=1.4526 | test_acc=0.3973 | time=8.7s\n",
      "  [Real] Epoch 20 | loss=1.4447 | test_acc=0.3897 | time=9.1s\n",
      "  [Real] Epoch 21 | loss=1.4271 | test_acc=0.3820 | time=9.6s\n",
      "  [Real] Epoch 22 | loss=1.4293 | test_acc=0.3757 | time=10.1s\n",
      "  [Real] Epoch 23 | loss=1.4119 | test_acc=0.3840 | time=10.5s\n",
      "  [Real] Epoch 24 | loss=1.4068 | test_acc=0.3870 | time=11.0s\n",
      "  [Real] Epoch 25 | loss=1.3962 | test_acc=0.3813 | time=11.4s\n",
      "  [Real] Epoch 26 | loss=1.3853 | test_acc=0.3960 | time=11.9s\n",
      "  [Real] Epoch 27 | loss=1.3881 | test_acc=0.3860 | time=12.3s\n",
      "  [Real] Epoch 28 | loss=1.3623 | test_acc=0.3900 | time=12.8s\n",
      "  [Real] Epoch 29 | loss=1.3606 | test_acc=0.3833 | time=13.3s\n",
      "  [Real] Early stop at epoch 29 (no improvement for 10 epochs)\n",
      "\n",
      "  → Saved preprocessor state from seed 42\n",
      "\n",
      "======================================================================\n",
      "SEED 123\n",
      "======================================================================\n",
      "\n",
      "  Training RealNet (seed=123)...\n",
      "  [Real] Epoch  1 | loss=1.9978 | test_acc=0.3283 | time=0.5s\n",
      "  [Real] Epoch  2 | loss=1.8132 | test_acc=0.3567 | time=0.9s\n",
      "  [Real] Epoch  3 | loss=1.7601 | test_acc=0.3523 | time=1.4s\n",
      "  [Real] Epoch  4 | loss=1.7176 | test_acc=0.3880 | time=1.8s\n",
      "  [Real] Epoch  5 | loss=1.6878 | test_acc=0.3850 | time=2.3s\n",
      "  [Real] Epoch  6 | loss=1.6528 | test_acc=0.3907 | time=2.7s\n",
      "  [Real] Epoch  7 | loss=1.6329 | test_acc=0.3910 | time=3.2s\n",
      "  [Real] Epoch  8 | loss=1.6106 | test_acc=0.3867 | time=3.6s\n",
      "  [Real] Epoch  9 | loss=1.5985 | test_acc=0.3820 | time=4.0s\n",
      "  [Real] Epoch 10 | loss=1.5776 | test_acc=0.3910 | time=4.5s\n",
      "  [Real] Epoch 11 | loss=1.5712 | test_acc=0.4080 | time=4.9s\n",
      "  [Real] Epoch 12 | loss=1.5525 | test_acc=0.3990 | time=5.3s\n",
      "  [Real] Epoch 13 | loss=1.5326 | test_acc=0.3943 | time=5.8s\n",
      "  [Real] Epoch 14 | loss=1.5207 | test_acc=0.4037 | time=6.2s\n",
      "  [Real] Epoch 15 | loss=1.5134 | test_acc=0.3927 | time=6.6s\n",
      "  [Real] Epoch 16 | loss=1.4963 | test_acc=0.4017 | time=7.1s\n",
      "  [Real] Epoch 17 | loss=1.4777 | test_acc=0.3950 | time=7.5s\n",
      "  [Real] Epoch 18 | loss=1.4688 | test_acc=0.3897 | time=8.0s\n",
      "  [Real] Epoch 19 | loss=1.4598 | test_acc=0.3920 | time=8.5s\n",
      "  [Real] Epoch 20 | loss=1.4584 | test_acc=0.4077 | time=8.9s\n",
      "  [Real] Epoch 21 | loss=1.4383 | test_acc=0.3930 | time=9.3s\n",
      "  [Real] Early stop at epoch 21 (no improvement for 10 epochs)\n",
      "\n",
      "======================================================================\n",
      "SEED 456\n",
      "======================================================================\n",
      "\n",
      "  Training RealNet (seed=456)...\n",
      "  [Real] Epoch  1 | loss=2.0038 | test_acc=0.3460 | time=0.4s\n",
      "  [Real] Epoch  2 | loss=1.8185 | test_acc=0.3640 | time=0.9s\n",
      "  [Real] Epoch  3 | loss=1.7599 | test_acc=0.3680 | time=1.3s\n",
      "  [Real] Epoch  4 | loss=1.7233 | test_acc=0.3727 | time=2.3s\n",
      "  [Real] Epoch  5 | loss=1.6974 | test_acc=0.3753 | time=2.8s\n",
      "  [Real] Epoch  6 | loss=1.6636 | test_acc=0.3820 | time=3.3s\n",
      "  [Real] Epoch  7 | loss=1.6303 | test_acc=0.3860 | time=3.7s\n",
      "  [Real] Epoch  8 | loss=1.6089 | test_acc=0.3873 | time=4.2s\n",
      "  [Real] Epoch  9 | loss=1.5883 | test_acc=0.3923 | time=4.6s\n",
      "  [Real] Epoch 10 | loss=1.5799 | test_acc=0.3850 | time=5.1s\n",
      "  [Real] Epoch 11 | loss=1.5520 | test_acc=0.3883 | time=5.5s\n",
      "  [Real] Epoch 12 | loss=1.5503 | test_acc=0.4017 | time=6.0s\n",
      "  [Real] Epoch 13 | loss=1.5302 | test_acc=0.3897 | time=6.4s\n",
      "  [Real] Epoch 14 | loss=1.5149 | test_acc=0.3967 | time=6.8s\n",
      "  [Real] Epoch 15 | loss=1.5073 | test_acc=0.3980 | time=7.3s\n",
      "  [Real] Epoch 16 | loss=1.4952 | test_acc=0.3970 | time=7.8s\n",
      "  [Real] Epoch 17 | loss=1.4872 | test_acc=0.3933 | time=8.2s\n",
      "  [Real] Epoch 18 | loss=1.4698 | test_acc=0.3927 | time=8.7s\n",
      "  [Real] Epoch 19 | loss=1.4535 | test_acc=0.3893 | time=9.1s\n",
      "  [Real] Epoch 20 | loss=1.4378 | test_acc=0.3890 | time=9.5s\n",
      "  [Real] Epoch 21 | loss=1.4339 | test_acc=0.3923 | time=10.0s\n",
      "  [Real] Epoch 22 | loss=1.4194 | test_acc=0.3960 | time=10.5s\n",
      "  [Real] Early stop at epoch 22 (no improvement for 10 epochs)\n",
      "\n",
      "======================================================================\n",
      "REALNET SUMMARY (CIFAR-10)\n",
      "======================================================================\n",
      "\n",
      "Accuracy:   0.4023 ± 0.0044\n",
      "Time:       11.0s ± 1.7s\n",
      "Epochs:     24.0 ± 3.6\n",
      "Parameters: 50,906\n",
      "\n",
      "Per-seed results:\n",
      "  Seed 42: acc=0.3973, time=13.3s, epochs=29\n",
      "  Seed 123: acc=0.4080, time=9.3s, epochs=21\n",
      "  Seed 456: acc=0.4017, time=10.5s, epochs=22\n",
      "\n",
      "✓ Saved results to: realnet_cifar10_results.pt\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Block 1: RealNet Training (CIFAR-10)\n",
    "=====================================\n",
    "Trains the baseline Real MLP on 3 seeds using CIFAR-10.\n",
    "Saves the trained preprocessor for reuse in subsequent blocks.\n",
    "\n",
    "Outputs:\n",
    "- realnet_results.pt: Contains results dict and trained preprocessor state\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "print(f\"Using PyTorch device: {device}\")\n",
    "\n",
    "# ============================================\n",
    "# Comprehensive seed setting for reproducibility\n",
    "# ============================================\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    \"\"\"Set seeds for all RNG sources for reproducibility\"\"\"\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    # Python\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # NumPy\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # PyTorch deterministic mode\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # CuPy (if available)\n",
    "    try:\n",
    "        import cupy as cp\n",
    "        cp.random.seed(seed)\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    # PennyLane (if available)\n",
    "    try:\n",
    "        import pennylane as qml\n",
    "        qml.numpy.random.seed(seed)\n",
    "    except (ImportError, AttributeError):\n",
    "        pass\n",
    "\n",
    "# ============================================\n",
    "# Shared Bottleneck Preprocessor: 3072 → 16\n",
    "# ============================================\n",
    "\n",
    "class SharedPreprocessor(nn.Module):\n",
    "    \"\"\"Shared classical feature extractor: 3072 → 16\"\"\"\n",
    "    def __init__(self, input_dim=3072, bottleneck_dim=16):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, bottleneck_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten to (batch, 3072)\n",
    "        x = torch.tanh(self.fc(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Real-valued Head and Network\n",
    "# ============================================\n",
    "\n",
    "class RealHead(nn.Module):\n",
    "    \"\"\"Standard MLP: 16 → 64 → 10\"\"\"\n",
    "    def __init__(self, bottleneck_dim=16, hidden_dim=64, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(bottleneck_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RealNet(nn.Module):\n",
    "    \"\"\"Complete Real network: Preprocessor + RealHead\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.preprocessor = SharedPreprocessor(3072, 16)\n",
    "        self.head = RealHead(16, 64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.preprocessor(x)\n",
    "        return self.head(features)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Training and Evaluation\n",
    "# ============================================\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, device, show_progress=False):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        total_samples += x.size(0)\n",
    "\n",
    "        if show_progress and batch_idx % 50 == 0:\n",
    "            print(f\"    Batch {batch_idx}/{len(loader)}, samples: {total_samples}\", end=\"\\r\")\n",
    "\n",
    "    if show_progress:\n",
    "        print()\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += x.size(0)\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "def train_with_early_stopping(model, train_loader, test_loader, optimizer,\n",
    "                              device, max_epochs=40, patience=10, name=\"Model\"):\n",
    "    if isinstance(device, str):\n",
    "        device = torch.device(device)\n",
    "\n",
    "    best_acc = 0.0\n",
    "    epochs_without_improvement = 0\n",
    "    start = time.time()\n",
    "    last_acc = 0.0\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        loss = train_one_epoch(model, train_loader, optimizer, device, show_progress=False)\n",
    "        acc = evaluate(model, test_loader, device)\n",
    "        last_acc = acc\n",
    "\n",
    "        elapsed = time.time() - start\n",
    "        print(f\"  [{name}] Epoch {epoch:2d} | loss={loss:.4f} \"\n",
    "              f\"| test_acc={acc:.4f} | time={elapsed:.1f}s\")\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"  [{name}] Early stop at epoch {epoch} \"\n",
    "                  f\"(no improvement for {patience} epochs)\")\n",
    "            break\n",
    "\n",
    "    total_time = time.time() - start\n",
    "    return {\n",
    "        \"best_acc\": best_acc,\n",
    "        \"final_acc\": last_acc,\n",
    "        \"time\": total_time,\n",
    "        \"epochs\": epoch,\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Data utilities\n",
    "# ============================================\n",
    "\n",
    "def stratified_sample_from_targets(dataset, n_samples_per_class, seed=42):\n",
    "    \"\"\"\n",
    "    Create a stratified sample with n_samples_per_class from each class.\n",
    "    Uses dataset.targets directly - NO image loading during sampling.\n",
    "    \"\"\"\n",
    "    # CIFAR-10 exposes targets as a list\n",
    "    if hasattr(dataset, 'targets'):\n",
    "        targets = np.array(dataset.targets)\n",
    "    else:\n",
    "        # Fallback for wrapped datasets\n",
    "        targets = np.array([dataset[i][1] for i in range(len(dataset))])\n",
    "    \n",
    "    rng = np.random.RandomState(seed)\n",
    "    \n",
    "    sampled_indices = []\n",
    "    for c in range(10):\n",
    "        # Find all indices for class c\n",
    "        idx_c = np.where(targets == c)[0]\n",
    "        k = min(n_samples_per_class, len(idx_c))\n",
    "        # Sample without replacement\n",
    "        selected = rng.choice(idx_c, size=k, replace=False).tolist()\n",
    "        sampled_indices.extend(selected)\n",
    "    \n",
    "    # Shuffle the combined indices\n",
    "    rng.shuffle(sampled_indices)\n",
    "    return sampled_indices\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Main\n",
    "# ============================================\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"BLOCK 1: RealNet Training (CIFAR-10)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nConfiguration:\")\n",
    "    print(\"  • Dataset: CIFAR-10 (32×32 RGB)\")\n",
    "    print(\"  • Input dimension: 3072 (32×32×3 flattened)\")\n",
    "    print(\"  • Batch size: 128\")\n",
    "    print(\"  • Patience: 10\")\n",
    "    print(\"  • Seeds: [42, 123, 456]\")\n",
    "    print(\"  • Architecture: 3072 → 16 → 64 → 10\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # CIFAR-10 normalization (channel-wise means and stds)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.4914, 0.4822, 0.4465],\n",
    "            std=[0.2470, 0.2435, 0.2616]\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    # Load full datasets\n",
    "    full_train_ds = datasets.CIFAR10(root=\"./data\", train=True,\n",
    "                                     download=True, transform=transform)\n",
    "    full_test_ds = datasets.CIFAR10(root=\"./data\", train=False,\n",
    "                                    download=True, transform=transform)\n",
    "\n",
    "    # Create stratified samples (fast - uses targets only, no image loading)\n",
    "    print(\"\\nCreating stratified samples...\")\n",
    "    t0 = time.time()\n",
    "    train_indices = stratified_sample_from_targets(full_train_ds, n_samples_per_class=1500, seed=42)\n",
    "    test_indices = stratified_sample_from_targets(full_test_ds, n_samples_per_class=300, seed=42)\n",
    "    print(f\"  Sampling took {time.time()-t0:.2f}s\")\n",
    "    \n",
    "    train_ds = Subset(full_train_ds, train_indices)\n",
    "    test_ds = Subset(full_test_ds, test_indices)\n",
    "    \n",
    "    print(f\"  Train samples: {len(train_ds)} (stratified, 1500 per class)\")\n",
    "    print(f\"  Test samples:  {len(test_ds)} (stratified, 300 per class)\")\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=128,\n",
    "                              shuffle=True, num_workers=4)\n",
    "    test_loader = DataLoader(test_ds, batch_size=256,\n",
    "                             shuffle=False, num_workers=4)\n",
    "\n",
    "    seeds = [42, 123, 456]\n",
    "    all_results = []\n",
    "    trained_preprocessor_state = None\n",
    "\n",
    "    for seed in seeds:\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(f\"SEED {seed}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        set_all_seeds(seed)\n",
    "\n",
    "        print(f\"\\n  Training RealNet (seed={seed})...\")\n",
    "        real_model = RealNet().to(device)\n",
    "        real_opt = torch.optim.Adam(real_model.parameters(), lr=1e-3)\n",
    "        \n",
    "        result = train_with_early_stopping(\n",
    "            real_model, train_loader, test_loader, real_opt, device,\n",
    "            max_epochs=40, patience=10, name=\"Real\"\n",
    "        )\n",
    "        result[\"params\"] = sum(p.numel() for p in real_model.parameters())\n",
    "        result[\"seed\"] = seed\n",
    "        \n",
    "        all_results.append(result)\n",
    "        \n",
    "        # Save preprocessor from first seed\n",
    "        if trained_preprocessor_state is None:\n",
    "            trained_preprocessor_state = real_model.preprocessor.state_dict()\n",
    "            print(f\"\\n  → Saved preprocessor state from seed {seed}\")\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"REALNET SUMMARY (CIFAR-10)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    accs = [r[\"best_acc\"] for r in all_results]\n",
    "    times = [r[\"time\"] for r in all_results]\n",
    "    epochs = [r[\"epochs\"] for r in all_results]\n",
    "    params = all_results[0][\"params\"]\n",
    "    \n",
    "    print(f\"\\nAccuracy:   {np.mean(accs):.4f} ± {np.std(accs):.4f}\")\n",
    "    print(f\"Time:       {np.mean(times):.1f}s ± {np.std(times):.1f}s\")\n",
    "    print(f\"Epochs:     {np.mean(epochs):.1f} ± {np.std(epochs):.1f}\")\n",
    "    print(f\"Parameters: {params:,}\")\n",
    "    \n",
    "    print(\"\\nPer-seed results:\")\n",
    "    for r in all_results:\n",
    "        print(f\"  Seed {r['seed']}: acc={r['best_acc']:.4f}, \"\n",
    "              f\"time={r['time']:.1f}s, epochs={r['epochs']}\")\n",
    "\n",
    "    # Save results and preprocessor\n",
    "    save_dict = {\n",
    "        \"results\": all_results,\n",
    "        \"preprocessor_state\": trained_preprocessor_state,\n",
    "        \"summary\": {\n",
    "            \"mean_acc\": np.mean(accs),\n",
    "            \"std_acc\": np.std(accs),\n",
    "            \"mean_time\": np.mean(times),\n",
    "            \"params\": params\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    torch.save(save_dict, \"realnet_cifar10_results.pt\")\n",
    "    print(f\"\\n✓ Saved results to: realnet_cifar10_results.pt\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b69a11e-2c9b-4a50-8101-ce4bbd6a13e4",
   "metadata": {},
   "source": [
    "# Diagnostic CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fdd8e26-dd4b-400a-b4d0-a4ee3d3ce14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[device] backend=lightning.gpu, wires=5 (aux_wire=4)\n",
      "\n",
      "[Euclid vs Adam vs FS-NG] DATASET=CIFAR-10 | backend=lightning.gpu\n",
      "  layers=2 | batch=8 | minibatches=10 | steps=10\n",
      "  ETA=0.01 | LAM=0.001 | alpha_shrink=1.0 | Adam(lr=0.001, betas=(0.9, 0.999))\n",
      "\n",
      "\n",
      "======================================================================\n",
      "SEED 42\n",
      "======================================================================\n",
      "  minibatch  1/10: full_lossK=2.3282 | diag_lossK=2.3282 | adam_lossK=2.3290\n",
      "  minibatch 10/10: full_lossK=2.3053 | diag_lossK=2.3053 | adam_lossK=2.3054\n",
      "\n",
      "======================================================================\n",
      "SEED 123\n",
      "======================================================================\n",
      "  minibatch  1/10: full_lossK=2.3336 | diag_lossK=2.3338 | adam_lossK=2.3337\n",
      "  minibatch 10/10: full_lossK=2.3474 | diag_lossK=2.3473 | adam_lossK=2.3479\n",
      "\n",
      "======================================================================\n",
      "SEED 456\n",
      "======================================================================\n",
      "  minibatch  1/10: full_lossK=2.3449 | diag_lossK=2.3449 | adam_lossK=2.3446\n",
      "  minibatch 10/10: full_lossK=2.3083 | diag_lossK=2.3082 | adam_lossK=2.3079\n",
      "\n",
      "======================================================================\n",
      "AGGREGATE RESULTS (across 3 seeds × 10 minibatches = 30 runs per method)\n",
      "======================================================================\n",
      "  euclid   : final_loss=2.3204 ± 0.0357 | time=6.09s ± 0.52s | cos(step1 vs full)=0.958 ± 0.031\n",
      "  adam     : final_loss=2.3196 ± 0.0357 | time=5.97s ± 0.39s | cos(step1 vs full)=0.689 ± 0.099\n",
      "  diag     : final_loss=2.3197 ± 0.0356 | time=7.48s ± 0.70s | cos(step1 vs full)=0.974 ± 0.016\n",
      "  block-diag: final_loss=2.3197 ± 0.0356 | time=7.37s ± 0.59s | cos(step1 vs full)=0.974 ± 0.016\n",
      "  full     : final_loss=2.3197 ± 0.0356 | time=18.23s ± 1.36s | cos(step1 vs full)=1.000\n",
      "\n",
      "Notes:\n",
      "  • Cosines compare the *applied step vector* at step 1 (not the raw gradient).\n",
      "  • Adam's delta is its actual applied update on θ (per minibatch, fresh state).\n",
      "  • If you want Adam to carry momentum across minibatches, move AdamState outside run_one_minibatch().\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Mini-script: Euclid vs Adam vs FS-NG covariance test (CIFAR-10)\n",
    "================================================================\n",
    "\n",
    "What this does\n",
    "--------------\n",
    "Runs K update steps on *multiple fixed minibatches* (10 minibatches) for each of 3 seeds,\n",
    "updating ONLY the circuit weights θ, while holding the preprocessor + feature mapping + readout fixed.\n",
    "\n",
    "We compare five update rules:\n",
    "\n",
    "  1) euclid     : plain Euclidean gradient descent        θ <- θ - η * g\n",
    "  2) adam       : Adam optimizer on θ (PyTorch-style)     θ <- AdamStep(g)\n",
    "  3) diag       : FS/QFI natural gradient (metric_tensor approx=\"diag\")\n",
    "  4) block-diag : FS/QFI natural gradient (approx=\"block-diag\")\n",
    "  5) full       : FS/QFI natural gradient (approx=None, needs aux wire)\n",
    "\n",
    "Optional hybrid (\"shrinkage\" FS metric toward Euclidean)\n",
    "--------------------------------------------------------\n",
    "Instead of pure FS-NG, you can blend the metric toward identity:\n",
    "\n",
    "    G_eff = α * G_bar + (1-α) * I\n",
    "    step  = (G_eff + λ I)^(-1) g\n",
    "\n",
    "Set ALPHA_SHRINK in (0,1]. If you want *pure* FS-NG, set ALPHA_SHRINK=1.0.\n",
    "\n",
    "Goal\n",
    "----\n",
    "- See whether FS covariance structure helps over Euclidean GD and Adam\n",
    "- Quantify directional difference (cosine similarity vs full step direction)\n",
    "- Keep runtime low: small minibatches, 10 minibatches, K steps each, 3 seeds\n",
    "\n",
    "Requires\n",
    "--------\n",
    "- realnet_cifar10_results.pt (from Block 1; contains frozen preprocessor weights)\n",
    "- pennylane\n",
    "- torchvision\n",
    "- optional: pennylane-lightning-gpu (for lightning.gpu)\n",
    "\n",
    "Notes\n",
    "-----\n",
    "- FULL metric_tensor (approx=None) requires an auxiliary wire on many devices.\n",
    "  We allocate N_QUBITS + 1 wires and reserve the last wire as aux.\n",
    "- Adam is implemented in a lightweight, deterministic \"PyTorch Adam\" way on θ only,\n",
    "  so you can compare it without rerunning multi-day experiments.\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as pnp\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Config (tune for speed)\n",
    "# ---------------------------\n",
    "SEEDS = [42, 123, 456]\n",
    "\n",
    "DATASET = \"CIFAR-10\"               # changed from FashionMNIST\n",
    "N_QUBITS = 4\n",
    "N_LAYERS = 2\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "N_MINIBATCHES = 10         # 10 minibatches per seed\n",
    "K_STEPS = 10               # steps per minibatch (kept from prior diagnostic)\n",
    "\n",
    "ETA = 0.01                 # Euclid/FS learning rate\n",
    "LAM = 1e-3                 # FS damping\n",
    "\n",
    "# Shrinkage: α=1 => pure FS, α<1 => blend toward Euclidean identity metric\n",
    "ALPHA_SHRINK = 1.0         # try 0.8 or 0.5 for \"Euclid+FS\" blend\n",
    "\n",
    "# Adam hyperparams (match your prior analysis spirit)\n",
    "ADAM_LR = 1e-3\n",
    "ADAM_BETAS = (0.9, 0.999)\n",
    "ADAM_EPS = 1e-8\n",
    "ADAM_WEIGHT_DECAY = 0.0\n",
    "\n",
    "PREFERRED_DEVICE = \"lightning.gpu\"\n",
    "FALLBACK_DEVICE = \"default.qubit\"\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Reproducibility\n",
    "# ---------------------------\n",
    "def set_all_seeds(seed: int):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    try:\n",
    "        qml.numpy.random.seed(seed)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Shared Preprocessor (Block 1) - CIFAR-10 version\n",
    "# ---------------------------\n",
    "class SharedPreprocessor(nn.Module):\n",
    "    \"\"\"Shared classical feature extractor: 3072 → 16\"\"\"\n",
    "    def __init__(self, input_dim=3072, bottleneck_dim=16):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, bottleneck_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten to (batch, 3072)\n",
    "        return torch.tanh(self.fc(x))\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Device factory (+1 aux wire for full metric)\n",
    "# ---------------------------\n",
    "def make_device(n_qubits: int):\n",
    "    n_wires_total = n_qubits + 1\n",
    "    aux_wire = n_qubits\n",
    "    try:\n",
    "        dev = qml.device(PREFERRED_DEVICE, wires=n_wires_total)\n",
    "        backend = PREFERRED_DEVICE\n",
    "    except Exception as e:\n",
    "        dev = qml.device(FALLBACK_DEVICE, wires=n_wires_total)\n",
    "        backend = FALLBACK_DEVICE\n",
    "        print(f\"[device] Could not load {PREFERRED_DEVICE} ({type(e).__name__}: {e}). Using {FALLBACK_DEVICE}.\")\n",
    "    print(f\"[device] backend={backend}, wires={n_wires_total} (aux_wire={aux_wire})\")\n",
    "    return dev, backend, aux_wire\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Helpers\n",
    "# ---------------------------\n",
    "def _cosine(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    na = np.linalg.norm(a) + 1e-12\n",
    "    nb = np.linalg.norm(b) + 1e-12\n",
    "    return float(np.dot(a, b) / (na * nb))\n",
    "\n",
    "\n",
    "def _flat(theta) -> pnp.ndarray:\n",
    "    return pnp.reshape(theta, (-1,))\n",
    "\n",
    "\n",
    "def _unflat(v, shape) -> pnp.ndarray:\n",
    "    return pnp.reshape(v, shape)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Build circuit + metric fns\n",
    "# ---------------------------\n",
    "dev, backend, aux_wire = make_device(N_QUBITS)\n",
    "\n",
    "@qml.qnode(dev, interface=\"autograd\", diff_method=\"parameter-shift\")\n",
    "def circuit(inputs, weights):\n",
    "    # circuit uses ONLY wires 0..N_QUBITS-1; last wire is aux-reserved\n",
    "    for layer in range(N_LAYERS):\n",
    "        for i in range(N_QUBITS):\n",
    "            qml.RY(inputs[i], wires=i)\n",
    "        for i in range(N_QUBITS):\n",
    "            qml.RY(weights[layer, i, 0], wires=i)\n",
    "            qml.RZ(weights[layer, i, 1], wires=i)\n",
    "\n",
    "        for i in range(N_QUBITS - 1):\n",
    "            qml.CNOT(wires=[i, i + 1])\n",
    "        if N_QUBITS > 2:\n",
    "            qml.CNOT(wires=[N_QUBITS - 1, 0])\n",
    "\n",
    "    return (\n",
    "        qml.expval(qml.PauliZ(0)),\n",
    "        qml.expval(qml.PauliZ(1)),\n",
    "        qml.expval(qml.PauliZ(2)),\n",
    "        qml.expval(qml.PauliZ(3)),\n",
    "        qml.expval(qml.PauliZ(0) @ qml.PauliZ(1)),\n",
    "        qml.expval(qml.PauliZ(2) @ qml.PauliZ(3)),\n",
    "    )\n",
    "\n",
    "metric_full  = qml.metric_tensor(circuit, approx=None,       aux_wire=aux_wire)\n",
    "metric_diag  = qml.metric_tensor(circuit, approx=\"diag\")\n",
    "metric_block = qml.metric_tensor(circuit, approx=\"block-diag\")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Load frozen preprocessor from Block 1\n",
    "# ---------------------------\n",
    "realnet = torch.load(\"realnet_cifar10_results.pt\", weights_only=False)\n",
    "pre_state = realnet[\"preprocessor_state\"]\n",
    "\n",
    "pre = SharedPreprocessor(3072, 16)  # CIFAR-10: 3072 input dims\n",
    "pre.load_state_dict(pre_state)\n",
    "for p in pre.parameters():\n",
    "    p.requires_grad = False\n",
    "pre.eval()\n",
    "\n",
    "# frozen feature_select (16 -> 4)\n",
    "feature_select = nn.Linear(16, N_QUBITS)\n",
    "for p in feature_select.parameters():\n",
    "    p.requires_grad = False\n",
    "feature_select.eval()\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Dataset: CIFAR-10\n",
    "# ---------------------------\n",
    "# CIFAR-10 normalization (channel-wise)\n",
    "norm_mean = [0.4914, 0.4822, 0.4465]\n",
    "norm_std = [0.2470, 0.2435, 0.2616]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std),\n",
    "])\n",
    "\n",
    "train_ds = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Fixed readout (constant) to isolate θ update behavior\n",
    "# ---------------------------\n",
    "C = 10\n",
    "W = pnp.array(np.random.randn(C, 6).astype(np.float64) * 0.1)  # (10,6)\n",
    "b = pnp.array(np.zeros((C,), dtype=np.float64))\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Loss factory (depends on current minibatch)\n",
    "# ---------------------------\n",
    "def make_batch_ce_loss(x_np, y_np):\n",
    "    def batch_ce_loss(theta):\n",
    "        losses = []\n",
    "        for i in range(len(y_np)):\n",
    "            q_raw = circuit(x_np[i], theta)\n",
    "            q = pnp.stack(q_raw)  # (6,)\n",
    "            logits = W @ q + b\n",
    "            logits = logits - pnp.max(logits)\n",
    "            ex = pnp.exp(logits)\n",
    "            probs = ex / pnp.sum(ex)\n",
    "            losses.append(-pnp.log(probs[int(y_np[i])] + 1e-12))\n",
    "        return pnp.mean(pnp.stack(losses))\n",
    "    return batch_ce_loss\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# FS-NG / Euclid delta\n",
    "# ---------------------------\n",
    "def fsng_or_euclid_delta(theta, approx: str, batch_ce_loss, x_np):\n",
    "    \"\"\"\n",
    "    approx in {\"euclid\",\"diag\",\"block-diag\",\"full\"}.\n",
    "\n",
    "    - euclid: delta = g\n",
    "    - others: delta = (G_eff + λI)^(-1) g\n",
    "              with optional shrinkage: G_eff = α G_bar + (1-α) I\n",
    "    \"\"\"\n",
    "    theta_shape = theta.shape\n",
    "    P = int(np.prod(theta_shape))\n",
    "    I = pnp.eye(P, dtype=pnp.float64)\n",
    "\n",
    "    g = qml.grad(batch_ce_loss)(theta)\n",
    "    g_flat = pnp.reshape(g, (P,))\n",
    "\n",
    "    if approx == \"euclid\":\n",
    "        return np.array(g_flat, dtype=np.float64)\n",
    "\n",
    "    if approx == \"diag\":\n",
    "        metric_fn = metric_diag\n",
    "    elif approx == \"block-diag\":\n",
    "        metric_fn = metric_block\n",
    "    elif approx == \"full\":\n",
    "        metric_fn = metric_full\n",
    "    else:\n",
    "        raise ValueError(\"approx must be one of: 'euclid','diag','block-diag','full'\")\n",
    "\n",
    "    G_sum = pnp.zeros((P, P), dtype=pnp.float64)\n",
    "    for i in range(x_np.shape[0]):\n",
    "        Gi = metric_fn(x_np[i], theta)\n",
    "        Gi = pnp.reshape(Gi, (P, P))\n",
    "        G_sum = G_sum + Gi\n",
    "    G_bar = G_sum / float(x_np.shape[0])\n",
    "\n",
    "    alpha = float(ALPHA_SHRINK)\n",
    "    G_eff = alpha * G_bar + (1.0 - alpha) * I\n",
    "    G_reg = G_eff + LAM * I\n",
    "\n",
    "    delta = pnp.linalg.solve(G_reg, g_flat)\n",
    "    return np.array(delta, dtype=np.float64)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Adam (PyTorch-style) on θ only\n",
    "# ---------------------------\n",
    "class AdamState:\n",
    "    def __init__(self, P, betas=(0.9, 0.999)):\n",
    "        self.t = 0\n",
    "        self.m = np.zeros((P,), dtype=np.float64)\n",
    "        self.v = np.zeros((P,), dtype=np.float64)\n",
    "        self.beta1, self.beta2 = betas\n",
    "\n",
    "def adam_step(theta, grad_flat, state: AdamState, lr=1e-3, eps=1e-8, weight_decay=0.0):\n",
    "    \"\"\"\n",
    "    Deterministic Adam update on a *flat* theta vector.\n",
    "    Returns updated flat theta and the applied update direction (delta).\n",
    "    \"\"\"\n",
    "    state.t += 1\n",
    "\n",
    "    g = grad_flat.astype(np.float64)\n",
    "    if weight_decay != 0.0:\n",
    "        g = g + weight_decay * theta\n",
    "\n",
    "    b1, b2 = state.beta1, state.beta2\n",
    "    state.m = b1 * state.m + (1.0 - b1) * g\n",
    "    state.v = b2 * state.v + (1.0 - b2) * (g * g)\n",
    "\n",
    "    mhat = state.m / (1.0 - b1 ** state.t)\n",
    "    vhat = state.v / (1.0 - b2 ** state.t)\n",
    "\n",
    "    step = lr * mhat / (np.sqrt(vhat) + eps)   # this is the update magnitude\n",
    "    theta_new = theta - step\n",
    "    return theta_new, step\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# One minibatch run for one method\n",
    "# ---------------------------\n",
    "def run_one_minibatch(theta0, method, x_np, y_np):\n",
    "    \"\"\"\n",
    "    Runs K_STEPS on a single minibatch and returns:\n",
    "      losses: (K_STEPS,)\n",
    "      delta1: first-step update direction (flat, numpy)\n",
    "      elapsed\n",
    "    \"\"\"\n",
    "    theta = pnp.array(theta0, requires_grad=True)\n",
    "    theta_shape = theta.shape\n",
    "    P = int(np.prod(theta_shape))\n",
    "\n",
    "    batch_ce_loss = make_batch_ce_loss(x_np, y_np)\n",
    "\n",
    "    # Adam needs state per minibatch (matches your \"diagnostic\" intent)\n",
    "    adam_state = AdamState(P, betas=ADAM_BETAS) if method == \"adam\" else None\n",
    "\n",
    "    losses = []\n",
    "    delta1 = None\n",
    "\n",
    "    t0 = time.time()\n",
    "    for k in range(K_STEPS):\n",
    "        loss_before = float(batch_ce_loss(theta))\n",
    "\n",
    "        if method == \"adam\":\n",
    "            g = qml.grad(batch_ce_loss)(theta)\n",
    "            g_flat = np.array(pnp.reshape(g, (P,)), dtype=np.float64)\n",
    "\n",
    "            theta_flat = np.array(pnp.reshape(theta, (P,)), dtype=np.float64)\n",
    "            theta_flat_new, step_vec = adam_step(\n",
    "                theta_flat, g_flat, adam_state,\n",
    "                lr=ADAM_LR, eps=ADAM_EPS, weight_decay=ADAM_WEIGHT_DECAY\n",
    "            )\n",
    "            # For cosine comparisons, treat Adam's *applied update* as delta\n",
    "            delta = step_vec / (ADAM_LR + 1e-12)  # normalized-ish direction (optional)\n",
    "            theta = _unflat(theta_flat_new, theta_shape)\n",
    "\n",
    "        else:\n",
    "            delta = fsng_or_euclid_delta(theta, method, batch_ce_loss, x_np)\n",
    "            theta_flat = np.array(pnp.reshape(theta, (P,)), dtype=np.float64)\n",
    "            theta = _unflat(theta_flat - ETA * delta, theta_shape)\n",
    "\n",
    "        loss_after = float(batch_ce_loss(theta))\n",
    "        losses.append(loss_after)\n",
    "\n",
    "        if k == 0:\n",
    "            # Store first-step applied direction in comparable units:\n",
    "            # - for FS/euclid: delta is the preconditioned direction (so update is ETA*delta)\n",
    "            # - for Adam: delta above is roughly direction; we'll store the *applied step* too\n",
    "            if method == \"adam\":\n",
    "                delta1 = np.array(step_vec, dtype=np.float64)  # actual applied update\n",
    "            else:\n",
    "                delta1 = np.array(ETA * delta, dtype=np.float64)  # applied update\n",
    "\n",
    "    return np.array(losses), delta1, time.time() - t0\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Collect minibatches (deterministic but seed-dependent)\n",
    "# ---------------------------\n",
    "def get_minibatches(seed, n_minibatches, batch_size):\n",
    "    \"\"\"\n",
    "    Returns list of (x_np, y_np) minibatches.\n",
    "    Uses a deterministic RNG over indices so we don't depend on DataLoader nondeterminism.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    batches = []\n",
    "    for _ in range(n_minibatches):\n",
    "        idx = rng.choice(len(train_ds), size=batch_size, replace=False)\n",
    "        x0, y0 = zip(*[train_ds[i] for i in idx])\n",
    "        x0 = torch.stack(list(x0), dim=0)\n",
    "        y0 = torch.tensor(list(y0), dtype=torch.long)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            feats16 = pre(x0)\n",
    "            xq = torch.tanh(feature_select(feats16))\n",
    "\n",
    "        batches.append((xq.cpu().numpy(), y0.cpu().numpy()))\n",
    "    return batches\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Main experiment loop\n",
    "# ---------------------------\n",
    "METHODS = [\"euclid\", \"adam\", \"diag\", \"block-diag\", \"full\"]\n",
    "\n",
    "def summarize(arr):\n",
    "    return float(np.mean(arr)), float(np.std(arr))\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(f\"\\n[Euclid vs Adam vs FS-NG] DATASET={DATASET} | backend={backend}\")\n",
    "    print(f\"  layers={N_LAYERS} | batch={BATCH_SIZE} | minibatches={N_MINIBATCHES} | steps={K_STEPS}\")\n",
    "    print(f\"  ETA={ETA} | LAM={LAM} | alpha_shrink={ALPHA_SHRINK} | Adam(lr={ADAM_LR}, betas={ADAM_BETAS})\\n\")\n",
    "\n",
    "    # Store per-method results across (seed, minibatch)\n",
    "    final_losses = {m: [] for m in METHODS}\n",
    "    times = {m: [] for m in METHODS}\n",
    "    cos_vs_full = {m: [] for m in METHODS if m != \"full\"}  # cosine of step1 update vs full step1 update\n",
    "\n",
    "    for seed in SEEDS:\n",
    "        set_all_seeds(seed)\n",
    "        print(f\"\\n{'='*70}\\nSEED {seed}\\n{'='*70}\")\n",
    "\n",
    "        # same initial theta per seed across methods\n",
    "        theta0 = pnp.array(np.random.randn(N_LAYERS, N_QUBITS, 2) * 0.1, requires_grad=True)\n",
    "\n",
    "        minibatches = get_minibatches(seed, N_MINIBATCHES, BATCH_SIZE)\n",
    "\n",
    "        for mb_i, (x_np, y_np) in enumerate(minibatches, start=1):\n",
    "            # run full first for reference direction cosine\n",
    "            full_losses, full_delta1, full_t = run_one_minibatch(theta0, \"full\", x_np, y_np)\n",
    "            final_losses[\"full\"].append(full_losses[-1])\n",
    "            times[\"full\"].append(full_t)\n",
    "\n",
    "            # run others\n",
    "            for m in [\"euclid\", \"adam\", \"diag\", \"block-diag\"]:\n",
    "                losses, delta1, tsec = run_one_minibatch(theta0, m, x_np, y_np)\n",
    "                final_losses[m].append(losses[-1])\n",
    "                times[m].append(tsec)\n",
    "\n",
    "                # cosine on step-1 APPLIED update (so compare apples-to-apples)\n",
    "                cos_vs_full[m].append(_cosine(delta1, full_delta1))\n",
    "\n",
    "            if mb_i in (1, N_MINIBATCHES):\n",
    "                print(f\"  minibatch {mb_i:2d}/{N_MINIBATCHES}: \"\n",
    "                      f\"full_lossK={full_losses[-1]:.4f} | \"\n",
    "                      f\"diag_lossK={final_losses['diag'][-1]:.4f} | \"\n",
    "                      f\"adam_lossK={final_losses['adam'][-1]:.4f}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"AGGREGATE RESULTS (across 3 seeds × 10 minibatches = 30 runs per method)\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Report: mean±std final loss, mean±std time, mean cosine vs full\n",
    "    for m in METHODS:\n",
    "        muL, sdL = summarize(final_losses[m])\n",
    "        muT, sdT = summarize(times[m])\n",
    "        if m == \"full\":\n",
    "            print(f\"  {m:9s}: final_loss={muL:.4f} ± {sdL:.4f} | time={muT:.2f}s ± {sdT:.2f}s | cos(step1 vs full)=1.000\")\n",
    "        else:\n",
    "            muC, sdC = summarize(cos_vs_full[m])\n",
    "            print(f\"  {m:9s}: final_loss={muL:.4f} ± {sdL:.4f} | time={muT:.2f}s ± {sdT:.2f}s | cos(step1 vs full)={muC:.3f} ± {sdC:.3f}\")\n",
    "\n",
    "    print(\"\\nNotes:\")\n",
    "    print(\"  • Cosines compare the *applied step vector* at step 1 (not the raw gradient).\")\n",
    "    print(\"  • Adam's delta is its actual applied update on θ (per minibatch, fresh state).\")\n",
    "    print(\"  • If you want Adam to carry momentum across minibatches, move AdamState outside run_one_minibatch().\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8700b5c-8fe0-41f5-ac66-2d9fdcb13442",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
