{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70a8b0a6-5f1a-4fdb-bd35-45eb4fbb5483",
   "metadata": {},
   "source": [
    "# Load QM9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d3238eb-9202-429a-9ea3-335688e3b7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading QM9 via PyTorch Geometric...\n",
      "(This handles authentication and mirrors automatically)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/molnet_publish/qm9.zip\n",
      "Extracting data/qm9/raw/qm9.zip\n",
      "Downloading https://ndownloader.figshare.com/files/3195404\n",
      "Processing...\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 133885/133885 [01:24<00:00, 1593.34it/s]\n",
      "Done!\n",
      "/tmp/ipykernel_92657/148869598.py:26: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  print(f\"✓ Properties available: {dataset.data.y.shape[1]} targets per molecule\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Downloaded 130831 molecules\n",
      "✓ Properties available: 19 targets per molecule\n",
      "   (Index 4 = HOMO-LUMO gap)\n",
      "\n",
      "Extracting HOMO-LUMO gap values...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 130831/130831 [00:07<00:00, 16862.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Extracted 130831 valid gap values\n",
      "\n",
      "Class distribution (HOMO-LUMO gap):\n",
      "  Class 0 (Low,  ≤6.131 eV): 43177 molecules\n",
      "  Class 1 (Med,  6.131-7.494 eV): 44534 molecules\n",
      "  Class 2 (High, >7.494 eV): 43120 molecules\n",
      "\n",
      "Generating 512-D Morgan fingerprints...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 130831/130831 [00:15<00:00, 8618.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Generated features: (129227, 512)\n",
      "\n",
      "✓ Saved preprocessed data to qm9_preprocessed_512d.pt\n",
      "  Total samples: 129227\n",
      "  Feature dim: 512\n",
      "  Classes: 3 (balanced)\n",
      "\n",
      "Final class counts:\n",
      "  Class 0: 42728 (33.1%)\n",
      "  Class 1: 44187 (34.2%)\n",
      "  Class 2: 42312 (32.7%)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "QM9 Download via PyTorch Geometric + Custom Feature Extraction\n",
    "Compatible with PyG 2.x API\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.datasets import QM9\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def download_qm9_via_pyg(root='./data'):\n",
    "    \"\"\"\n",
    "    Download QM9 using PyTorch Geometric (handles mirrors/auth)\n",
    "    Returns: PyG dataset object\n",
    "    \"\"\"\n",
    "    print(\"Downloading QM9 via PyTorch Geometric...\")\n",
    "    print(\"(This handles authentication and mirrors automatically)\")\n",
    "    \n",
    "    # No target parameter in newer PyG - it returns all properties\n",
    "    dataset = QM9(root=root)\n",
    "    \n",
    "    print(f\"✓ Downloaded {len(dataset)} molecules\")\n",
    "    print(f\"✓ Properties available: {dataset.data.y.shape[1]} targets per molecule\")\n",
    "    print(f\"   (Index 4 = HOMO-LUMO gap)\")\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def pyg_to_rdkit_mol(data):\n",
    "    \"\"\"\n",
    "    Convert PyG Data object back to RDKit molecule\n",
    "    (For consistent feature extraction)\n",
    "    \"\"\"\n",
    "    # PyG stores atom types as atomic numbers (Z)\n",
    "    atom_types = data.z.cpu().numpy()\n",
    "    \n",
    "    # Create RDKit molecule\n",
    "    mol = Chem.RWMol()\n",
    "    for z in atom_types:\n",
    "        mol.AddAtom(Chem.Atom(int(z)))\n",
    "    \n",
    "    # Add bonds from edge_index\n",
    "    edge_index = data.edge_index.cpu().numpy()\n",
    "    bonds_added = set()\n",
    "    \n",
    "    for i in range(edge_index.shape[1]):\n",
    "        src, dst = int(edge_index[0, i]), int(edge_index[1, i])\n",
    "        \n",
    "        # Add each bond only once (edges are bidirectional in PyG)\n",
    "        bond_id = tuple(sorted([src, dst]))\n",
    "        if bond_id not in bonds_added:\n",
    "            mol.AddBond(src, dst, Chem.BondType.SINGLE)\n",
    "            bonds_added.add(bond_id)\n",
    "    \n",
    "    # Add 3D coordinates if available\n",
    "    if hasattr(data, 'pos'):\n",
    "        pos = data.pos.cpu().numpy()\n",
    "        conf = Chem.Conformer(len(atom_types))\n",
    "        for i, xyz in enumerate(pos):\n",
    "            conf.SetAtomPosition(i, tuple(xyz.tolist()))\n",
    "        mol.AddConformer(conf)\n",
    "    \n",
    "    mol = mol.GetMol()\n",
    "    \n",
    "    # Sanitize\n",
    "    try:\n",
    "        Chem.SanitizeMol(mol)\n",
    "        return mol\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def mol_to_morgan_fp(mol, fp_size=512, radius=2):\n",
    "    \"\"\"\n",
    "    Generate Morgan fingerprint (ECFP-like)\n",
    "    \"\"\"\n",
    "    if mol is None:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=fp_size)\n",
    "        arr = np.zeros(fp_size, dtype=np.float32)\n",
    "        Chem.DataStructs.ConvertToNumpyArray(fp, arr)\n",
    "        return arr\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def preprocess_qm9_custom(fp_size=512, output_file='qm9_preprocessed_512d.pt'):\n",
    "    \"\"\"\n",
    "    Complete QM9 preprocessing:\n",
    "    1. Download via PyG (reliable)\n",
    "    2. Extract features manually (Morgan FP)\n",
    "    3. Create balanced 3-class labels from HOMO-LUMO gap\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Download via PyG\n",
    "    dataset = download_qm9_via_pyg(root='./data/qm9')\n",
    "    \n",
    "    # Step 2: Extract HOMO-LUMO gap values\n",
    "    # In QM9, properties are stored as:\n",
    "    # Index 0: dipole moment\n",
    "    # Index 1: isotropic polarizability\n",
    "    # Index 2: HOMO energy\n",
    "    # Index 3: LUMO energy  \n",
    "    # Index 4: HOMO-LUMO gap ← THIS ONE\n",
    "    # Index 5: electronic spatial extent\n",
    "    # ... etc\n",
    "    \n",
    "    print(\"\\nExtracting HOMO-LUMO gap values...\")\n",
    "    gaps = []\n",
    "    valid_indices = []\n",
    "    \n",
    "    for i, data in enumerate(tqdm(dataset)):\n",
    "        try:\n",
    "            # Extract gap (index 4 in y tensor)\n",
    "            gap = float(data.y[0, 4])  # Shape is [1, num_properties]\n",
    "            gaps.append(gap)\n",
    "            valid_indices.append(i)\n",
    "        except:\n",
    "            # Skip molecules with missing properties\n",
    "            continue\n",
    "    \n",
    "    gaps = np.array(gaps)\n",
    "    print(f\"✓ Extracted {len(gaps)} valid gap values\")\n",
    "    \n",
    "    # Step 3: Create balanced class labels\n",
    "    q33, q67 = np.percentile(gaps, [33, 67])\n",
    "    labels = np.zeros(len(gaps), dtype=np.int64)\n",
    "    labels[gaps > q33] = 1\n",
    "    labels[gaps > q67] = 2\n",
    "    \n",
    "    print(f\"\\nClass distribution (HOMO-LUMO gap):\")\n",
    "    print(f\"  Class 0 (Low,  ≤{q33:.3f} eV): {np.sum(labels==0)} molecules\")\n",
    "    print(f\"  Class 1 (Med,  {q33:.3f}-{q67:.3f} eV): {np.sum(labels==1)} molecules\")\n",
    "    print(f\"  Class 2 (High, >{q67:.3f} eV): {np.sum(labels==2)} molecules\")\n",
    "    \n",
    "    # Step 4: Generate Morgan fingerprints\n",
    "    print(f\"\\nGenerating {fp_size}-D Morgan fingerprints...\")\n",
    "    features = []\n",
    "    final_labels = []\n",
    "    final_gaps = []\n",
    "    \n",
    "    for idx, i in enumerate(tqdm(valid_indices)):\n",
    "        data = dataset[i]\n",
    "        \n",
    "        # Convert PyG Data to RDKit molecule\n",
    "        mol = pyg_to_rdkit_mol(data)\n",
    "        \n",
    "        if mol is None:\n",
    "            continue\n",
    "        \n",
    "        # Generate fingerprint\n",
    "        fp = mol_to_morgan_fp(mol, fp_size=fp_size)\n",
    "        \n",
    "        if fp is None:\n",
    "            continue\n",
    "        \n",
    "        features.append(fp)\n",
    "        final_labels.append(labels[idx])\n",
    "        final_gaps.append(gaps[idx])\n",
    "    \n",
    "    features = np.stack(features)\n",
    "    final_labels = np.array(final_labels)\n",
    "    final_gaps = np.array(final_gaps)\n",
    "    \n",
    "    print(f\"\\n✓ Generated features: {features.shape}\")\n",
    "    \n",
    "    # Step 5: Save preprocessed data\n",
    "    torch.save({\n",
    "        'features': torch.from_numpy(features),\n",
    "        'labels': torch.from_numpy(final_labels),\n",
    "        'gap_values': final_gaps,\n",
    "        'class_thresholds': (q33, q67),\n",
    "        'feature_type': f'Morgan_fp{fp_size}_radius2',\n",
    "        'n_samples': len(features),\n",
    "        'n_classes': 3\n",
    "    }, output_file)\n",
    "    \n",
    "    print(f\"\\n✓ Saved preprocessed data to {output_file}\")\n",
    "    print(f\"  Total samples: {len(features)}\")\n",
    "    print(f\"  Feature dim: {fp_size}\")\n",
    "    print(f\"  Classes: 3 (balanced)\")\n",
    "    \n",
    "    # Verify class balance\n",
    "    print(f\"\\nFinal class counts:\")\n",
    "    for c in range(3):\n",
    "        count = np.sum(final_labels == c)\n",
    "        pct = 100 * count / len(final_labels)\n",
    "        print(f\"  Class {c}: {count} ({pct:.1f}%)\")\n",
    "    \n",
    "    return features, final_labels\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess_qm9_custom(fp_size=512, output_file='qm9_preprocessed_512d.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88133838-a8c0-4d6a-8207-fa2d21e93254",
   "metadata": {},
   "source": [
    "# RealNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7523e9f8-9ce5-4e89-8ed0-3bd165852716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "======================================================================\n",
      "BLOCK 1: RealNet Training on QM9\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "  • Dataset: QM9 HOMO-LUMO gap classification\n",
      "  • Features: 512-D Morgan fingerprints (frozen)\n",
      "  • Classes: 3 (Low/Med/High gap)\n",
      "  • Architecture: 512 → 64 → 3\n",
      "  • Batch size: 128 (train), 256 (test)\n",
      "  • Max epochs: 200\n",
      "  • Patience: 10\n",
      "  • Seeds: [42, 123, 456]\n",
      "======================================================================\n",
      "\n",
      "Loading preprocessed QM9 data...\n",
      "  Loaded 129227 molecules\n",
      "  Feature dim: 512\n",
      "  Classes: 3\n",
      "\n",
      "Creating stratified samples...\n",
      "  Train samples: 4500 (1500 per class)\n",
      "  Test samples: 875 (300 per class)\n",
      "\n",
      "======================================================================\n",
      "SEED 42\n",
      "======================================================================\n",
      "\n",
      "  Training RealNet (seed=42)...\n",
      "  [RealNet] Epoch  1 | loss=1.0426 | test_acc=0.6491 | time=0.1s\n",
      "  [RealNet] Epoch  2 | loss=0.8262 | test_acc=0.6754 | time=0.1s\n",
      "  [RealNet] Epoch  3 | loss=0.6712 | test_acc=0.7120 | time=0.2s\n",
      "  [RealNet] Epoch  4 | loss=0.5893 | test_acc=0.7177 | time=0.2s\n",
      "  [RealNet] Epoch  5 | loss=0.5453 | test_acc=0.7154 | time=0.3s\n",
      "  [RealNet] Epoch  6 | loss=0.5134 | test_acc=0.7211 | time=0.3s\n",
      "  [RealNet] Epoch  7 | loss=0.4938 | test_acc=0.7257 | time=0.4s\n",
      "  [RealNet] Epoch  8 | loss=0.4732 | test_acc=0.7269 | time=0.4s\n",
      "  [RealNet] Epoch  9 | loss=0.4571 | test_acc=0.7200 | time=0.5s\n",
      "  [RealNet] Epoch 10 | loss=0.4439 | test_acc=0.7143 | time=0.5s\n",
      "  [RealNet] Epoch 11 | loss=0.4310 | test_acc=0.7200 | time=0.6s\n",
      "  [RealNet] Epoch 12 | loss=0.4176 | test_acc=0.7234 | time=0.6s\n",
      "  [RealNet] Epoch 13 | loss=0.4070 | test_acc=0.7211 | time=0.7s\n",
      "  [RealNet] Epoch 14 | loss=0.3957 | test_acc=0.7189 | time=0.7s\n",
      "  [RealNet] Epoch 15 | loss=0.3841 | test_acc=0.7200 | time=0.8s\n",
      "  [RealNet] Epoch 16 | loss=0.3714 | test_acc=0.7234 | time=0.8s\n",
      "  [RealNet] Epoch 17 | loss=0.3611 | test_acc=0.7246 | time=0.9s\n",
      "  [RealNet] Epoch 18 | loss=0.3507 | test_acc=0.7143 | time=0.9s\n",
      "  [RealNet] Early stop at epoch 18 (no improvement for 10 epochs)\n",
      "\n",
      "======================================================================\n",
      "SEED 123\n",
      "======================================================================\n",
      "\n",
      "  Training RealNet (seed=123)...\n",
      "  [RealNet] Epoch  1 | loss=1.0346 | test_acc=0.6640 | time=0.1s\n",
      "  [RealNet] Epoch  2 | loss=0.8175 | test_acc=0.6903 | time=0.1s\n",
      "  [RealNet] Epoch  3 | loss=0.6652 | test_acc=0.6971 | time=0.1s\n",
      "  [RealNet] Epoch  4 | loss=0.5876 | test_acc=0.7143 | time=0.2s\n",
      "  [RealNet] Epoch  5 | loss=0.5481 | test_acc=0.7177 | time=0.2s\n",
      "  [RealNet] Epoch  6 | loss=0.5182 | test_acc=0.7109 | time=0.3s\n",
      "  [RealNet] Epoch  7 | loss=0.4966 | test_acc=0.7211 | time=0.4s\n",
      "  [RealNet] Epoch  8 | loss=0.4816 | test_acc=0.7166 | time=0.4s\n",
      "  [RealNet] Epoch  9 | loss=0.4679 | test_acc=0.7269 | time=0.4s\n",
      "  [RealNet] Epoch 10 | loss=0.4566 | test_acc=0.7211 | time=0.5s\n",
      "  [RealNet] Epoch 11 | loss=0.4439 | test_acc=0.7269 | time=0.5s\n",
      "  [RealNet] Epoch 12 | loss=0.4361 | test_acc=0.7166 | time=0.6s\n",
      "  [RealNet] Epoch 13 | loss=0.4268 | test_acc=0.7029 | time=0.6s\n",
      "  [RealNet] Epoch 14 | loss=0.4172 | test_acc=0.7154 | time=0.7s\n",
      "  [RealNet] Epoch 15 | loss=0.4112 | test_acc=0.7154 | time=0.7s\n",
      "  [RealNet] Epoch 16 | loss=0.3985 | test_acc=0.7131 | time=0.8s\n",
      "  [RealNet] Epoch 17 | loss=0.3914 | test_acc=0.7246 | time=0.8s\n",
      "  [RealNet] Epoch 18 | loss=0.3820 | test_acc=0.7131 | time=0.9s\n",
      "  [RealNet] Epoch 19 | loss=0.3713 | test_acc=0.7177 | time=0.9s\n",
      "  [RealNet] Early stop at epoch 19 (no improvement for 10 epochs)\n",
      "\n",
      "======================================================================\n",
      "SEED 456\n",
      "======================================================================\n",
      "\n",
      "  Training RealNet (seed=456)...\n",
      "  [RealNet] Epoch  1 | loss=1.0282 | test_acc=0.6377 | time=0.1s\n",
      "  [RealNet] Epoch  2 | loss=0.8097 | test_acc=0.6754 | time=0.1s\n",
      "  [RealNet] Epoch  3 | loss=0.6619 | test_acc=0.6994 | time=0.2s\n",
      "  [RealNet] Epoch  4 | loss=0.5844 | test_acc=0.7029 | time=0.2s\n",
      "  [RealNet] Epoch  5 | loss=0.5428 | test_acc=0.7109 | time=0.3s\n",
      "  [RealNet] Epoch  6 | loss=0.5114 | test_acc=0.7234 | time=0.3s\n",
      "  [RealNet] Epoch  7 | loss=0.4907 | test_acc=0.7234 | time=0.4s\n",
      "  [RealNet] Epoch  8 | loss=0.4727 | test_acc=0.7211 | time=0.4s\n",
      "  [RealNet] Epoch  9 | loss=0.4606 | test_acc=0.7223 | time=0.5s\n",
      "  [RealNet] Epoch 10 | loss=0.4476 | test_acc=0.7246 | time=0.5s\n",
      "  [RealNet] Epoch 11 | loss=0.4369 | test_acc=0.7303 | time=0.6s\n",
      "  [RealNet] Epoch 12 | loss=0.4273 | test_acc=0.7200 | time=0.6s\n",
      "  [RealNet] Epoch 13 | loss=0.4151 | test_acc=0.7109 | time=0.7s\n",
      "  [RealNet] Epoch 14 | loss=0.4072 | test_acc=0.7211 | time=0.7s\n",
      "  [RealNet] Epoch 15 | loss=0.3972 | test_acc=0.7154 | time=0.8s\n",
      "  [RealNet] Epoch 16 | loss=0.3876 | test_acc=0.7166 | time=0.8s\n",
      "  [RealNet] Epoch 17 | loss=0.3759 | test_acc=0.7200 | time=0.9s\n",
      "  [RealNet] Epoch 18 | loss=0.3683 | test_acc=0.7097 | time=0.9s\n",
      "  [RealNet] Epoch 19 | loss=0.3573 | test_acc=0.7166 | time=1.0s\n",
      "  [RealNet] Epoch 20 | loss=0.3489 | test_acc=0.7211 | time=1.0s\n",
      "  [RealNet] Epoch 21 | loss=0.3383 | test_acc=0.7234 | time=1.1s\n",
      "  [RealNet] Early stop at epoch 21 (no improvement for 10 epochs)\n",
      "\n",
      "======================================================================\n",
      "REALNET QM9 SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Accuracy:     0.7280 ± 0.0016\n",
      "Time:         1.0s ± 0.1s\n",
      "Epochs:       19.3 ± 1.2\n",
      "Parameters:   33,027\n",
      "\n",
      "Per-seed results:\n",
      "  Seed 42: acc=0.7269, time=0.9s, epochs=18\n",
      "  Seed 123: acc=0.7269, time=0.9s, epochs=19\n",
      "  Seed 456: acc=0.7303, time=1.1s, epochs=21\n",
      "\n",
      "✓ Saved results to: realnet_qm9_results.pt\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Block 1: RealNet Training on QM9\n",
    "=================================\n",
    "Trains real-valued MLP baseline on QM9 HOMO-LUMO gap classification.\n",
    "Creates frozen feature extractor (identity - features already frozen).\n",
    "\n",
    "Requirements:\n",
    "- qm9_preprocessed_512d.pt (from Block 0)\n",
    "\n",
    "Outputs:\n",
    "- realnet_qm9_results.pt: Contains RealNet results\n",
    "\"\"\"\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ============================================\n",
    "# Seed setting\n",
    "# ============================================\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    \"\"\"Set seeds for all RNG sources for reproducibility\"\"\"\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# ============================================\n",
    "# RealNet Model\n",
    "# ============================================\n",
    "\n",
    "class RealNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Real-valued MLP classifier for QM9\n",
    "    512-D features → 64 hidden → 3 classes\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=512, hidden_dim=64, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "# ============================================\n",
    "# Training and Evaluation\n",
    "# ============================================\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "    \n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += x.size(0)\n",
    "    \n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "def train_with_early_stopping(model, train_loader, test_loader, optimizer,\n",
    "                              device, max_epochs=200, patience=10, name=\"Model\"):\n",
    "    best_acc = 0.0\n",
    "    epochs_without_improvement = 0\n",
    "    start = time.time()\n",
    "    last_acc = 0.0\n",
    "    \n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        loss = train_one_epoch(model, train_loader, optimizer, device)\n",
    "        acc = evaluate(model, test_loader, device)\n",
    "        last_acc = acc\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        print(f\"  [{name}] Epoch {epoch:2d} | loss={loss:.4f} | \"\n",
    "              f\"test_acc={acc:.4f} | time={elapsed:.1f}s\")\n",
    "        \n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        \n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"  [{name}] Early stop at epoch {epoch} \"\n",
    "                  f\"(no improvement for {patience} epochs)\")\n",
    "            break\n",
    "    \n",
    "    total_time = time.time() - start\n",
    "    return {\n",
    "        \"best_acc\": best_acc,\n",
    "        \"final_acc\": last_acc,\n",
    "        \"time\": total_time,\n",
    "        \"epochs\": epoch,\n",
    "    }\n",
    "\n",
    "# ============================================\n",
    "# Data utilities\n",
    "# ============================================\n",
    "\n",
    "def stratified_sample(labels, n_per_class, seed=42):\n",
    "    \"\"\"Create stratified sample with n_per_class from each class\"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    n_classes = int(labels.max().item()) + 1\n",
    "    \n",
    "    sampled_indices = []\n",
    "    for c in range(n_classes):\n",
    "        idx_c = torch.where(labels == c)[0].numpy()\n",
    "        k = min(n_per_class, len(idx_c))\n",
    "        selected = rng.choice(idx_c, size=k, replace=False).tolist()\n",
    "        sampled_indices.extend(selected)\n",
    "    \n",
    "    rng.shuffle(sampled_indices)\n",
    "    return sampled_indices\n",
    "\n",
    "# ============================================\n",
    "# Main\n",
    "# ============================================\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"BLOCK 1: RealNet Training on QM9\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nConfiguration:\")\n",
    "    print(\"  • Dataset: QM9 HOMO-LUMO gap classification\")\n",
    "    print(\"  • Features: 512-D Morgan fingerprints (frozen)\")\n",
    "    print(\"  • Classes: 3 (Low/Med/High gap)\")\n",
    "    print(\"  • Architecture: 512 → 64 → 3\")\n",
    "    print(\"  • Batch size: 128 (train), 256 (test)\")\n",
    "    print(\"  • Max epochs: 200\")\n",
    "    print(\"  • Patience: 10\")\n",
    "    print(\"  • Seeds: [42, 123, 456]\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Load preprocessed data\n",
    "    print(\"\\nLoading preprocessed QM9 data...\")\n",
    "    data = torch.load(\"qm9_preprocessed_512d.pt\", weights_only=False)\n",
    "    features = data['features']\n",
    "    labels = data['labels']\n",
    "    \n",
    "    print(f\"  Loaded {len(features)} molecules\")\n",
    "    print(f\"  Feature dim: {features.shape[1]}\")\n",
    "    print(f\"  Classes: {len(torch.unique(labels))}\")\n",
    "    \n",
    "    # Create stratified train/test splits\n",
    "    print(\"\\nCreating stratified samples...\")\n",
    "    train_indices = stratified_sample(labels, n_per_class=1500, seed=42)\n",
    "    test_indices = stratified_sample(labels, n_per_class=300, seed=43)\n",
    "    \n",
    "    # Remove overlap\n",
    "    test_indices = [i for i in test_indices if i not in train_indices]\n",
    "    \n",
    "    print(f\"  Train samples: {len(train_indices)} (1500 per class)\")\n",
    "    print(f\"  Test samples: {len(test_indices)} (300 per class)\")\n",
    "    \n",
    "    # Create datasets\n",
    "    full_dataset = TensorDataset(features, labels)\n",
    "    train_ds = Subset(full_dataset, train_indices)\n",
    "    test_ds = Subset(full_dataset, test_indices)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\n",
    "    \n",
    "    seeds = [42, 123, 456]\n",
    "    all_results = []\n",
    "    \n",
    "    for seed in seeds:\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(f\"SEED {seed}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        set_all_seeds(seed)\n",
    "        \n",
    "        print(f\"\\n  Training RealNet (seed={seed})...\")\n",
    "        model = RealNet(input_dim=512, hidden_dim=64, num_classes=3).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "        \n",
    "        result = train_with_early_stopping(\n",
    "            model, train_loader, test_loader, optimizer, device,\n",
    "            max_epochs=200, patience=10, name=\"RealNet\"\n",
    "        )\n",
    "        \n",
    "        trainable_params = sum(p.numel() for p in model.parameters())\n",
    "        result[\"trainable_params\"] = trainable_params\n",
    "        result[\"seed\"] = seed\n",
    "        \n",
    "        all_results.append(result)\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"REALNET QM9 SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    accs = [r[\"best_acc\"] for r in all_results]\n",
    "    times = [r[\"time\"] for r in all_results]\n",
    "    epochs = [r[\"epochs\"] for r in all_results]\n",
    "    params = all_results[0][\"trainable_params\"]\n",
    "    \n",
    "    print(f\"\\nAccuracy:     {np.mean(accs):.4f} ± {np.std(accs):.4f}\")\n",
    "    print(f\"Time:         {np.mean(times):.1f}s ± {np.std(times):.1f}s\")\n",
    "    print(f\"Epochs:       {np.mean(epochs):.1f} ± {np.std(epochs):.1f}\")\n",
    "    print(f\"Parameters:   {params:,}\")\n",
    "    \n",
    "    print(\"\\nPer-seed results:\")\n",
    "    for r in all_results:\n",
    "        print(f\"  Seed {r['seed']}: acc={r['best_acc']:.4f}, \"\n",
    "              f\"time={r['time']:.1f}s, epochs={r['epochs']}\")\n",
    "    \n",
    "    # Save results\n",
    "    save_dict = {\n",
    "        \"results\": all_results,\n",
    "        \"summary\": {\n",
    "            \"mean_acc\": np.mean(accs),\n",
    "            \"std_acc\": np.std(accs),\n",
    "            \"mean_time\": np.mean(times),\n",
    "            \"trainable_params\": params\n",
    "        },\n",
    "        \"train_indices\": train_indices,\n",
    "        \"test_indices\": test_indices\n",
    "    }\n",
    "    \n",
    "    torch.save(save_dict, \"realnet_qm9_results.pt\")\n",
    "    print(f\"\\n✓ Saved results to: realnet_qm9_results.pt\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6e51c9-07da-4faa-87b9-fd26173dac51",
   "metadata": {},
   "source": [
    "# QuatNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5ab80e7-c94a-4248-8ddf-d2fb37771131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "======================================================================\n",
      "BLOCK 2: QuatNet Training on QM9 (MATCHED CAPACITY)\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "  • Dataset: QM9 HOMO-LUMO gap classification\n",
      "  • Features: 512-D Morgan fingerprints (frozen)\n",
      "  • Classes: 3 (Low/Med/High gap)\n",
      "  • Architecture: 128 quats → 64 quats → 3 quats\n",
      "  • Target params: ~33K (matching RealNet)\n",
      "  • Batch size: 128 (train), 256 (test)\n",
      "  • Max epochs: 200\n",
      "  • Patience: 10\n",
      "  • Seeds: [42, 123, 456]\n",
      "======================================================================\n",
      "\n",
      "Loading preprocessed QM9 data...\n",
      "  Loaded 129227 molecules\n",
      "  Feature dim: 512\n",
      "\n",
      "Loading train/test split from Block 1...\n",
      "  Train samples: 4500\n",
      "  Test samples: 875\n",
      "\n",
      "======================================================================\n",
      "SEED 42\n",
      "======================================================================\n",
      "\n",
      "  Training QuatNet (matched capacity, seed=42)...\n",
      "  QuatNet parameters: 33,804 (RealNet: 33,027)\n",
      "  [QuatNet] Epoch  1 | loss=1.1132 | test_acc=0.3886 | time=0.1s\n",
      "  [QuatNet] Epoch  2 | loss=1.0424 | test_acc=0.4560 | time=0.3s\n",
      "  [QuatNet] Epoch  3 | loss=0.9892 | test_acc=0.4949 | time=0.4s\n",
      "  [QuatNet] Epoch  4 | loss=0.9448 | test_acc=0.5269 | time=0.6s\n",
      "  [QuatNet] Epoch  5 | loss=0.9072 | test_acc=0.5543 | time=0.8s\n",
      "  [QuatNet] Epoch  6 | loss=0.8748 | test_acc=0.5806 | time=0.9s\n",
      "  [QuatNet] Epoch  7 | loss=0.8467 | test_acc=0.6046 | time=1.1s\n",
      "  [QuatNet] Epoch  8 | loss=0.8226 | test_acc=0.6126 | time=1.2s\n",
      "  [QuatNet] Epoch  9 | loss=0.8012 | test_acc=0.6274 | time=1.3s\n",
      "  [QuatNet] Epoch 10 | loss=0.7825 | test_acc=0.6217 | time=1.5s\n",
      "  [QuatNet] Epoch 11 | loss=0.7658 | test_acc=0.6354 | time=1.6s\n",
      "  [QuatNet] Epoch 12 | loss=0.7509 | test_acc=0.6389 | time=1.8s\n",
      "  [QuatNet] Epoch 13 | loss=0.7370 | test_acc=0.6469 | time=1.9s\n",
      "  [QuatNet] Epoch 14 | loss=0.7246 | test_acc=0.6583 | time=2.0s\n",
      "  [QuatNet] Epoch 15 | loss=0.7129 | test_acc=0.6640 | time=2.2s\n",
      "  [QuatNet] Epoch 16 | loss=0.7033 | test_acc=0.6674 | time=2.3s\n",
      "  [QuatNet] Epoch 17 | loss=0.6933 | test_acc=0.6686 | time=2.4s\n",
      "  [QuatNet] Epoch 18 | loss=0.6846 | test_acc=0.6674 | time=2.6s\n",
      "  [QuatNet] Epoch 19 | loss=0.6765 | test_acc=0.6686 | time=2.7s\n",
      "  [QuatNet] Epoch 20 | loss=0.6692 | test_acc=0.6743 | time=2.8s\n",
      "  [QuatNet] Epoch 21 | loss=0.6623 | test_acc=0.6800 | time=3.0s\n",
      "  [QuatNet] Epoch 22 | loss=0.6551 | test_acc=0.6731 | time=3.1s\n",
      "  [QuatNet] Epoch 23 | loss=0.6493 | test_acc=0.6777 | time=3.2s\n",
      "  [QuatNet] Epoch 24 | loss=0.6429 | test_acc=0.6731 | time=3.4s\n",
      "  [QuatNet] Epoch 25 | loss=0.6370 | test_acc=0.6811 | time=3.5s\n",
      "  [QuatNet] Epoch 26 | loss=0.6312 | test_acc=0.6800 | time=3.7s\n",
      "  [QuatNet] Epoch 27 | loss=0.6260 | test_acc=0.6846 | time=3.8s\n",
      "  [QuatNet] Epoch 28 | loss=0.6211 | test_acc=0.6834 | time=4.0s\n",
      "  [QuatNet] Epoch 29 | loss=0.6161 | test_acc=0.6811 | time=4.2s\n",
      "  [QuatNet] Epoch 30 | loss=0.6111 | test_acc=0.6823 | time=4.3s\n",
      "  [QuatNet] Epoch 31 | loss=0.6066 | test_acc=0.6823 | time=4.5s\n",
      "  [QuatNet] Epoch 32 | loss=0.6022 | test_acc=0.6880 | time=4.6s\n",
      "  [QuatNet] Epoch 33 | loss=0.5979 | test_acc=0.6880 | time=4.8s\n",
      "  [QuatNet] Epoch 34 | loss=0.5933 | test_acc=0.6823 | time=4.9s\n",
      "  [QuatNet] Epoch 35 | loss=0.5893 | test_acc=0.6880 | time=5.0s\n",
      "  [QuatNet] Epoch 36 | loss=0.5851 | test_acc=0.6834 | time=5.2s\n",
      "  [QuatNet] Epoch 37 | loss=0.5812 | test_acc=0.6834 | time=5.3s\n",
      "  [QuatNet] Epoch 38 | loss=0.5777 | test_acc=0.6857 | time=5.5s\n",
      "  [QuatNet] Epoch 39 | loss=0.5747 | test_acc=0.6891 | time=5.6s\n",
      "  [QuatNet] Epoch 40 | loss=0.5708 | test_acc=0.6914 | time=5.8s\n",
      "  [QuatNet] Epoch 41 | loss=0.5663 | test_acc=0.6891 | time=5.9s\n",
      "  [QuatNet] Epoch 42 | loss=0.5627 | test_acc=0.6903 | time=6.0s\n",
      "  [QuatNet] Epoch 43 | loss=0.5595 | test_acc=0.6937 | time=6.2s\n",
      "  [QuatNet] Epoch 44 | loss=0.5557 | test_acc=0.6926 | time=6.3s\n",
      "  [QuatNet] Epoch 45 | loss=0.5527 | test_acc=0.6949 | time=6.5s\n",
      "  [QuatNet] Epoch 46 | loss=0.5493 | test_acc=0.6949 | time=6.6s\n",
      "  [QuatNet] Epoch 47 | loss=0.5464 | test_acc=0.6891 | time=6.8s\n",
      "  [QuatNet] Epoch 48 | loss=0.5425 | test_acc=0.6869 | time=6.9s\n",
      "  [QuatNet] Epoch 49 | loss=0.5397 | test_acc=0.6869 | time=7.1s\n",
      "  [QuatNet] Epoch 50 | loss=0.5361 | test_acc=0.6914 | time=7.2s\n",
      "  [QuatNet] Epoch 51 | loss=0.5331 | test_acc=0.6857 | time=7.4s\n",
      "  [QuatNet] Epoch 52 | loss=0.5297 | test_acc=0.6846 | time=7.5s\n",
      "  [QuatNet] Epoch 53 | loss=0.5266 | test_acc=0.6914 | time=7.7s\n",
      "  [QuatNet] Epoch 54 | loss=0.5240 | test_acc=0.6960 | time=7.8s\n",
      "  [QuatNet] Epoch 55 | loss=0.5211 | test_acc=0.6937 | time=8.0s\n",
      "  [QuatNet] Epoch 56 | loss=0.5175 | test_acc=0.6903 | time=8.1s\n",
      "  [QuatNet] Epoch 57 | loss=0.5145 | test_acc=0.6926 | time=8.2s\n",
      "  [QuatNet] Epoch 58 | loss=0.5114 | test_acc=0.6937 | time=8.4s\n",
      "  [QuatNet] Epoch 59 | loss=0.5083 | test_acc=0.6926 | time=8.5s\n",
      "  [QuatNet] Epoch 60 | loss=0.5052 | test_acc=0.6949 | time=8.7s\n",
      "  [QuatNet] Epoch 61 | loss=0.5021 | test_acc=0.6937 | time=8.8s\n",
      "  [QuatNet] Epoch 62 | loss=0.4990 | test_acc=0.6926 | time=9.0s\n",
      "  [QuatNet] Epoch 63 | loss=0.4961 | test_acc=0.6937 | time=9.2s\n",
      "  [QuatNet] Epoch 64 | loss=0.4932 | test_acc=0.6891 | time=9.3s\n",
      "  [QuatNet] Early stop at epoch 64 (no improvement for 10 epochs)\n",
      "\n",
      "======================================================================\n",
      "SEED 123\n",
      "======================================================================\n",
      "\n",
      "  Training QuatNet (matched capacity, seed=123)...\n",
      "  QuatNet parameters: 33,804 (RealNet: 33,027)\n",
      "  [QuatNet] Epoch  1 | loss=1.1291 | test_acc=0.4354 | time=0.1s\n",
      "  [QuatNet] Epoch  2 | loss=1.0558 | test_acc=0.4937 | time=0.3s\n",
      "  [QuatNet] Epoch  3 | loss=1.0000 | test_acc=0.5154 | time=0.4s\n",
      "  [QuatNet] Epoch  4 | loss=0.9506 | test_acc=0.5566 | time=0.6s\n",
      "  [QuatNet] Epoch  5 | loss=0.9097 | test_acc=0.5726 | time=0.7s\n",
      "  [QuatNet] Epoch  6 | loss=0.8761 | test_acc=0.5909 | time=0.9s\n",
      "  [QuatNet] Epoch  7 | loss=0.8478 | test_acc=0.6103 | time=1.0s\n",
      "  [QuatNet] Epoch  8 | loss=0.8235 | test_acc=0.6160 | time=1.1s\n",
      "  [QuatNet] Epoch  9 | loss=0.8024 | test_acc=0.6240 | time=1.3s\n",
      "  [QuatNet] Epoch 10 | loss=0.7844 | test_acc=0.6309 | time=1.4s\n",
      "  [QuatNet] Epoch 11 | loss=0.7680 | test_acc=0.6411 | time=1.6s\n",
      "  [QuatNet] Epoch 12 | loss=0.7534 | test_acc=0.6411 | time=1.7s\n",
      "  [QuatNet] Epoch 13 | loss=0.7399 | test_acc=0.6491 | time=1.8s\n",
      "  [QuatNet] Epoch 14 | loss=0.7277 | test_acc=0.6526 | time=2.0s\n",
      "  [QuatNet] Epoch 15 | loss=0.7171 | test_acc=0.6583 | time=2.1s\n",
      "  [QuatNet] Epoch 16 | loss=0.7070 | test_acc=0.6583 | time=2.2s\n",
      "  [QuatNet] Epoch 17 | loss=0.6973 | test_acc=0.6663 | time=2.4s\n",
      "  [QuatNet] Epoch 18 | loss=0.6893 | test_acc=0.6686 | time=2.5s\n",
      "  [QuatNet] Epoch 19 | loss=0.6810 | test_acc=0.6754 | time=2.6s\n",
      "  [QuatNet] Epoch 20 | loss=0.6735 | test_acc=0.6777 | time=2.8s\n",
      "  [QuatNet] Epoch 21 | loss=0.6663 | test_acc=0.6811 | time=2.9s\n",
      "  [QuatNet] Epoch 22 | loss=0.6597 | test_acc=0.6846 | time=3.1s\n",
      "  [QuatNet] Epoch 23 | loss=0.6531 | test_acc=0.6857 | time=3.2s\n",
      "  [QuatNet] Epoch 24 | loss=0.6472 | test_acc=0.6891 | time=3.3s\n",
      "  [QuatNet] Epoch 25 | loss=0.6412 | test_acc=0.6880 | time=3.5s\n",
      "  [QuatNet] Epoch 26 | loss=0.6357 | test_acc=0.6903 | time=3.6s\n",
      "  [QuatNet] Epoch 27 | loss=0.6307 | test_acc=0.6971 | time=3.7s\n",
      "  [QuatNet] Epoch 28 | loss=0.6253 | test_acc=0.6914 | time=3.9s\n",
      "  [QuatNet] Epoch 29 | loss=0.6203 | test_acc=0.6949 | time=4.0s\n",
      "  [QuatNet] Epoch 30 | loss=0.6153 | test_acc=0.6983 | time=4.2s\n",
      "  [QuatNet] Epoch 31 | loss=0.6109 | test_acc=0.6937 | time=4.3s\n",
      "  [QuatNet] Epoch 32 | loss=0.6067 | test_acc=0.6983 | time=4.4s\n",
      "  [QuatNet] Epoch 33 | loss=0.6022 | test_acc=0.6926 | time=4.6s\n",
      "  [QuatNet] Epoch 34 | loss=0.5976 | test_acc=0.6937 | time=4.7s\n",
      "  [QuatNet] Epoch 35 | loss=0.5934 | test_acc=0.6949 | time=4.9s\n",
      "  [QuatNet] Epoch 36 | loss=0.5895 | test_acc=0.6914 | time=5.0s\n",
      "  [QuatNet] Epoch 37 | loss=0.5850 | test_acc=0.6937 | time=5.2s\n",
      "  [QuatNet] Epoch 38 | loss=0.5814 | test_acc=0.6971 | time=5.3s\n",
      "  [QuatNet] Epoch 39 | loss=0.5774 | test_acc=0.6926 | time=5.5s\n",
      "  [QuatNet] Epoch 40 | loss=0.5734 | test_acc=0.6971 | time=5.6s\n",
      "  [QuatNet] Early stop at epoch 40 (no improvement for 10 epochs)\n",
      "\n",
      "======================================================================\n",
      "SEED 456\n",
      "======================================================================\n",
      "\n",
      "  Training QuatNet (matched capacity, seed=456)...\n",
      "  QuatNet parameters: 33,804 (RealNet: 33,027)\n",
      "  [QuatNet] Epoch  1 | loss=1.1331 | test_acc=0.3954 | time=0.2s\n",
      "  [QuatNet] Epoch  2 | loss=1.0524 | test_acc=0.4560 | time=0.3s\n",
      "  [QuatNet] Epoch  3 | loss=0.9931 | test_acc=0.4971 | time=0.4s\n",
      "  [QuatNet] Epoch  4 | loss=0.9474 | test_acc=0.5211 | time=0.6s\n",
      "  [QuatNet] Epoch  5 | loss=0.9110 | test_acc=0.5406 | time=0.7s\n",
      "  [QuatNet] Epoch  6 | loss=0.8807 | test_acc=0.5589 | time=0.9s\n",
      "  [QuatNet] Epoch  7 | loss=0.8551 | test_acc=0.5737 | time=1.0s\n",
      "  [QuatNet] Epoch  8 | loss=0.8325 | test_acc=0.5863 | time=1.2s\n",
      "  [QuatNet] Epoch  9 | loss=0.8125 | test_acc=0.6011 | time=1.4s\n",
      "  [QuatNet] Epoch 10 | loss=0.7944 | test_acc=0.6114 | time=1.5s\n",
      "  [QuatNet] Epoch 11 | loss=0.7785 | test_acc=0.6251 | time=1.7s\n",
      "  [QuatNet] Epoch 12 | loss=0.7643 | test_acc=0.6274 | time=1.9s\n",
      "  [QuatNet] Epoch 13 | loss=0.7507 | test_acc=0.6297 | time=2.0s\n",
      "  [QuatNet] Epoch 14 | loss=0.7386 | test_acc=0.6377 | time=3.2s\n",
      "  [QuatNet] Epoch 15 | loss=0.7276 | test_acc=0.6480 | time=3.3s\n",
      "  [QuatNet] Epoch 16 | loss=0.7173 | test_acc=0.6560 | time=3.5s\n",
      "  [QuatNet] Epoch 17 | loss=0.7078 | test_acc=0.6617 | time=3.7s\n",
      "  [QuatNet] Epoch 18 | loss=0.6983 | test_acc=0.6686 | time=3.8s\n",
      "  [QuatNet] Epoch 19 | loss=0.6896 | test_acc=0.6720 | time=4.0s\n",
      "  [QuatNet] Epoch 20 | loss=0.6816 | test_acc=0.6754 | time=4.1s\n",
      "  [QuatNet] Epoch 21 | loss=0.6737 | test_acc=0.6754 | time=4.3s\n",
      "  [QuatNet] Epoch 22 | loss=0.6658 | test_acc=0.6834 | time=4.4s\n",
      "  [QuatNet] Epoch 23 | loss=0.6590 | test_acc=0.6869 | time=4.6s\n",
      "  [QuatNet] Epoch 24 | loss=0.6524 | test_acc=0.6891 | time=4.8s\n",
      "  [QuatNet] Epoch 25 | loss=0.6458 | test_acc=0.6891 | time=4.9s\n",
      "  [QuatNet] Epoch 26 | loss=0.6400 | test_acc=0.6880 | time=5.1s\n",
      "  [QuatNet] Epoch 27 | loss=0.6338 | test_acc=0.6903 | time=5.3s\n",
      "  [QuatNet] Epoch 28 | loss=0.6280 | test_acc=0.6903 | time=5.4s\n",
      "  [QuatNet] Epoch 29 | loss=0.6228 | test_acc=0.6949 | time=5.5s\n",
      "  [QuatNet] Epoch 30 | loss=0.6174 | test_acc=0.6914 | time=5.7s\n",
      "  [QuatNet] Epoch 31 | loss=0.6124 | test_acc=0.6983 | time=5.9s\n",
      "  [QuatNet] Epoch 32 | loss=0.6080 | test_acc=0.6926 | time=6.0s\n",
      "  [QuatNet] Epoch 33 | loss=0.6028 | test_acc=0.6926 | time=6.2s\n",
      "  [QuatNet] Epoch 34 | loss=0.5980 | test_acc=0.6983 | time=6.3s\n",
      "  [QuatNet] Epoch 35 | loss=0.5938 | test_acc=0.6994 | time=6.5s\n",
      "  [QuatNet] Epoch 36 | loss=0.5892 | test_acc=0.7063 | time=6.6s\n",
      "  [QuatNet] Epoch 37 | loss=0.5848 | test_acc=0.7017 | time=6.8s\n",
      "  [QuatNet] Epoch 38 | loss=0.5804 | test_acc=0.6983 | time=6.9s\n",
      "  [QuatNet] Epoch 39 | loss=0.5757 | test_acc=0.7074 | time=7.1s\n",
      "  [QuatNet] Epoch 40 | loss=0.5718 | test_acc=0.7074 | time=7.2s\n",
      "  [QuatNet] Epoch 41 | loss=0.5678 | test_acc=0.7040 | time=7.4s\n",
      "  [QuatNet] Epoch 42 | loss=0.5639 | test_acc=0.7086 | time=7.6s\n",
      "  [QuatNet] Epoch 43 | loss=0.5600 | test_acc=0.7051 | time=7.8s\n",
      "  [QuatNet] Epoch 44 | loss=0.5561 | test_acc=0.7086 | time=8.0s\n",
      "  [QuatNet] Epoch 45 | loss=0.5526 | test_acc=0.7017 | time=8.2s\n",
      "  [QuatNet] Epoch 46 | loss=0.5487 | test_acc=0.7074 | time=8.4s\n",
      "  [QuatNet] Epoch 47 | loss=0.5456 | test_acc=0.7097 | time=8.6s\n",
      "  [QuatNet] Epoch 48 | loss=0.5419 | test_acc=0.7063 | time=8.7s\n",
      "  [QuatNet] Epoch 49 | loss=0.5381 | test_acc=0.7017 | time=8.9s\n",
      "  [QuatNet] Epoch 50 | loss=0.5347 | test_acc=0.7063 | time=9.1s\n",
      "  [QuatNet] Epoch 51 | loss=0.5313 | test_acc=0.7040 | time=9.2s\n",
      "  [QuatNet] Epoch 52 | loss=0.5278 | test_acc=0.7063 | time=9.4s\n",
      "  [QuatNet] Epoch 53 | loss=0.5243 | test_acc=0.7086 | time=9.5s\n",
      "  [QuatNet] Epoch 54 | loss=0.5209 | test_acc=0.7086 | time=9.7s\n",
      "  [QuatNet] Epoch 55 | loss=0.5174 | test_acc=0.7097 | time=9.8s\n",
      "  [QuatNet] Epoch 56 | loss=0.5145 | test_acc=0.7051 | time=10.0s\n",
      "  [QuatNet] Epoch 57 | loss=0.5111 | test_acc=0.7086 | time=10.1s\n",
      "  [QuatNet] Early stop at epoch 57 (no improvement for 10 epochs)\n",
      "\n",
      "======================================================================\n",
      "QUATNET QM9 SUMMARY (MATCHED CAPACITY)\n",
      "======================================================================\n",
      "\n",
      "Accuracy:     0.7013 ± 0.0060\n",
      "Time:         8.4s ± 2.0s\n",
      "Epochs:       53.7 ± 10.1\n",
      "Parameters:   33,804\n",
      "\n",
      "Per-seed results:\n",
      "  Seed 42: acc=0.6960, time=9.3s, epochs=64\n",
      "  Seed 123: acc=0.6983, time=5.6s, epochs=40\n",
      "  Seed 456: acc=0.7097, time=10.1s, epochs=57\n",
      "\n",
      "======================================================================\n",
      "COMPARISON TO REALNET (MATCHED CAPACITY)\n",
      "======================================================================\n",
      "RealNet:  72.80% (33,027 params)\n",
      "QuatNet:  70.13% (33,804 params)\n",
      "Gap:      -2.67pp\n",
      "\n",
      "✗ QuatNet still underperforms by 2.67pp\n",
      "  Conclusion: Geometric mismatch confirmed with matched capacity\n",
      "\n",
      "✓ Saved results to: quatnet_qm9_results.pt\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Block 2: QuatNet Training on QM9\n",
    "=================================\n",
    "Trains quaternion-valued classifier on QM9 HOMO-LUMO gap classification.\n",
    "Uses frozen molecular fingerprints from Block 0.\n",
    "\n",
    "Requirements:\n",
    "- qm9_preprocessed_512d.pt (from Block 0)\n",
    "- realnet_qm9_results.pt (from Block 1) - for train/test indices\n",
    "\n",
    "Outputs:\n",
    "- quatnet_qm9_results.pt: Contains QuatNet results\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ============================================\n",
    "# Seed setting\n",
    "# ============================================\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    \"\"\"Set seeds for all RNG sources for reproducibility\"\"\"\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# ============================================\n",
    "# Quaternion Layer Implementation\n",
    "# ============================================\n",
    "\n",
    "class QuaternionLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Quaternion linear layer with Hamilton product (VECTORIZED)\n",
    "    Input: (batch, n_in, 4) - n_in quaternions\n",
    "    Output: (batch, n_out, 4) - n_out quaternions\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Weights: (out_features, in_features, 4)\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features, 4) * 0.1)\n",
    "        self.bias = nn.Parameter(torch.zeros(out_features, 4))\n",
    "        \n",
    "        # Initialize as unit quaternions\n",
    "        with torch.no_grad():\n",
    "            self._normalize_weights()\n",
    "    \n",
    "    def _normalize_weights(self):\n",
    "        \"\"\"Normalize weights to unit quaternions\"\"\"\n",
    "        norms = torch.sqrt((self.weight ** 2).sum(dim=2, keepdim=True))\n",
    "        self.weight.data = self.weight.data / (norms + 1e-8)\n",
    "    \n",
    "    def hamilton_product_vectorized(self, q1, q2):\n",
    "        \"\"\"\n",
    "        Vectorized Hamilton product supporting arbitrary leading dimensions\n",
    "        q1, q2: (..., 4) tensors [w, x, y, z]\n",
    "        Returns: (..., 4) tensor\n",
    "        \"\"\"\n",
    "        w1, x1, y1, z1 = q1[..., 0], q1[..., 1], q1[..., 2], q1[..., 3]\n",
    "        w2, x2, y2, z2 = q2[..., 0], q2[..., 1], q2[..., 2], q2[..., 3]\n",
    "        \n",
    "        w = w1*w2 - x1*x2 - y1*y2 - z1*z2\n",
    "        x = w1*x2 + x1*w2 + y1*z2 - z1*y2\n",
    "        y = w1*y2 - x1*z2 + y1*w2 + z1*x2\n",
    "        z = w1*z2 + x1*y2 - y1*x2 + z1*w2\n",
    "        \n",
    "        return torch.stack([w, x, y, z], dim=-1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Vectorized forward pass\n",
    "        x: (batch, in_features, 4)\n",
    "        Returns: (batch, out_features, 4)\n",
    "        \"\"\"\n",
    "        # Normalize weights\n",
    "        self._normalize_weights()\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Expand dimensions for broadcasting\n",
    "        # x: (batch, 1, in_features, 4)\n",
    "        # weight: (1, out_features, in_features, 4)\n",
    "        x_expanded = x.unsqueeze(1)  # (batch, 1, in_features, 4)\n",
    "        w_expanded = self.weight.unsqueeze(0)  # (1, out_features, in_features, 4)\n",
    "        \n",
    "        # Vectorized Hamilton product\n",
    "        # Computes all products at once: (batch, out_features, in_features, 4)\n",
    "        products = self.hamilton_product_vectorized(w_expanded, x_expanded)\n",
    "        \n",
    "        # Sum over input features\n",
    "        output = products.sum(dim=2)  # (batch, out_features, 4)\n",
    "        \n",
    "        # Add bias\n",
    "        output = output + self.bias.unsqueeze(0)  # (batch, out_features, 4)\n",
    "        \n",
    "        # Normalize outputs\n",
    "        norms = torch.sqrt((output ** 2).sum(dim=2, keepdim=True))\n",
    "        output = output / (norms + 1e-8)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class QuatNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Quaternion classifier for QM9 with MATCHED capacity to RealNet\n",
    "    512-D → 64 quats → 3 quats → 3 classes (~33K params, same as RealNet)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=512, num_classes=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # input_dim must be divisible by 4\n",
    "        assert input_dim % 4 == 0, \"input_dim must be divisible by 4\"\n",
    "        \n",
    "        self.n_input_quats = input_dim // 4  # 128 quaternions\n",
    "        \n",
    "        # Match RealNet structure: 512 → 64 → 3\n",
    "        # In quaternion space: 128 quats → 64 quats → 3 quats\n",
    "        self.quat1 = QuaternionLinear(self.n_input_quats, 64)  # 128×64×4 = 32,768\n",
    "        self.quat2 = QuaternionLinear(64, num_classes)         # 64×3×4 = 768\n",
    "        # Total: ~33,536 params (matches RealNet's 33,027)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch, 512) real features\n",
    "        Returns: (batch, 3) class logits\n",
    "        \"\"\"\n",
    "        # Reshape to quaternions: (batch, 128, 4)\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, self.n_input_quats, 4)\n",
    "        \n",
    "        # First quaternion layer\n",
    "        x = self.quat1(x)  # (batch, 64, 4)\n",
    "        x = torch.tanh(x)\n",
    "        \n",
    "        # Second quaternion layer\n",
    "        x = self.quat2(x)  # (batch, 3, 4)\n",
    "        \n",
    "        # Extract real part for classification\n",
    "        logits = x[..., 0]  # (batch, 3)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# ============================================\n",
    "# Training and Evaluation\n",
    "# ============================================\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "    \n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += x.size(0)\n",
    "    \n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "def train_with_early_stopping(model, train_loader, test_loader, optimizer,\n",
    "                              device, max_epochs=200, patience=10, name=\"Model\"):\n",
    "    best_acc = 0.0\n",
    "    epochs_without_improvement = 0\n",
    "    start = time.time()\n",
    "    last_acc = 0.0\n",
    "    \n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        loss = train_one_epoch(model, train_loader, optimizer, device)\n",
    "        acc = evaluate(model, test_loader, device)\n",
    "        last_acc = acc\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        print(f\"  [{name}] Epoch {epoch:2d} | loss={loss:.4f} | \"\n",
    "              f\"test_acc={acc:.4f} | time={elapsed:.1f}s\")\n",
    "        \n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        \n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"  [{name}] Early stop at epoch {epoch} \"\n",
    "                  f\"(no improvement for {patience} epochs)\")\n",
    "            break\n",
    "    \n",
    "    total_time = time.time() - start\n",
    "    return {\n",
    "        \"best_acc\": best_acc,\n",
    "        \"final_acc\": last_acc,\n",
    "        \"time\": total_time,\n",
    "        \"epochs\": epoch,\n",
    "    }\n",
    "\n",
    "# ============================================\n",
    "# Main\n",
    "# ============================================\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"BLOCK 2: QuatNet Training on QM9 (MATCHED CAPACITY)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nConfiguration:\")\n",
    "    print(\"  • Dataset: QM9 HOMO-LUMO gap classification\")\n",
    "    print(\"  • Features: 512-D Morgan fingerprints (frozen)\")\n",
    "    print(\"  • Classes: 3 (Low/Med/High gap)\")\n",
    "    print(\"  • Architecture: 128 quats → 64 quats → 3 quats\")\n",
    "    print(\"  • Target params: ~33K (matching RealNet)\")\n",
    "    print(\"  • Batch size: 128 (train), 256 (test)\")\n",
    "    print(\"  • Max epochs: 200\")\n",
    "    print(\"  • Patience: 10\")\n",
    "    print(\"  • Seeds: [42, 123, 456]\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Load preprocessed data\n",
    "    print(\"\\nLoading preprocessed QM9 data...\")\n",
    "    data = torch.load(\"qm9_preprocessed_512d.pt\", weights_only=False)\n",
    "    features = data['features']\n",
    "    labels = data['labels']\n",
    "    \n",
    "    print(f\"  Loaded {len(features)} molecules\")\n",
    "    print(f\"  Feature dim: {features.shape[1]}\")\n",
    "    \n",
    "    # Load train/test indices from Block 1\n",
    "    print(\"\\nLoading train/test split from Block 1...\")\n",
    "    realnet_data = torch.load(\"realnet_qm9_results.pt\", weights_only=False)\n",
    "    train_indices = realnet_data['train_indices']\n",
    "    test_indices = realnet_data['test_indices']\n",
    "    \n",
    "    print(f\"  Train samples: {len(train_indices)}\")\n",
    "    print(f\"  Test samples: {len(test_indices)}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    full_dataset = TensorDataset(features, labels)\n",
    "    train_ds = Subset(full_dataset, train_indices)\n",
    "    test_ds = Subset(full_dataset, test_indices)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=256, shuffle=False)\n",
    "    \n",
    "    seeds = [42, 123, 456]\n",
    "    all_results = []\n",
    "    \n",
    "    for seed in seeds:\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(f\"SEED {seed}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        set_all_seeds(seed)\n",
    "        \n",
    "        print(f\"\\n  Training QuatNet (matched capacity, seed={seed})...\")\n",
    "        model = QuatNet(input_dim=512, num_classes=3).to(device)\n",
    "        \n",
    "        # Verify parameter count\n",
    "        trainable_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"  QuatNet parameters: {trainable_params:,} (RealNet: 33,027)\")\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "        \n",
    "        result = train_with_early_stopping(\n",
    "            model, train_loader, test_loader, optimizer, device,\n",
    "            max_epochs=200, patience=10, name=\"QuatNet\"\n",
    "        )\n",
    "        \n",
    "        result[\"trainable_params\"] = trainable_params\n",
    "        result[\"seed\"] = seed\n",
    "        \n",
    "        all_results.append(result)\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"QUATNET QM9 SUMMARY (MATCHED CAPACITY)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    accs = [r[\"best_acc\"] for r in all_results]\n",
    "    times = [r[\"time\"] for r in all_results]\n",
    "    epochs = [r[\"epochs\"] for r in all_results]\n",
    "    params = all_results[0][\"trainable_params\"]\n",
    "    \n",
    "    print(f\"\\nAccuracy:     {np.mean(accs):.4f} ± {np.std(accs):.4f}\")\n",
    "    print(f\"Time:         {np.mean(times):.1f}s ± {np.std(times):.1f}s\")\n",
    "    print(f\"Epochs:       {np.mean(epochs):.1f} ± {np.std(epochs):.1f}\")\n",
    "    print(f\"Parameters:   {params:,}\")\n",
    "    \n",
    "    print(\"\\nPer-seed results:\")\n",
    "    for r in all_results:\n",
    "        print(f\"  Seed {r['seed']}: acc={r['best_acc']:.4f}, \"\n",
    "              f\"time={r['time']:.1f}s, epochs={r['epochs']}\")\n",
    "    \n",
    "    # Compare to RealNet\n",
    "    realnet_acc = 0.7280  # From Block 1\n",
    "    quat_acc = np.mean(accs)\n",
    "    gap = quat_acc - realnet_acc\n",
    "    \n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(\"COMPARISON TO REALNET (MATCHED CAPACITY)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"RealNet:  72.80% (33,027 params)\")\n",
    "    print(f\"QuatNet:  {quat_acc*100:.2f}% ({params:,} params)\")\n",
    "    print(f\"Gap:      {gap*100:+.2f}pp\")\n",
    "    \n",
    "    if abs(gap) < 0.01:\n",
    "        print(\"\\n✓ QuatNet MATCHES RealNet with fair comparison!\")\n",
    "        print(\"  Conclusion: Earlier degradation was parameter starvation\")\n",
    "    else:\n",
    "        print(f\"\\n✗ QuatNet still underperforms by {abs(gap)*100:.2f}pp\")\n",
    "        print(\"  Conclusion: Geometric mismatch confirmed with matched capacity\")\n",
    "    \n",
    "    # Save results\n",
    "    save_dict = {\n",
    "        \"results\": all_results,\n",
    "        \"summary\": {\n",
    "            \"mean_acc\": np.mean(accs),\n",
    "            \"std_acc\": np.std(accs),\n",
    "            \"mean_time\": np.mean(times),\n",
    "            \"trainable_params\": params\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    torch.save(save_dict, \"quatnet_qm9_results.pt\")\n",
    "    print(f\"\\n✓ Saved results to: quatnet_qm9_results.pt\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d265882b-0a86-41b9-ada0-f3d3f6860723",
   "metadata": {},
   "source": [
    "# Quantum No Entanglement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4564c42f-0446-4f47-8c01-8ce6ee1c2e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using lightning.gpu device\n",
      "Using PyTorch device: cuda\n",
      "======================================================================\n",
      "BLOCK 3: Quantum (NO Entanglement) Training on QM9\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "  • Dataset: QM9 HOMO-LUMO gap classification\n",
      "  • Features: 512-D Morgan fingerprints (frozen)\n",
      "  • Classes: 3 (Low/Med/High gap)\n",
      "  • Architecture: 512 → 4 qubits (3 layers, NO ent) → 6 meas → 3\n",
      "  • Batch size: 32 (train), 64 (test)\n",
      "  • Max epochs: 200\n",
      "  • Patience: 10\n",
      "  • Seeds: [42, 123, 456]\n",
      "  • Quantum device: lightning.gpu\n",
      "======================================================================\n",
      "\n",
      "Loading preprocessed QM9 data...\n",
      "  Loaded 129227 molecules\n",
      "  Feature dim: 512\n",
      "\n",
      "Loading train/test split from Block 1...\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnpicklingError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 333\u001b[39m\n\u001b[32m    330\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m)\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 256\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    254\u001b[39m \u001b[38;5;66;03m# Load train/test indices from Block 1\u001b[39;00m\n\u001b[32m    255\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mLoading train/test split from Block 1...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m realnet_data = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrealnet_qm9_results.pt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    257\u001b[39m train_indices = realnet_data[\u001b[33m'\u001b[39m\u001b[33mtrain_indices\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    258\u001b[39m test_indices = realnet_data[\u001b[33m'\u001b[39m\u001b[33mtest_indices\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/torch/lib/python3.12/site-packages/torch/serialization.py:1529\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1521\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[32m   1522\u001b[39m                     opened_zipfile,\n\u001b[32m   1523\u001b[39m                     map_location,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1526\u001b[39m                     **pickle_load_args,\n\u001b[32m   1527\u001b[39m                 )\n\u001b[32m   1528\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle.UnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1529\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1530\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[32m   1531\u001b[39m             opened_zipfile,\n\u001b[32m   1532\u001b[39m             map_location,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1535\u001b[39m             **pickle_load_args,\n\u001b[32m   1536\u001b[39m         )\n\u001b[32m   1537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[31mUnpicklingError\u001b[39m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Block 3: Quantum (No Entanglement) Training on QM9\n",
    "===================================================\n",
    "Trains quantum classifier WITHOUT entanglement on QM9 HOMO-LUMO gap.\n",
    "Uses frozen molecular fingerprints from Block 0.\n",
    "\n",
    "Requirements:\n",
    "- qm9_preprocessed_512d.pt (from Block 0)\n",
    "- realnet_qm9_results.pt (from Block 1) - for train/test indices\n",
    "- pennylane, pennylane-lightning-gpu\n",
    "\n",
    "Outputs:\n",
    "- quantum_noent_qm9_results.pt: Contains quantum (no ent) results\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "\n",
    "try:\n",
    "    import pennylane as qml\n",
    "    QUANTUM_DEVICE = \"lightning.gpu\"\n",
    "    PENNYLANE_AVAILABLE = True\n",
    "    print(\"✓ Using lightning.gpu device\")\n",
    "except ImportError:\n",
    "    PENNYLANE_AVAILABLE = False\n",
    "    print(\"✗ PennyLane not installed\")\n",
    "    exit(1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using PyTorch device: {device}\")\n",
    "\n",
    "# ============================================\n",
    "# Seed setting\n",
    "# ============================================\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    \"\"\"Set seeds for all RNG sources for reproducibility\"\"\"\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    try:\n",
    "        import cupy as cp\n",
    "        cp.random.seed(seed)\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        qml.numpy.random.seed(seed)\n",
    "    except (ImportError, AttributeError):\n",
    "        pass\n",
    "\n",
    "# ============================================\n",
    "# Quantum Head (No Entanglement)\n",
    "# ============================================\n",
    "\n",
    "class QuantumHead(nn.Module):\n",
    "    \"\"\"\n",
    "    VQC with 4 qubits, 3 layers, NO entanglement → 3 classes\n",
    "    Uses Lightning acceleration (GPU)\n",
    "    Maps 512 features → 4 qubits\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=512, n_qubits=4, n_layers=3, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Map 512 features → n_qubits\n",
    "        self.feature_select = nn.Linear(input_dim, n_qubits)\n",
    "        \n",
    "        # Quantum device\n",
    "        self.dev = qml.device(QUANTUM_DEVICE, wires=n_qubits)\n",
    "        \n",
    "        # Use adjoint differentiation for lightning\n",
    "        @qml.qnode(self.dev, interface=\"torch\", diff_method=\"adjoint\")\n",
    "        def quantum_circuit(inputs, weights):\n",
    "            \"\"\"\n",
    "            Single-sample circuit WITHOUT entanglement\n",
    "            inputs: (n_qubits,)\n",
    "            weights: (n_layers, n_qubits, 2)\n",
    "            \"\"\"\n",
    "            for layer in range(n_layers):\n",
    "                # Data re-uploading\n",
    "                for i in range(n_qubits):\n",
    "                    qml.RY(inputs[i], wires=i)\n",
    "                \n",
    "                # Trainable rotations\n",
    "                for i in range(n_qubits):\n",
    "                    qml.RY(weights[layer, i, 0], wires=i)\n",
    "                    qml.RZ(weights[layer, i, 1], wires=i)\n",
    "                \n",
    "                # NO ENTANGLEMENT\n",
    "            \n",
    "            # Measurements\n",
    "            measurements = []\n",
    "            # Single-qubit Z\n",
    "            for i in range(n_qubits):\n",
    "                measurements.append(qml.expval(qml.PauliZ(i)))\n",
    "            # Two-qubit ZZ (pairs)\n",
    "            for i in range(0, n_qubits-1, 2):\n",
    "                measurements.append(qml.expval(qml.PauliZ(i) @ qml.PauliZ(i+1)))\n",
    "            \n",
    "            return measurements\n",
    "        \n",
    "        self.quantum_circuit = quantum_circuit\n",
    "        \n",
    "        weight_shape = (n_layers, n_qubits, 2)\n",
    "        self.q_weights = nn.Parameter(torch.randn(weight_shape) * 0.1)\n",
    "        \n",
    "        # Output: 6 measurements → 3 classes\n",
    "        n_measurements = n_qubits + (n_qubits // 2)\n",
    "        self.fc_out = nn.Linear(n_measurements, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch, 512) features\n",
    "        Returns: (batch, 3) logits\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        x = torch.tanh(self.feature_select(x))\n",
    "        \n",
    "        # Process samples one by one\n",
    "        quantum_outputs = []\n",
    "        for i in range(batch_size):\n",
    "            q_raw = self.quantum_circuit(x[i], self.q_weights)\n",
    "            if isinstance(q_raw, (list, tuple)):\n",
    "                q_out = torch.stack(q_raw)\n",
    "            else:\n",
    "                q_out = q_raw\n",
    "            quantum_outputs.append(q_out)\n",
    "        \n",
    "        quantum_outputs = torch.stack(quantum_outputs).float()\n",
    "        quantum_outputs = quantum_outputs.to(self.fc_out.weight.device)\n",
    "        \n",
    "        output = self.fc_out(quantum_outputs)\n",
    "        return output\n",
    "\n",
    "# ============================================\n",
    "# Training and Evaluation\n",
    "# ============================================\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, device, show_progress=True):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (x, y) in enumerate(loader):\n",
    "        x = x.to(device)\n",
    "        # y stays on CPU for quantum model\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        \n",
    "        if show_progress and batch_idx % 20 == 0:\n",
    "            print(f\"    Batch {batch_idx}/{len(loader)}\", end=\"\\r\")\n",
    "    \n",
    "    if show_progress:\n",
    "        print()\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            logits = model(x)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += x.size(0)\n",
    "    \n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "def train_with_early_stopping(model, train_loader, test_loader, optimizer,\n",
    "                              device, max_epochs=200, patience=10, name=\"Model\"):\n",
    "    best_acc = 0.0\n",
    "    epochs_without_improvement = 0\n",
    "    start = time.time()\n",
    "    last_acc = 0.0\n",
    "    \n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        print(f\"  [{name}] Epoch {epoch}/{max_epochs}\")\n",
    "        loss = train_one_epoch(model, train_loader, optimizer, device, show_progress=True)\n",
    "        acc = evaluate(model, test_loader, device)\n",
    "        last_acc = acc\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        print(f\"  [{name}] Epoch {epoch:2d} | loss={loss:.4f} | \"\n",
    "              f\"test_acc={acc:.4f} | time={elapsed:.1f}s\")\n",
    "        \n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        \n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"  [{name}] Early stop at epoch {epoch} \"\n",
    "                  f\"(no improvement for {patience} epochs)\")\n",
    "            break\n",
    "    \n",
    "    total_time = time.time() - start\n",
    "    return {\n",
    "        \"best_acc\": best_acc,\n",
    "        \"final_acc\": last_acc,\n",
    "        \"time\": total_time,\n",
    "        \"epochs\": epoch,\n",
    "    }\n",
    "\n",
    "# ============================================\n",
    "# Main\n",
    "# ============================================\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"BLOCK 3: Quantum (NO Entanglement) Training on QM9\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nConfiguration:\")\n",
    "    print(\"  • Dataset: QM9 HOMO-LUMO gap classification\")\n",
    "    print(\"  • Features: 512-D Morgan fingerprints (frozen)\")\n",
    "    print(\"  • Classes: 3 (Low/Med/High gap)\")\n",
    "    print(\"  • Architecture: 512 → 4 qubits (3 layers, NO ent) → 6 meas → 3\")\n",
    "    print(\"  • Batch size: 32 (train), 64 (test)\")\n",
    "    print(\"  • Max epochs: 200\")\n",
    "    print(\"  • Patience: 10\")\n",
    "    print(\"  • Seeds: [42, 123, 456]\")\n",
    "    print(f\"  • Quantum device: {QUANTUM_DEVICE}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Load preprocessed data\n",
    "    print(\"\\nLoading preprocessed QM9 data...\")\n",
    "    data = torch.load(\"qm9_preprocessed_512d.pt\", weights_only=False)\n",
    "    features = data['features']\n",
    "    labels = data['labels']\n",
    "    \n",
    "    print(f\"  Loaded {len(features)} molecules\")\n",
    "    print(f\"  Feature dim: {features.shape[1]}\")\n",
    "    \n",
    "    # Load train/test indices from Block 1\n",
    "    print(\"\\nLoading train/test split from Block 1...\")\n",
    "    realnet_data = torch.load(\"realnet_qm9_results.pt\")\n",
    "    train_indices = realnet_data['train_indices']\n",
    "    test_indices = realnet_data['test_indices']\n",
    "    \n",
    "    print(f\"  Train samples: {len(train_indices)}\")\n",
    "    print(f\"  Test samples: {len(test_indices)}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    full_dataset = TensorDataset(features, labels)\n",
    "    train_ds = Subset(full_dataset, train_indices)\n",
    "    test_ds = Subset(full_dataset, test_indices)\n",
    "    \n",
    "    # Smaller batches for quantum\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_ds, batch_size=64, shuffle=False, num_workers=0)\n",
    "    \n",
    "    seeds = [42, 123, 456]\n",
    "    all_results = []\n",
    "    \n",
    "    for seed in seeds:\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(f\"SEED {seed}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        set_all_seeds(seed)\n",
    "        \n",
    "        print(f\"\\n  Training QuantumNet (NO entanglement, seed={seed})...\")\n",
    "        model = QuantumHead(input_dim=512, n_qubits=4, n_layers=3, num_classes=3)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "        \n",
    "        result = train_with_early_stopping(\n",
    "            model, train_loader, test_loader, optimizer, device,\n",
    "            max_epochs=200, patience=10, name=\"QuantumNoEnt\"\n",
    "        )\n",
    "        \n",
    "        trainable_params = sum(p.numel() for p in model.parameters())\n",
    "        result[\"trainable_params\"] = trainable_params\n",
    "        result[\"seed\"] = seed\n",
    "        \n",
    "        all_results.append(result)\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"QUANTUM (NO ENTANGLEMENT) QM9 SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    accs = [r[\"best_acc\"] for r in all_results]\n",
    "    times = [r[\"time\"] for r in all_results]\n",
    "    epochs = [r[\"epochs\"] for r in all_results]\n",
    "    params = all_results[0][\"trainable_params\"]\n",
    "    \n",
    "    print(f\"\\nAccuracy:     {np.mean(accs):.4f} ± {np.std(accs):.4f}\")\n",
    "    print(f\"Time:         {np.mean(times):.1f}s ± {np.std(times):.1f}s\")\n",
    "    print(f\"Epochs:       {np.mean(epochs):.1f} ± {np.std(epochs):.1f}\")\n",
    "    print(f\"Parameters:   {params:,}\")\n",
    "    \n",
    "    print(\"\\nPer-seed results:\")\n",
    "    for r in all_results:\n",
    "        print(f\"  Seed {r['seed']}: acc={r['best_acc']:.4f}, \"\n",
    "              f\"time={r['time']:.1f}s, epochs={r['epochs']}\")\n",
    "    \n",
    "    # Save results\n",
    "    save_dict = {\n",
    "        \"results\": all_results,\n",
    "        \"summary\": {\n",
    "            \"mean_acc\": np.mean(accs),\n",
    "            \"std_acc\": np.std(accs),\n",
    "            \"mean_time\": np.mean(times),\n",
    "            \"trainable_params\": params\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    torch.save(save_dict, \"quantum_noent_qm9_results.pt\")\n",
    "    print(f\"\\n✓ Saved results to: quantum_noent_qm9_results.pt\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70d4e22-6fe3-4056-a372-d457ae0d44ee",
   "metadata": {},
   "source": [
    "# Quantum Entanglement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0471ea49-1eca-4564-b02c-9d9e421c5095",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Block 4: Quantum (WITH Entanglement) Training on QM9\n",
    "=====================================================\n",
    "Trains quantum classifier WITH entanglement on QM9 HOMO-LUMO gap.\n",
    "Uses frozen molecular fingerprints from Block 0.\n",
    "\n",
    "Requirements:\n",
    "- qm9_preprocessed_512d.pt (from Block 0)\n",
    "- realnet_qm9_results.pt (from Block 1) - for train/test indices\n",
    "- pennylane, pennylane-lightning-gpu\n",
    "\n",
    "Outputs:\n",
    "- quantum_ent_qm9_results.pt: Contains quantum (with ent) results\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "\n",
    "try:\n",
    "    import pennylane as qml\n",
    "    QUANTUM_DEVICE = \"lightning.gpu\"\n",
    "    PENNYLANE_AVAILABLE = True\n",
    "    print(\"✓ Using lightning.gpu device\")\n",
    "except ImportError:\n",
    "    PENNYLANE_AVAILABLE = False\n",
    "    print(\"✗ PennyLane not installed\")\n",
    "    exit(1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using PyTorch device: {device}\")\n",
    "\n",
    "# ============================================\n",
    "# Seed setting\n",
    "# ============================================\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    \"\"\"Set seeds for all RNG sources for reproducibility\"\"\"\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    try:\n",
    "        import cupy as cp\n",
    "        cp.random.seed(seed)\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        qml.numpy.random.seed(seed)\n",
    "    except (ImportError, AttributeError):\n",
    "        pass\n",
    "\n",
    "# ============================================\n",
    "# Quantum Head (WITH Entanglement)\n",
    "# ============================================\n",
    "\n",
    "class QuantumHeadEnt(nn.Module):\n",
    "    \"\"\"\n",
    "    VQC with 4 qubits, 3 layers, WITH CNOT ring entanglement → 3 classes\n",
    "    Uses Lightning acceleration (GPU)\n",
    "    Maps 512 features → 4 qubits\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=512, n_qubits=4, n_layers=3, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Map 512 features → n_qubits\n",
    "        self.feature_select = nn.Linear(input_dim, n_qubits)\n",
    "        \n",
    "        # Quantum device\n",
    "        self.dev = qml.device(QUANTUM_DEVICE, wires=n_qubits)\n",
    "        \n",
    "        # Use adjoint differentiation for lightning\n",
    "        @qml.qnode(self.dev, interface=\"torch\", diff_method=\"adjoint\")\n",
    "        def quantum_circuit(inputs, weights):\n",
    "            \"\"\"\n",
    "            Single-sample circuit WITH CNOT ring entanglement\n",
    "            inputs: (n_qubits,)\n",
    "            weights: (n_layers, n_qubits, 2)\n",
    "            \"\"\"\n",
    "            for layer in range(n_layers):\n",
    "                # Data re-uploading\n",
    "                for i in range(n_qubits):\n",
    "                    qml.RY(inputs[i], wires=i)\n",
    "                \n",
    "                # Trainable rotations\n",
    "                for i in range(n_qubits):\n",
    "                    qml.RY(weights[layer, i, 0], wires=i)\n",
    "                    qml.RZ(weights[layer, i, 1], wires=i)\n",
    "                \n",
    "                # CNOT ring: 0→1→2→3→0\n",
    "                for i in range(n_qubits):\n",
    "                    qml.CNOT(wires=[i, (i + 1) % n_qubits])\n",
    "            \n",
    "            # Measurements\n",
    "            measurements = []\n",
    "            # Single-qubit Z\n",
    "            for i in range(n_qubits):\n",
    "                measurements.append(qml.expval(qml.PauliZ(i)))\n",
    "            # Two-qubit ZZ (pairs)\n",
    "            for i in range(0, n_qubits-1, 2):\n",
    "                measurements.append(qml.expval(qml.PauliZ(i) @ qml.PauliZ(i+1)))\n",
    "            \n",
    "            return measurements\n",
    "        \n",
    "        self.quantum_circuit = quantum_circuit\n",
    "        \n",
    "        weight_shape = (n_layers, n_qubits, 2)\n",
    "        self.q_weights = nn.Parameter(torch.randn(weight_shape) * 0.1)\n",
    "        \n",
    "        # Output: 6 measurements → 3 classes\n",
    "        n_measurements = n_qubits + (n_qubits // 2)\n",
    "        self.fc_out = nn.Linear(n_measurements, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch, 512) features\n",
    "        Returns: (batch, 3) logits\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        x = torch.tanh(self.feature_select(x))\n",
    "        \n",
    "        # Process samples one by one\n",
    "        quantum_outputs = []\n",
    "        for i in range(batch_size):\n",
    "            q_raw = self.quantum_circuit(x[i], self.q_weights)\n",
    "            if isinstance(q_raw, (list, tuple)):\n",
    "                q_out = torch.stack(q_raw)\n",
    "            else:\n",
    "                q_out = q_raw\n",
    "            quantum_outputs.append(q_out)\n",
    "        \n",
    "        quantum_outputs = torch.stack(quantum_outputs).float()\n",
    "        quantum_outputs = quantum_outputs.to(self.fc_out.weight.device)\n",
    "        \n",
    "        output = self.fc_out(quantum_outputs)\n",
    "        return output\n",
    "\n",
    "# ============================================\n",
    "# Training and Evaluation\n",
    "# ============================================\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, device, show_progress=True):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (x, y) in enumerate(loader):\n",
    "        x = x.to(device)\n",
    "        # y stays on CPU for quantum model\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        \n",
    "        if show_progress and batch_idx % 20 == 0:\n",
    "            print(f\"    Batch {batch_idx}/{len(loader)}\", end=\"\\r\")\n",
    "    \n",
    "    if show_progress:\n",
    "        print()\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            logits = model(x)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += x.size(0)\n",
    "    \n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "def train_with_early_stopping(model, train_loader, test_loader, optimizer,\n",
    "                              device, max_epochs=200, patience=10, name=\"Model\"):\n",
    "    best_acc = 0.0\n",
    "    epochs_without_improvement = 0\n",
    "    start = time.time()\n",
    "    last_acc = 0.0\n",
    "    \n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        print(f\"  [{name}] Epoch {epoch}/{max_epochs}\")\n",
    "        loss = train_one_epoch(model, train_loader, optimizer, device, show_progress=True)\n",
    "        acc = evaluate(model, test_loader, device)\n",
    "        last_acc = acc\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        print(f\"  [{name}] Epoch {epoch:2d} | loss={loss:.4f} | \"\n",
    "              f\"test_acc={acc:.4f} | time={elapsed:.1f}s\")\n",
    "        \n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        \n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"  [{name}] Early stop at epoch {epoch} \"\n",
    "                  f\"(no improvement for {patience} epochs)\")\n",
    "            break\n",
    "    \n",
    "    total_time = time.time() - start\n",
    "    return {\n",
    "        \"best_acc\": best_acc,\n",
    "        \"final_acc\": last_acc,\n",
    "        \"time\": total_time,\n",
    "        \"epochs\": epoch,\n",
    "    }\n",
    "\n",
    "# ============================================\n",
    "# Main\n",
    "# ============================================\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"BLOCK 4: Quantum (WITH Entanglement) Training on QM9\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nConfiguration:\")\n",
    "    print(\"  • Dataset: QM9 HOMO-LUMO gap classification\")\n",
    "    print(\"  • Features: 512-D Morgan fingerprints (frozen)\")\n",
    "    print(\"  • Classes: 3 (Low/Med/High gap)\")\n",
    "    print(\"  • Architecture: 512 → 4 qubits (3 layers, CNOT ring) → 6 meas → 3\")\n",
    "    print(\"  • Batch size: 32 (train), 64 (test)\")\n",
    "    print(\"  • Max epochs: 200\")\n",
    "    print(\"  • Patience: 10\")\n",
    "    print(\"  • Seeds: [42, 123, 456]\")\n",
    "    print(f\"  • Quantum device: {QUANTUM_DEVICE}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Load preprocessed data\n",
    "    print(\"\\nLoading preprocessed QM9 data...\")\n",
    "    data = torch.load(\"qm9_preprocessed_512d.pt\", weights_only=False)\n",
    "    features = data['features']\n",
    "    labels = data['labels']\n",
    "    \n",
    "    print(f\"  Loaded {len(features)} molecules\")\n",
    "    print(f\"  Feature dim: {features.shape[1]}\")\n",
    "    \n",
    "    # Load train/test indices from Block 1\n",
    "    print(\"\\nLoading train/test split from Block 1...\")\n",
    "    realnet_data = torch.load(\"realnet_qm9_results.pt\")\n",
    "    train_indices = realnet_data['train_indices']\n",
    "    test_indices = realnet_data['test_indices']\n",
    "    \n",
    "    print(f\"  Train samples: {len(train_indices)}\")\n",
    "    print(f\"  Test samples: {len(test_indices)}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    full_dataset = TensorDataset(features, labels)\n",
    "    train_ds = Subset(full_dataset, train_indices)\n",
    "    test_ds = Subset(full_dataset, test_indices)\n",
    "    \n",
    "    # Smaller batches for quantum\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_ds, batch_size=64, shuffle=False, num_workers=0)\n",
    "    \n",
    "    seeds = [42, 123, 456]\n",
    "    all_results = []\n",
    "    \n",
    "    for seed in seeds:\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(f\"SEED {seed}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        set_all_seeds(seed)\n",
    "        \n",
    "        print(f\"\\n  Training QuantumNet (WITH entanglement, seed={seed})...\")\n",
    "        model = QuantumHeadEnt(input_dim=512, n_qubits=4, n_layers=3, num_classes=3)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "        \n",
    "        result = train_with_early_stopping(\n",
    "            model, train_loader, test_loader, optimizer, device,\n",
    "            max_epochs=200, patience=10, name=\"QuantumEnt\"\n",
    "        )\n",
    "        \n",
    "        trainable_params = sum(p.numel() for p in model.parameters())\n",
    "        result[\"trainable_params\"] = trainable_params\n",
    "        result[\"seed\"] = seed\n",
    "        \n",
    "        all_results.append(result)\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"QUANTUM (WITH ENTANGLEMENT) QM9 SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    accs = [r[\"best_acc\"] for r in all_results]\n",
    "    times = [r[\"time\"] for r in all_results]\n",
    "    epochs = [r[\"epochs\"] for r in all_results]\n",
    "    params = all_results[0][\"trainable_params\"]\n",
    "    \n",
    "    print(f\"\\nAccuracy:     {np.mean(accs):.4f} ± {np.std(accs):.4f}\")\n",
    "    print(f\"Time:         {np.mean(times):.1f}s ± {np.std(times):.1f}s\")\n",
    "    print(f\"Epochs:       {np.mean(epochs):.1f} ± {np.std(epochs):.1f}\")\n",
    "    print(f\"Parameters:   {params:,}\")\n",
    "    \n",
    "    print(\"\\nPer-seed results:\")\n",
    "    for r in all_results:\n",
    "        print(f\"  Seed {r['seed']}: acc={r['best_acc']:.4f}, \"\n",
    "              f\"time={r['time']:.1f}s, epochs={r['epochs']}\")\n",
    "    \n",
    "    # Save results\n",
    "    save_dict = {\n",
    "        \"results\": all_results,\n",
    "        \"summary\": {\n",
    "            \"mean_acc\": np.mean(accs),\n",
    "            \"std_acc\": np.std(accs),\n",
    "            \"mean_time\": np.mean(times),\n",
    "            \"trainable_params\": params\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    torch.save(save_dict, \"quantum_ent_qm9_results.pt\")\n",
    "    print(f\"\\n✓ Saved results to: quantum_ent_qm9_results.pt\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
