{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9246b37d-b5b6-4822-b55f-18ef530a85a4",
   "metadata": {},
   "source": [
    "# RealNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2af5b4d-d255-4b56-acab-b0f7fef21d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch device: cuda\n",
      "======================================================================\n",
      "BLOCK 1: RealNet Training\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "  • Batch size: 32\n",
      "  • Patience: 10\n",
      "  • Seeds: [42, 123, 456]\n",
      "  • Architecture: 784 → 16 → 64 → 10\n",
      "======================================================================\n",
      "\n",
      "Creating stratified samples...\n",
      "  Train samples: 15000 (stratified, 1500 per class)\n",
      "  Test samples:  3000 (stratified, 300 per class)\n",
      "\n",
      "======================================================================\n",
      "SEED 42\n",
      "======================================================================\n",
      "\n",
      "  Training RealNet (seed=42)...\n",
      "  [Real] Epoch  1 | loss=0.7188 | test_acc=0.8977 | time=1.5s\n",
      "  [Real] Epoch  2 | loss=0.2981 | test_acc=0.9090 | time=2.9s\n",
      "  [Real] Epoch  3 | loss=0.2502 | test_acc=0.9157 | time=4.3s\n",
      "  [Real] Epoch  4 | loss=0.2143 | test_acc=0.9173 | time=5.8s\n",
      "  [Real] Epoch  5 | loss=0.1910 | test_acc=0.9293 | time=7.2s\n",
      "  [Real] Epoch  6 | loss=0.1768 | test_acc=0.9210 | time=8.7s\n",
      "  [Real] Epoch  7 | loss=0.1631 | test_acc=0.9253 | time=10.2s\n",
      "  [Real] Epoch  8 | loss=0.1486 | test_acc=0.9290 | time=11.6s\n",
      "  [Real] Epoch  9 | loss=0.1428 | test_acc=0.9260 | time=13.1s\n",
      "  [Real] Epoch 10 | loss=0.1289 | test_acc=0.9250 | time=14.5s\n",
      "  [Real] Epoch 11 | loss=0.1239 | test_acc=0.9237 | time=15.9s\n",
      "  [Real] Epoch 12 | loss=0.1204 | test_acc=0.9270 | time=17.4s\n",
      "  [Real] Epoch 13 | loss=0.1106 | test_acc=0.9193 | time=18.8s\n",
      "  [Real] Epoch 14 | loss=0.1045 | test_acc=0.9237 | time=20.3s\n",
      "  [Real] Epoch 15 | loss=0.1019 | test_acc=0.9267 | time=21.7s\n",
      "  [Real] Early stop at epoch 15 (no improvement for 10 epochs)\n",
      "\n",
      "  → Saved preprocessor state from seed 42\n",
      "\n",
      "======================================================================\n",
      "SEED 123\n",
      "======================================================================\n",
      "\n",
      "  Training RealNet (seed=123)...\n",
      "  [Real] Epoch  1 | loss=0.7262 | test_acc=0.9087 | time=1.4s\n",
      "  [Real] Epoch  2 | loss=0.2878 | test_acc=0.9247 | time=2.9s\n",
      "  [Real] Epoch  3 | loss=0.2327 | test_acc=0.9323 | time=4.2s\n",
      "  [Real] Epoch  4 | loss=0.2044 | test_acc=0.9287 | time=5.6s\n",
      "  [Real] Epoch  5 | loss=0.1806 | test_acc=0.9290 | time=6.9s\n",
      "  [Real] Epoch  6 | loss=0.1628 | test_acc=0.9370 | time=8.2s\n",
      "  [Real] Epoch  7 | loss=0.1496 | test_acc=0.9337 | time=9.5s\n",
      "  [Real] Epoch  8 | loss=0.1420 | test_acc=0.9387 | time=10.8s\n",
      "  [Real] Epoch  9 | loss=0.1306 | test_acc=0.9337 | time=12.2s\n",
      "  [Real] Epoch 10 | loss=0.1194 | test_acc=0.9333 | time=13.6s\n",
      "  [Real] Epoch 11 | loss=0.1158 | test_acc=0.9313 | time=15.0s\n",
      "  [Real] Epoch 12 | loss=0.1122 | test_acc=0.9347 | time=16.4s\n",
      "  [Real] Epoch 13 | loss=0.1006 | test_acc=0.9360 | time=17.7s\n",
      "  [Real] Epoch 14 | loss=0.0952 | test_acc=0.9420 | time=19.1s\n",
      "  [Real] Epoch 15 | loss=0.0905 | test_acc=0.9273 | time=20.5s\n",
      "  [Real] Epoch 16 | loss=0.0869 | test_acc=0.9327 | time=21.8s\n",
      "  [Real] Epoch 17 | loss=0.0821 | test_acc=0.9387 | time=23.2s\n",
      "  [Real] Epoch 18 | loss=0.0840 | test_acc=0.9387 | time=24.7s\n",
      "  [Real] Epoch 19 | loss=0.0712 | test_acc=0.9330 | time=26.1s\n",
      "  [Real] Epoch 20 | loss=0.0794 | test_acc=0.9343 | time=27.4s\n",
      "  [Real] Epoch 21 | loss=0.0744 | test_acc=0.9363 | time=28.8s\n",
      "  [Real] Epoch 22 | loss=0.0712 | test_acc=0.9357 | time=30.2s\n",
      "  [Real] Epoch 23 | loss=0.0612 | test_acc=0.9367 | time=31.7s\n",
      "  [Real] Epoch 24 | loss=0.0595 | test_acc=0.9273 | time=33.1s\n",
      "  [Real] Early stop at epoch 24 (no improvement for 10 epochs)\n",
      "\n",
      "======================================================================\n",
      "SEED 456\n",
      "======================================================================\n",
      "\n",
      "  Training RealNet (seed=456)...\n",
      "  [Real] Epoch  1 | loss=0.7188 | test_acc=0.9053 | time=1.4s\n",
      "  [Real] Epoch  2 | loss=0.3021 | test_acc=0.9193 | time=2.8s\n",
      "  [Real] Epoch  3 | loss=0.2458 | test_acc=0.9263 | time=4.3s\n",
      "  [Real] Epoch  4 | loss=0.2096 | test_acc=0.9207 | time=5.7s\n",
      "  [Real] Epoch  5 | loss=0.1902 | test_acc=0.9230 | time=7.1s\n",
      "  [Real] Epoch  6 | loss=0.1754 | test_acc=0.9287 | time=8.5s\n",
      "  [Real] Epoch  7 | loss=0.1599 | test_acc=0.9293 | time=9.9s\n",
      "  [Real] Epoch  8 | loss=0.1456 | test_acc=0.9327 | time=11.3s\n",
      "  [Real] Epoch  9 | loss=0.1389 | test_acc=0.9240 | time=12.8s\n",
      "  [Real] Epoch 10 | loss=0.1278 | test_acc=0.9267 | time=14.2s\n",
      "  [Real] Epoch 11 | loss=0.1196 | test_acc=0.9240 | time=15.6s\n",
      "  [Real] Epoch 12 | loss=0.1129 | test_acc=0.9267 | time=17.0s\n",
      "  [Real] Epoch 13 | loss=0.1103 | test_acc=0.9233 | time=18.5s\n",
      "  [Real] Epoch 14 | loss=0.1083 | test_acc=0.9247 | time=19.9s\n",
      "  [Real] Epoch 15 | loss=0.1050 | test_acc=0.9350 | time=21.4s\n",
      "  [Real] Epoch 16 | loss=0.0894 | test_acc=0.9340 | time=22.8s\n",
      "  [Real] Epoch 17 | loss=0.0888 | test_acc=0.9263 | time=24.2s\n",
      "  [Real] Epoch 18 | loss=0.0921 | test_acc=0.9317 | time=25.8s\n",
      "  [Real] Epoch 19 | loss=0.0816 | test_acc=0.9270 | time=27.2s\n",
      "  [Real] Epoch 20 | loss=0.0793 | test_acc=0.9347 | time=28.6s\n",
      "  [Real] Epoch 21 | loss=0.0739 | test_acc=0.9277 | time=30.0s\n",
      "  [Real] Epoch 22 | loss=0.0765 | test_acc=0.9317 | time=31.3s\n",
      "  [Real] Epoch 23 | loss=0.0713 | test_acc=0.9250 | time=32.7s\n",
      "  [Real] Epoch 24 | loss=0.0655 | test_acc=0.9290 | time=34.0s\n",
      "  [Real] Epoch 25 | loss=0.0633 | test_acc=0.9297 | time=35.3s\n",
      "  [Real] Early stop at epoch 25 (no improvement for 10 epochs)\n",
      "\n",
      "======================================================================\n",
      "REALNET SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Accuracy:   0.9354 ± 0.0052\n",
      "Time:       30.0s ± 5.9s\n",
      "Epochs:     21.3 ± 4.5\n",
      "Parameters: 14,298\n",
      "\n",
      "Per-seed results:\n",
      "  Seed 42: acc=0.9293, time=21.7s, epochs=15\n",
      "  Seed 123: acc=0.9420, time=33.1s, epochs=24\n",
      "  Seed 456: acc=0.9350, time=35.3s, epochs=25\n",
      "\n",
      "✓ Saved results to: realnet_results.pt\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Block 1: RealNet Training\n",
    "==========================\n",
    "Trains the baseline Real MLP on 3 seeds.\n",
    "Saves the trained preprocessor for reuse in subsequent blocks.\n",
    "\n",
    "Outputs:\n",
    "- realnet_results.pt: Contains results dict and trained preprocessor state\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from collections import defaultdict\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "print(f\"Using PyTorch device: {device}\")\n",
    "\n",
    "# ============================================\n",
    "# Comprehensive seed setting for reproducibility\n",
    "# ============================================\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    \"\"\"Set seeds for all RNG sources for reproducibility\"\"\"\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    # Python\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # NumPy\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # PyTorch deterministic mode\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # CuPy (if available)\n",
    "    try:\n",
    "        import cupy as cp\n",
    "        cp.random.seed(seed)\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    # PennyLane (if available)\n",
    "    try:\n",
    "        import pennylane as qml\n",
    "        qml.numpy.random.seed(seed)\n",
    "    except (ImportError, AttributeError):\n",
    "        pass\n",
    "\n",
    "# ============================================\n",
    "# Shared Bottleneck Preprocessor: 784 → 16\n",
    "# ============================================\n",
    "\n",
    "class SharedPreprocessor(nn.Module):\n",
    "    \"\"\"Shared classical feature extractor: 784 → 16\"\"\"\n",
    "    def __init__(self, input_dim=784, bottleneck_dim=16):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, bottleneck_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.tanh(self.fc(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Real-valued Head and Network\n",
    "# ============================================\n",
    "\n",
    "class RealHead(nn.Module):\n",
    "    \"\"\"Standard MLP: 16 → 64 → 10\"\"\"\n",
    "    def __init__(self, bottleneck_dim=16, hidden_dim=64, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(bottleneck_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RealNet(nn.Module):\n",
    "    \"\"\"Complete Real network: Preprocessor + RealHead\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.preprocessor = SharedPreprocessor(784, 16)\n",
    "        self.head = RealHead(16, 64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.preprocessor(x)\n",
    "        return self.head(features)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Training and Evaluation\n",
    "# ============================================\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, device, show_progress=False):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        total_samples += x.size(0)\n",
    "\n",
    "        if show_progress and batch_idx % 50 == 0:\n",
    "            print(f\"    Batch {batch_idx}/{len(loader)}, samples: {total_samples}\", end=\"\\r\")\n",
    "\n",
    "    if show_progress:\n",
    "        print()\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += x.size(0)\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "def train_with_early_stopping(model, train_loader, test_loader, optimizer,\n",
    "                              device, max_epochs=40, patience=10, name=\"Model\"):\n",
    "    if isinstance(device, str):\n",
    "        device = torch.device(device)\n",
    "\n",
    "    best_acc = 0.0\n",
    "    epochs_without_improvement = 0\n",
    "    start = time.time()\n",
    "    last_acc = 0.0\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        loss = train_one_epoch(model, train_loader, optimizer, device, show_progress=False)\n",
    "        acc = evaluate(model, test_loader, device)\n",
    "        last_acc = acc\n",
    "\n",
    "        elapsed = time.time() - start\n",
    "        print(f\"  [{name}] Epoch {epoch:2d} | loss={loss:.4f} \"\n",
    "              f\"| test_acc={acc:.4f} | time={elapsed:.1f}s\")\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"  [{name}] Early stop at epoch {epoch} \"\n",
    "                  f\"(no improvement for {patience} epochs)\")\n",
    "            break\n",
    "\n",
    "    total_time = time.time() - start\n",
    "    return {\n",
    "        \"best_acc\": best_acc,\n",
    "        \"final_acc\": last_acc,\n",
    "        \"time\": total_time,\n",
    "        \"epochs\": epoch,\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Data utilities\n",
    "# ============================================\n",
    "\n",
    "def stratified_sample(dataset, n_samples_per_class):\n",
    "    \"\"\"\n",
    "    Create a stratified sample with n_samples_per_class from each class.\n",
    "    Maintains class balance.\n",
    "    \"\"\"\n",
    "    # Group indices by class\n",
    "    class_indices = defaultdict(list)\n",
    "    for idx, (_, label) in enumerate(dataset):\n",
    "        class_indices[label].append(idx)\n",
    "    \n",
    "    # Sample from each class\n",
    "    sampled_indices = []\n",
    "    for class_label in sorted(class_indices.keys()):\n",
    "        indices = class_indices[class_label]\n",
    "        # Use fixed random seed for reproducibility\n",
    "        rng = np.random.RandomState(42)\n",
    "        selected = rng.choice(indices, size=min(n_samples_per_class, len(indices)), \n",
    "                             replace=False)\n",
    "        sampled_indices.extend(selected)\n",
    "    \n",
    "    return sampled_indices\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Main\n",
    "# ============================================\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"BLOCK 1: RealNet Training\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nConfiguration:\")\n",
    "    print(\"  • Batch size: 32\")\n",
    "    print(\"  • Patience: 10\")\n",
    "    print(\"  • Seeds: [42, 123, 456]\")\n",
    "    print(\"  • Architecture: 784 → 16 → 64 → 10\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    # Load full datasets\n",
    "    full_train_ds = datasets.MNIST(root=\"./data\", train=True,\n",
    "                                   download=True, transform=transform)\n",
    "    full_test_ds = datasets.MNIST(root=\"./data\", train=False,\n",
    "                                  download=True, transform=transform)\n",
    "\n",
    "    # Create stratified samples\n",
    "    print(\"\\nCreating stratified samples...\")\n",
    "    train_indices = stratified_sample(full_train_ds, n_samples_per_class=1500)  # 15K total\n",
    "    test_indices = stratified_sample(full_test_ds, n_samples_per_class=300)     # 3K total\n",
    "    \n",
    "    train_ds = Subset(full_train_ds, train_indices)\n",
    "    test_ds = Subset(full_test_ds, test_indices)\n",
    "    \n",
    "    print(f\"  Train samples: {len(train_ds)} (stratified, 1500 per class)\")\n",
    "    print(f\"  Test samples:  {len(test_ds)} (stratified, 300 per class)\")\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=32,\n",
    "                              shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_ds, batch_size=64,\n",
    "                             shuffle=False, num_workers=0)\n",
    "\n",
    "    seeds = [42, 123, 456]\n",
    "    all_results = []\n",
    "    trained_preprocessor_state = None\n",
    "\n",
    "    for seed in seeds:\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(f\"SEED {seed}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        set_all_seeds(seed)\n",
    "\n",
    "        print(f\"\\n  Training RealNet (seed={seed})...\")\n",
    "        real_model = RealNet().to(device)\n",
    "        real_opt = torch.optim.Adam(real_model.parameters(), lr=1e-3)\n",
    "        \n",
    "        result = train_with_early_stopping(\n",
    "            real_model, train_loader, test_loader, real_opt, device,\n",
    "            max_epochs=40, patience=10, name=\"Real\"\n",
    "        )\n",
    "        result[\"params\"] = sum(p.numel() for p in real_model.parameters())\n",
    "        result[\"seed\"] = seed\n",
    "        \n",
    "        all_results.append(result)\n",
    "        \n",
    "        # Save preprocessor from first seed\n",
    "        if trained_preprocessor_state is None:\n",
    "            trained_preprocessor_state = real_model.preprocessor.state_dict()\n",
    "            print(f\"\\n  → Saved preprocessor state from seed {seed}\")\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"REALNET SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    accs = [r[\"best_acc\"] for r in all_results]\n",
    "    times = [r[\"time\"] for r in all_results]\n",
    "    epochs = [r[\"epochs\"] for r in all_results]\n",
    "    params = all_results[0][\"params\"]\n",
    "    \n",
    "    print(f\"\\nAccuracy:   {np.mean(accs):.4f} ± {np.std(accs):.4f}\")\n",
    "    print(f\"Time:       {np.mean(times):.1f}s ± {np.std(times):.1f}s\")\n",
    "    print(f\"Epochs:     {np.mean(epochs):.1f} ± {np.std(epochs):.1f}\")\n",
    "    print(f\"Parameters: {params:,}\")\n",
    "    \n",
    "    print(\"\\nPer-seed results:\")\n",
    "    for r in all_results:\n",
    "        print(f\"  Seed {r['seed']}: acc={r['best_acc']:.4f}, \"\n",
    "              f\"time={r['time']:.1f}s, epochs={r['epochs']}\")\n",
    "\n",
    "    # Save results and preprocessor\n",
    "    save_dict = {\n",
    "        \"results\": all_results,\n",
    "        \"preprocessor_state\": trained_preprocessor_state,\n",
    "        \"summary\": {\n",
    "            \"mean_acc\": np.mean(accs),\n",
    "            \"std_acc\": np.std(accs),\n",
    "            \"mean_time\": np.mean(times),\n",
    "            \"params\": params\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    torch.save(save_dict, \"realnet_results.pt\")\n",
    "    print(f\"\\n✓ Saved results to: realnet_results.pt\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d988cd26-7705-40d2-bae3-dce89859cca5",
   "metadata": {},
   "source": [
    "# QuatNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1b2faff-b01a-4e50-9b40-093b32374e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch device: cuda\n",
      "======================================================================\n",
      "BLOCK 2: QuatNet Training (Frozen Preprocessor)\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "  • Batch size: 32\n",
      "  • Patience: 10\n",
      "  • Seeds: [42, 123, 456]\n",
      "  • Architecture: [frozen 784→16] → 4 quats → 16 quats → 10\n",
      "======================================================================\n",
      "\n",
      "Loading frozen preprocessor from Block 1...\n",
      "✓ Loaded preprocessor state from realnet_results.pt\n",
      "  Preprocessor frozen with 12,560 params\n",
      "\n",
      "Creating stratified samples...\n",
      "  Train samples: 15000 (stratified, 1500 per class)\n",
      "  Test samples:  3000 (stratified, 300 per class)\n",
      "\n",
      "======================================================================\n",
      "SEED 42\n",
      "======================================================================\n",
      "\n",
      "  Training QuatNet (seed=42)...\n",
      "  [Quat] Epoch  1 | loss=1.3729 | test_acc=0.8883 | time=2.6s\n",
      "  [Quat] Epoch  2 | loss=0.3108 | test_acc=0.9183 | time=5.1s\n",
      "  [Quat] Epoch  3 | loss=0.1967 | test_acc=0.9273 | time=7.3s\n",
      "  [Quat] Epoch  4 | loss=0.1507 | test_acc=0.9293 | time=9.6s\n",
      "  [Quat] Epoch  5 | loss=0.1253 | test_acc=0.9307 | time=12.0s\n",
      "  [Quat] Epoch  6 | loss=0.1094 | test_acc=0.9320 | time=14.5s\n",
      "  [Quat] Epoch  7 | loss=0.0986 | test_acc=0.9330 | time=16.9s\n",
      "  [Quat] Epoch  8 | loss=0.0908 | test_acc=0.9337 | time=19.3s\n",
      "  [Quat] Epoch  9 | loss=0.0850 | test_acc=0.9337 | time=21.7s\n",
      "  [Quat] Epoch 10 | loss=0.0804 | test_acc=0.9333 | time=24.0s\n",
      "  [Quat] Epoch 11 | loss=0.0766 | test_acc=0.9350 | time=26.4s\n",
      "  [Quat] Epoch 12 | loss=0.0732 | test_acc=0.9357 | time=29.2s\n",
      "  [Quat] Epoch 13 | loss=0.0705 | test_acc=0.9353 | time=31.9s\n",
      "  [Quat] Epoch 14 | loss=0.0681 | test_acc=0.9350 | time=34.5s\n",
      "  [Quat] Epoch 15 | loss=0.0661 | test_acc=0.9360 | time=37.2s\n",
      "  [Quat] Epoch 16 | loss=0.0642 | test_acc=0.9353 | time=39.9s\n",
      "  [Quat] Epoch 17 | loss=0.0627 | test_acc=0.9353 | time=42.2s\n",
      "  [Quat] Epoch 18 | loss=0.0610 | test_acc=0.9367 | time=44.6s\n",
      "  [Quat] Epoch 19 | loss=0.0601 | test_acc=0.9360 | time=47.0s\n",
      "  [Quat] Epoch 20 | loss=0.0589 | test_acc=0.9383 | time=49.4s\n",
      "  [Quat] Epoch 21 | loss=0.0577 | test_acc=0.9363 | time=51.8s\n",
      "  [Quat] Epoch 22 | loss=0.0568 | test_acc=0.9367 | time=54.3s\n",
      "  [Quat] Epoch 23 | loss=0.0560 | test_acc=0.9343 | time=56.7s\n",
      "  [Quat] Epoch 24 | loss=0.0549 | test_acc=0.9347 | time=59.3s\n",
      "  [Quat] Epoch 25 | loss=0.0541 | test_acc=0.9363 | time=62.0s\n",
      "  [Quat] Epoch 26 | loss=0.0534 | test_acc=0.9330 | time=64.5s\n",
      "  [Quat] Epoch 27 | loss=0.0529 | test_acc=0.9347 | time=67.0s\n",
      "  [Quat] Epoch 28 | loss=0.0522 | test_acc=0.9343 | time=69.4s\n",
      "  [Quat] Epoch 29 | loss=0.0515 | test_acc=0.9367 | time=72.0s\n",
      "  [Quat] Epoch 30 | loss=0.0509 | test_acc=0.9347 | time=74.6s\n",
      "  [Quat] Early stop at epoch 30 (no improvement for 10 epochs)\n",
      "\n",
      "======================================================================\n",
      "SEED 123\n",
      "======================================================================\n",
      "\n",
      "  Training QuatNet (seed=123)...\n",
      "  [Quat] Epoch  1 | loss=1.0984 | test_acc=0.8853 | time=2.5s\n",
      "  [Quat] Epoch  2 | loss=0.2755 | test_acc=0.9130 | time=4.9s\n",
      "  [Quat] Epoch  3 | loss=0.1787 | test_acc=0.9227 | time=7.4s\n",
      "  [Quat] Epoch  4 | loss=0.1404 | test_acc=0.9280 | time=9.9s\n",
      "  [Quat] Epoch  5 | loss=0.1192 | test_acc=0.9300 | time=12.5s\n",
      "  [Quat] Epoch  6 | loss=0.1057 | test_acc=0.9303 | time=15.0s\n",
      "  [Quat] Epoch  7 | loss=0.0961 | test_acc=0.9320 | time=17.5s\n",
      "  [Quat] Epoch  8 | loss=0.0889 | test_acc=0.9313 | time=19.9s\n",
      "  [Quat] Epoch  9 | loss=0.0837 | test_acc=0.9320 | time=22.5s\n",
      "  [Quat] Epoch 10 | loss=0.0793 | test_acc=0.9320 | time=24.8s\n",
      "  [Quat] Epoch 11 | loss=0.0758 | test_acc=0.9320 | time=27.2s\n",
      "  [Quat] Epoch 12 | loss=0.0727 | test_acc=0.9310 | time=29.8s\n",
      "  [Quat] Epoch 13 | loss=0.0704 | test_acc=0.9310 | time=32.3s\n",
      "  [Quat] Epoch 14 | loss=0.0682 | test_acc=0.9333 | time=34.7s\n",
      "  [Quat] Epoch 15 | loss=0.0663 | test_acc=0.9320 | time=37.3s\n",
      "  [Quat] Epoch 16 | loss=0.0648 | test_acc=0.9343 | time=39.7s\n",
      "  [Quat] Epoch 17 | loss=0.0634 | test_acc=0.9313 | time=42.3s\n",
      "  [Quat] Epoch 18 | loss=0.0620 | test_acc=0.9327 | time=44.8s\n",
      "  [Quat] Epoch 19 | loss=0.0609 | test_acc=0.9323 | time=47.2s\n",
      "  [Quat] Epoch 20 | loss=0.0597 | test_acc=0.9333 | time=49.7s\n",
      "  [Quat] Epoch 21 | loss=0.0588 | test_acc=0.9323 | time=52.0s\n",
      "  [Quat] Epoch 22 | loss=0.0579 | test_acc=0.9323 | time=54.5s\n",
      "  [Quat] Epoch 23 | loss=0.0569 | test_acc=0.9330 | time=57.0s\n",
      "  [Quat] Epoch 24 | loss=0.0562 | test_acc=0.9337 | time=59.3s\n",
      "  [Quat] Epoch 25 | loss=0.0556 | test_acc=0.9347 | time=61.7s\n",
      "  [Quat] Epoch 26 | loss=0.0550 | test_acc=0.9323 | time=64.3s\n",
      "  [Quat] Epoch 27 | loss=0.0543 | test_acc=0.9323 | time=66.9s\n",
      "  [Quat] Epoch 28 | loss=0.0537 | test_acc=0.9333 | time=69.5s\n",
      "  [Quat] Epoch 29 | loss=0.0530 | test_acc=0.9343 | time=71.9s\n",
      "  [Quat] Epoch 30 | loss=0.0525 | test_acc=0.9340 | time=74.3s\n",
      "  [Quat] Epoch 31 | loss=0.0519 | test_acc=0.9343 | time=76.7s\n",
      "  [Quat] Epoch 32 | loss=0.0514 | test_acc=0.9333 | time=79.0s\n",
      "  [Quat] Epoch 33 | loss=0.0509 | test_acc=0.9347 | time=81.5s\n",
      "  [Quat] Epoch 34 | loss=0.0506 | test_acc=0.9340 | time=83.9s\n",
      "  [Quat] Epoch 35 | loss=0.0501 | test_acc=0.9327 | time=86.4s\n",
      "  [Quat] Early stop at epoch 35 (no improvement for 10 epochs)\n",
      "\n",
      "======================================================================\n",
      "SEED 456\n",
      "======================================================================\n",
      "\n",
      "  Training QuatNet (seed=456)...\n",
      "  [Quat] Epoch  1 | loss=1.0579 | test_acc=0.8973 | time=2.5s\n",
      "  [Quat] Epoch  2 | loss=0.2724 | test_acc=0.9217 | time=4.9s\n",
      "  [Quat] Epoch  3 | loss=0.1785 | test_acc=0.9300 | time=7.4s\n",
      "  [Quat] Epoch  4 | loss=0.1403 | test_acc=0.9303 | time=9.8s\n",
      "  [Quat] Epoch  5 | loss=0.1190 | test_acc=0.9333 | time=12.2s\n",
      "  [Quat] Epoch  6 | loss=0.1058 | test_acc=0.9320 | time=14.5s\n",
      "  [Quat] Epoch  7 | loss=0.0964 | test_acc=0.9320 | time=16.8s\n",
      "  [Quat] Epoch  8 | loss=0.0896 | test_acc=0.9327 | time=19.1s\n",
      "  [Quat] Epoch  9 | loss=0.0844 | test_acc=0.9323 | time=21.3s\n",
      "  [Quat] Epoch 10 | loss=0.0801 | test_acc=0.9333 | time=23.7s\n",
      "  [Quat] Epoch 11 | loss=0.0765 | test_acc=0.9340 | time=26.0s\n",
      "  [Quat] Epoch 12 | loss=0.0738 | test_acc=0.9333 | time=28.2s\n",
      "  [Quat] Epoch 13 | loss=0.0712 | test_acc=0.9357 | time=30.5s\n",
      "  [Quat] Epoch 14 | loss=0.0691 | test_acc=0.9340 | time=33.0s\n",
      "  [Quat] Epoch 15 | loss=0.0673 | test_acc=0.9340 | time=35.4s\n",
      "  [Quat] Epoch 16 | loss=0.0656 | test_acc=0.9347 | time=38.0s\n",
      "  [Quat] Epoch 17 | loss=0.0640 | test_acc=0.9330 | time=40.5s\n",
      "  [Quat] Epoch 18 | loss=0.0629 | test_acc=0.9343 | time=43.0s\n",
      "  [Quat] Epoch 19 | loss=0.0616 | test_acc=0.9333 | time=45.2s\n",
      "  [Quat] Epoch 20 | loss=0.0606 | test_acc=0.9363 | time=47.4s\n",
      "  [Quat] Epoch 21 | loss=0.0595 | test_acc=0.9353 | time=49.6s\n",
      "  [Quat] Epoch 22 | loss=0.0585 | test_acc=0.9340 | time=52.2s\n",
      "  [Quat] Epoch 23 | loss=0.0577 | test_acc=0.9350 | time=54.8s\n",
      "  [Quat] Epoch 24 | loss=0.0568 | test_acc=0.9347 | time=57.2s\n",
      "  [Quat] Epoch 25 | loss=0.0562 | test_acc=0.9353 | time=59.5s\n",
      "  [Quat] Epoch 26 | loss=0.0553 | test_acc=0.9327 | time=61.8s\n",
      "  [Quat] Epoch 27 | loss=0.0548 | test_acc=0.9357 | time=64.2s\n",
      "  [Quat] Epoch 28 | loss=0.0540 | test_acc=0.9347 | time=66.7s\n",
      "  [Quat] Epoch 29 | loss=0.0535 | test_acc=0.9350 | time=69.0s\n",
      "  [Quat] Epoch 30 | loss=0.0529 | test_acc=0.9350 | time=71.4s\n",
      "  [Quat] Early stop at epoch 30 (no improvement for 10 epochs)\n",
      "\n",
      "======================================================================\n",
      "QUATNET SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Accuracy:   0.9364 ± 0.0015\n",
      "Time:       77.4s ± 6.5s\n",
      "Epochs:     31.7 ± 2.4\n",
      "Parameters: 1,000 trainable (head), 13,560 total\n",
      "\n",
      "Per-seed results:\n",
      "  Seed 42: acc=0.9383, time=74.6s, epochs=30\n",
      "  Seed 123: acc=0.9347, time=86.4s, epochs=35\n",
      "  Seed 456: acc=0.9363, time=71.4s, epochs=30\n",
      "\n",
      "✓ Saved results to: quatnet_results.pt\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Block 2: QuatNet Training\n",
    "==========================\n",
    "Loads frozen preprocessor from Block 1 and trains quaternion head on 3 seeds.\n",
    "\n",
    "Requirements:\n",
    "- realnet_results.pt (from Block 1)\n",
    "\n",
    "Outputs:\n",
    "- quatnet_results.pt: Contains quaternion head results\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from collections import defaultdict\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "print(f\"Using PyTorch device: {device}\")\n",
    "\n",
    "# ============================================\n",
    "# Comprehensive seed setting for reproducibility\n",
    "# ============================================\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    \"\"\"Set seeds for all RNG sources for reproducibility\"\"\"\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    # Python\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # NumPy\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # PyTorch deterministic mode\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # CuPy (if available)\n",
    "    try:\n",
    "        import cupy as cp\n",
    "        cp.random.seed(seed)\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    # PennyLane (if available)\n",
    "    try:\n",
    "        import pennylane as qml\n",
    "        qml.numpy.random.seed(seed)\n",
    "    except (ImportError, AttributeError):\n",
    "        pass\n",
    "\n",
    "# ============================================\n",
    "# Quaternion utilities (PyTorch tensors)\n",
    "# ============================================\n",
    "\n",
    "def q_normalize(q):\n",
    "    norm = torch.linalg.norm(q, dim=-1, keepdim=True) + 1e-8\n",
    "    return q / norm\n",
    "\n",
    "def q_conj(q):\n",
    "    w, x, y, z = torch.unbind(q, dim=-1)\n",
    "    return torch.stack([w, -x, -y, -z], dim=-1)\n",
    "\n",
    "def q_mul(a, b):\n",
    "    \"\"\"Hamilton product of two quaternions\"\"\"\n",
    "    aw, ax, ay, az = torch.unbind(a, dim=-1)\n",
    "    bw, bx, by, bz = torch.unbind(b, dim=-1)\n",
    "\n",
    "    w = aw * bw - ax * bx - ay * by - az * bz\n",
    "    x = aw * bx + ax * bw + ay * bz - az * by\n",
    "    y = aw * by - ax * bz + ay * bw + az * bx\n",
    "    z = aw * bz + ax * by - ay * bx + az * bw\n",
    "\n",
    "    return torch.stack([w, x, y, z], dim=-1)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Shared Preprocessor (must match Block 1)\n",
    "# ============================================\n",
    "\n",
    "class SharedPreprocessor(nn.Module):\n",
    "    \"\"\"Shared classical feature extractor: 784 → 16\"\"\"\n",
    "    def __init__(self, input_dim=784, bottleneck_dim=16):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, bottleneck_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.tanh(self.fc(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Quaternion Head and Network\n",
    "# ============================================\n",
    "\n",
    "class QuaternionLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        \"\"\"\n",
    "        in_features, out_features are in \"quaternion units\".\n",
    "        Internally weight: (out_features, in_features, 4)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.empty(out_features, in_features, 4))\n",
    "        self.bias = nn.Parameter(torch.empty(out_features, 4))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.normal_(self.weight, mean=0.0, std=0.1)\n",
    "        with torch.no_grad():\n",
    "            self.weight[:] = q_normalize(self.weight)\n",
    "            nn.init.constant_(self.bias[..., 0], 1.0)\n",
    "            nn.init.constant_(self.bias[..., 1:], 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, in_features, 4)\n",
    "        Returns: (B, out_features, 4)\n",
    "        \"\"\"\n",
    "        w = self.weight.unsqueeze(0)\n",
    "        x_exp = x.unsqueeze(1)\n",
    "        prod = q_mul(w, x_exp)\n",
    "        out = prod.sum(dim=2) + self.bias\n",
    "        return out\n",
    "\n",
    "\n",
    "class QuatHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Quaternion head: 4 quats → 16 quats → 10 quats → 10 logits.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.quat_fc1 = QuaternionLinear(4, 16)\n",
    "        self.quat_fc2 = QuaternionLinear(16, num_classes)\n",
    "\n",
    "    def real_to_quat(self, x):\n",
    "        \"\"\"Convert 16 real features to 4 quaternions\"\"\"\n",
    "        B = x.size(0)\n",
    "        return x.view(B, 4, 4)\n",
    "\n",
    "    def quat_to_real(self, q):\n",
    "        \"\"\"Extract real part of quaternions for classification\"\"\"\n",
    "        return q[..., 0]\n",
    "\n",
    "    def forward(self, x):\n",
    "        q_in = self.real_to_quat(x)\n",
    "        hq = self.quat_fc1(q_in)\n",
    "        hq = q_normalize(hq)\n",
    "        hq = torch.tanh(hq)\n",
    "        q_out = self.quat_fc2(hq)\n",
    "        logits = self.quat_to_real(q_out)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class QuatNet(nn.Module):\n",
    "    \"\"\"Complete Quaternion network: Preprocessor + QuatHead\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.preprocessor = SharedPreprocessor(784, 16)\n",
    "        self.head = QuatHead(10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.preprocessor(x)\n",
    "        return self.head(features)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Training and Evaluation\n",
    "# ============================================\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, device, show_progress=False):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        total_samples += x.size(0)\n",
    "\n",
    "        if show_progress and batch_idx % 50 == 0:\n",
    "            print(f\"    Batch {batch_idx}/{len(loader)}, samples: {total_samples}\", end=\"\\r\")\n",
    "\n",
    "    if show_progress:\n",
    "        print()\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += x.size(0)\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "def train_with_early_stopping(model, train_loader, test_loader, optimizer,\n",
    "                              device, max_epochs=40, patience=10, name=\"Model\"):\n",
    "    if isinstance(device, str):\n",
    "        device = torch.device(device)\n",
    "\n",
    "    best_acc = 0.0\n",
    "    epochs_without_improvement = 0\n",
    "    start = time.time()\n",
    "    last_acc = 0.0\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        loss = train_one_epoch(model, train_loader, optimizer, device, show_progress=False)\n",
    "        acc = evaluate(model, test_loader, device)\n",
    "        last_acc = acc\n",
    "\n",
    "        elapsed = time.time() - start\n",
    "        print(f\"  [{name}] Epoch {epoch:2d} | loss={loss:.4f} \"\n",
    "              f\"| test_acc={acc:.4f} | time={elapsed:.1f}s\")\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"  [{name}] Early stop at epoch {epoch} \"\n",
    "                  f\"(no improvement for {patience} epochs)\")\n",
    "            break\n",
    "\n",
    "    total_time = time.time() - start\n",
    "    return {\n",
    "        \"best_acc\": best_acc,\n",
    "        \"final_acc\": last_acc,\n",
    "        \"time\": total_time,\n",
    "        \"epochs\": epoch,\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Data utilities\n",
    "# ============================================\n",
    "\n",
    "def stratified_sample(dataset, n_samples_per_class):\n",
    "    \"\"\"\n",
    "    Create a stratified sample with n_samples_per_class from each class.\n",
    "    Maintains class balance.\n",
    "    \"\"\"\n",
    "    # Group indices by class\n",
    "    class_indices = defaultdict(list)\n",
    "    for idx, (_, label) in enumerate(dataset):\n",
    "        class_indices[label].append(idx)\n",
    "    \n",
    "    # Sample from each class\n",
    "    sampled_indices = []\n",
    "    for class_label in sorted(class_indices.keys()):\n",
    "        indices = class_indices[class_label]\n",
    "        # Use fixed random seed for reproducibility\n",
    "        rng = np.random.RandomState(42)\n",
    "        selected = rng.choice(indices, size=min(n_samples_per_class, len(indices)), \n",
    "                             replace=False)\n",
    "        sampled_indices.extend(selected)\n",
    "    \n",
    "    return sampled_indices\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Main\n",
    "# ============================================\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"BLOCK 2: QuatNet Training (Frozen Preprocessor)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nConfiguration:\")\n",
    "    print(\"  • Batch size: 32\")\n",
    "    print(\"  • Patience: 10\")\n",
    "    print(\"  • Seeds: [42, 123, 456]\")\n",
    "    print(\"  • Architecture: [frozen 784→16] → 4 quats → 16 quats → 10\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Load frozen preprocessor from Block 1\n",
    "    print(\"\\nLoading frozen preprocessor from Block 1...\")\n",
    "    try:\n",
    "        realnet_data = torch.load(\"realnet_results.pt\", weights_only=False)\n",
    "        preprocessor_state = realnet_data[\"preprocessor_state\"]\n",
    "        print(\"✓ Loaded preprocessor state from realnet_results.pt\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"✗ ERROR: realnet_results.pt not found!\")\n",
    "        print(\"  Run Block 1 first to train RealNet and save preprocessor.\")\n",
    "        return\n",
    "\n",
    "    # Create frozen preprocessor\n",
    "    shared_preprocessor = SharedPreprocessor(784, 16).to(device)\n",
    "    shared_preprocessor.load_state_dict(preprocessor_state)\n",
    "    for p in shared_preprocessor.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    preprocessor_params = sum(p.numel() for p in shared_preprocessor.parameters())\n",
    "    print(f\"  Preprocessor frozen with {preprocessor_params:,} params\")\n",
    "\n",
    "    # Load data\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    full_train_ds = datasets.MNIST(root=\"./data\", train=True,\n",
    "                                   download=True, transform=transform)\n",
    "    full_test_ds = datasets.MNIST(root=\"./data\", train=False,\n",
    "                                  download=True, transform=transform)\n",
    "\n",
    "    # Create stratified samples (same as Block 1)\n",
    "    print(\"\\nCreating stratified samples...\")\n",
    "    train_indices = stratified_sample(full_train_ds, n_samples_per_class=1500)\n",
    "    test_indices = stratified_sample(full_test_ds, n_samples_per_class=300)\n",
    "    \n",
    "    train_ds = Subset(full_train_ds, train_indices)\n",
    "    test_ds = Subset(full_test_ds, test_indices)\n",
    "    \n",
    "    print(f\"  Train samples: {len(train_ds)} (stratified, 1500 per class)\")\n",
    "    print(f\"  Test samples:  {len(test_ds)} (stratified, 300 per class)\")\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=32,\n",
    "                              shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_ds, batch_size=64,\n",
    "                             shuffle=False, num_workers=0)\n",
    "\n",
    "    seeds = [42, 123, 456]\n",
    "    all_results = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(f\"SEED {seed}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        set_all_seeds(seed)\n",
    "\n",
    "        print(f\"\\n  Training QuatNet (seed={seed})...\")\n",
    "        quat_model = QuatNet().to(device)\n",
    "        quat_model.preprocessor = shared_preprocessor  # Use frozen preprocessor\n",
    "        \n",
    "        quat_opt = torch.optim.Adam(quat_model.head.parameters(), lr=1e-3)\n",
    "        \n",
    "        result = train_with_early_stopping(\n",
    "            quat_model, train_loader, test_loader, quat_opt, device,\n",
    "            max_epochs=40, patience=10, name=\"Quat\"\n",
    "        )\n",
    "        \n",
    "        result[\"trainable_params\"] = sum(p.numel() for p in quat_model.parameters()\n",
    "                                         if p.requires_grad)\n",
    "        result[\"total_params\"] = sum(p.numel() for p in quat_model.parameters())\n",
    "        result[\"seed\"] = seed\n",
    "        \n",
    "        all_results.append(result)\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"QUATNET SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    accs = [r[\"best_acc\"] for r in all_results]\n",
    "    times = [r[\"time\"] for r in all_results]\n",
    "    epochs = [r[\"epochs\"] for r in all_results]\n",
    "    trainable = all_results[0][\"trainable_params\"]\n",
    "    total = all_results[0][\"total_params\"]\n",
    "    \n",
    "    print(f\"\\nAccuracy:   {np.mean(accs):.4f} ± {np.std(accs):.4f}\")\n",
    "    print(f\"Time:       {np.mean(times):.1f}s ± {np.std(times):.1f}s\")\n",
    "    print(f\"Epochs:     {np.mean(epochs):.1f} ± {np.std(epochs):.1f}\")\n",
    "    print(f\"Parameters: {trainable:,} trainable (head), {total:,} total\")\n",
    "    \n",
    "    print(\"\\nPer-seed results:\")\n",
    "    for r in all_results:\n",
    "        print(f\"  Seed {r['seed']}: acc={r['best_acc']:.4f}, \"\n",
    "              f\"time={r['time']:.1f}s, epochs={r['epochs']}\")\n",
    "\n",
    "    # Save results\n",
    "    save_dict = {\n",
    "        \"results\": all_results,\n",
    "        \"summary\": {\n",
    "            \"mean_acc\": np.mean(accs),\n",
    "            \"std_acc\": np.std(accs),\n",
    "            \"mean_time\": np.mean(times),\n",
    "            \"trainable_params\": trainable,\n",
    "            \"total_params\": total\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    torch.save(save_dict, \"quatnet_results.pt\")\n",
    "    print(f\"\\n✓ Saved results to: quatnet_results.pt\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3469a9-84e6-4425-9308-d43e56c2b064",
   "metadata": {},
   "source": [
    "# Quantum No Entanglement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c8f0c5d-9897-4c68-8c32-e3c0ab6d340e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using lightning.gpu device\n",
      "Using PyTorch device: cuda\n",
      "======================================================================\n",
      "BLOCK 3: Quantum (NO Entanglement) Training\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "  • Batch size: 32\n",
      "  • Patience: 10\n",
      "  • Max epochs: 200\n",
      "  • Seeds: [42, 123, 456]\n",
      "  • Architecture: [frozen 784→16] → 4 qubits (3 layers, NO ent) → 10\n",
      "  • Quantum device: lightning.gpu\n",
      "======================================================================\n",
      "\n",
      "Loading frozen preprocessor from Block 1...\n",
      "✓ Loaded preprocessor state from realnet_results.pt\n",
      "  Preprocessor frozen with 12,560 params\n",
      "\n",
      "Creating stratified samples...\n",
      "  Train samples: 15000 (stratified, 1500 per class)\n",
      "  Test samples:  3000 (stratified, 300 per class)\n",
      "\n",
      "======================================================================\n",
      "SEED 42\n",
      "======================================================================\n",
      "\n",
      "  Training QuantumNet (NO entanglement, seed=42)...\n",
      "  [QuantumNoEnt] Epoch 1/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  1 | loss=1.7968 | test_acc=0.6567 | time=451.5s\n",
      "  [QuantumNoEnt] Epoch 2/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  2 | loss=1.1060 | test_acc=0.7373 | time=1367.2s\n",
      "  [QuantumNoEnt] Epoch 3/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  3 | loss=0.8168 | test_acc=0.7833 | time=2182.9s\n",
      "  [QuantumNoEnt] Epoch 4/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  4 | loss=0.6715 | test_acc=0.8160 | time=2651.2s\n",
      "  [QuantumNoEnt] Epoch 5/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  5 | loss=0.5860 | test_acc=0.8233 | time=3098.1s\n",
      "  [QuantumNoEnt] Epoch 6/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  6 | loss=0.5343 | test_acc=0.8277 | time=3548.8s\n",
      "  [QuantumNoEnt] Epoch 7/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  7 | loss=0.4990 | test_acc=0.8313 | time=4005.9s\n",
      "  [QuantumNoEnt] Epoch 8/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  8 | loss=0.4701 | test_acc=0.8377 | time=4464.9s\n",
      "  [QuantumNoEnt] Epoch 9/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  9 | loss=0.4418 | test_acc=0.8577 | time=4923.6s\n",
      "  [QuantumNoEnt] Epoch 10/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 10 | loss=0.4143 | test_acc=0.8640 | time=5382.8s\n",
      "  [QuantumNoEnt] Epoch 11/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 11 | loss=0.3925 | test_acc=0.8653 | time=5840.2s\n",
      "  [QuantumNoEnt] Epoch 12/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 12 | loss=0.3757 | test_acc=0.8653 | time=6299.6s\n",
      "  [QuantumNoEnt] Epoch 13/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 13 | loss=0.3638 | test_acc=0.8697 | time=6755.3s\n",
      "  [QuantumNoEnt] Epoch 14/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 14 | loss=0.3548 | test_acc=0.8680 | time=7212.4s\n",
      "  [QuantumNoEnt] Epoch 15/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 15 | loss=0.3474 | test_acc=0.8690 | time=7662.7s\n",
      "  [QuantumNoEnt] Epoch 16/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 16 | loss=0.3415 | test_acc=0.8720 | time=8122.4s\n",
      "  [QuantumNoEnt] Epoch 17/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 17 | loss=0.3367 | test_acc=0.8693 | time=8587.4s\n",
      "  [QuantumNoEnt] Epoch 18/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 18 | loss=0.3323 | test_acc=0.8740 | time=9049.3s\n",
      "  [QuantumNoEnt] Epoch 19/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 19 | loss=0.3288 | test_acc=0.8750 | time=9525.1s\n",
      "  [QuantumNoEnt] Epoch 20/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 20 | loss=0.3255 | test_acc=0.8767 | time=9990.9s\n",
      "  [QuantumNoEnt] Epoch 21/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 21 | loss=0.3237 | test_acc=0.8767 | time=10458.4s\n",
      "  [QuantumNoEnt] Epoch 22/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 22 | loss=0.3205 | test_acc=0.8763 | time=10931.4s\n",
      "  [QuantumNoEnt] Epoch 23/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 23 | loss=0.3184 | test_acc=0.8780 | time=11401.8s\n",
      "  [QuantumNoEnt] Epoch 24/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 24 | loss=0.3167 | test_acc=0.8733 | time=11852.9s\n",
      "  [QuantumNoEnt] Epoch 25/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 25 | loss=0.3145 | test_acc=0.8797 | time=12305.4s\n",
      "  [QuantumNoEnt] Epoch 26/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 26 | loss=0.3122 | test_acc=0.8750 | time=12757.7s\n",
      "  [QuantumNoEnt] Epoch 27/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 27 | loss=0.3116 | test_acc=0.8830 | time=13246.1s\n",
      "  [QuantumNoEnt] Epoch 28/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 28 | loss=0.3094 | test_acc=0.8783 | time=13739.1s\n",
      "  [QuantumNoEnt] Epoch 29/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 29 | loss=0.3086 | test_acc=0.8820 | time=14237.7s\n",
      "  [QuantumNoEnt] Epoch 30/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 30 | loss=0.3069 | test_acc=0.8807 | time=14703.1s\n",
      "  [QuantumNoEnt] Epoch 31/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 31 | loss=0.3055 | test_acc=0.8810 | time=15162.8s\n",
      "  [QuantumNoEnt] Epoch 32/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 32 | loss=0.3045 | test_acc=0.8803 | time=15620.7s\n",
      "  [QuantumNoEnt] Epoch 33/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 33 | loss=0.3041 | test_acc=0.8847 | time=16077.6s\n",
      "  [QuantumNoEnt] Epoch 34/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 34 | loss=0.3025 | test_acc=0.8827 | time=16551.0s\n",
      "  [QuantumNoEnt] Epoch 35/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 35 | loss=0.3009 | test_acc=0.8847 | time=17019.7s\n",
      "  [QuantumNoEnt] Epoch 36/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 36 | loss=0.3006 | test_acc=0.8850 | time=17513.2s\n",
      "  [QuantumNoEnt] Epoch 37/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 37 | loss=0.2991 | test_acc=0.8867 | time=17987.2s\n",
      "  [QuantumNoEnt] Epoch 38/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 38 | loss=0.2977 | test_acc=0.8867 | time=18469.0s\n",
      "  [QuantumNoEnt] Epoch 39/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 39 | loss=0.2980 | test_acc=0.8840 | time=18948.8s\n",
      "  [QuantumNoEnt] Epoch 40/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 40 | loss=0.2967 | test_acc=0.8823 | time=19421.2s\n",
      "  [QuantumNoEnt] Epoch 41/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 41 | loss=0.2970 | test_acc=0.8847 | time=19899.9s\n",
      "  [QuantumNoEnt] Epoch 42/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 42 | loss=0.2955 | test_acc=0.8837 | time=20379.9s\n",
      "  [QuantumNoEnt] Epoch 43/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 43 | loss=0.2943 | test_acc=0.8870 | time=20837.4s\n",
      "  [QuantumNoEnt] Epoch 44/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 44 | loss=0.2950 | test_acc=0.8850 | time=21308.9s\n",
      "  [QuantumNoEnt] Epoch 45/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 45 | loss=0.2934 | test_acc=0.8863 | time=21784.0s\n",
      "  [QuantumNoEnt] Epoch 46/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 46 | loss=0.2931 | test_acc=0.8877 | time=22249.5s\n",
      "  [QuantumNoEnt] Epoch 47/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 47 | loss=0.2920 | test_acc=0.8897 | time=22709.4s\n",
      "  [QuantumNoEnt] Epoch 48/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 48 | loss=0.2921 | test_acc=0.8880 | time=23190.5s\n",
      "  [QuantumNoEnt] Epoch 49/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 49 | loss=0.2914 | test_acc=0.8867 | time=23668.1s\n",
      "  [QuantumNoEnt] Epoch 50/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 50 | loss=0.2910 | test_acc=0.8833 | time=24140.6s\n",
      "  [QuantumNoEnt] Epoch 51/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 51 | loss=0.2905 | test_acc=0.8883 | time=24627.1s\n",
      "  [QuantumNoEnt] Epoch 52/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 52 | loss=0.2891 | test_acc=0.8863 | time=25104.3s\n",
      "  [QuantumNoEnt] Epoch 53/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 53 | loss=0.2899 | test_acc=0.8867 | time=25589.7s\n",
      "  [QuantumNoEnt] Epoch 54/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 54 | loss=0.2890 | test_acc=0.8850 | time=26067.7s\n",
      "  [QuantumNoEnt] Epoch 55/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 55 | loss=0.2881 | test_acc=0.8870 | time=26535.1s\n",
      "  [QuantumNoEnt] Epoch 56/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 56 | loss=0.2878 | test_acc=0.8847 | time=27020.5s\n",
      "  [QuantumNoEnt] Epoch 57/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 57 | loss=0.2879 | test_acc=0.8877 | time=27507.6s\n",
      "  [QuantumNoEnt] Early stop at epoch 57 (no improvement for 10 epochs)\n",
      "\n",
      "======================================================================\n",
      "SEED 123\n",
      "======================================================================\n",
      "\n",
      "  Training QuantumNet (NO entanglement, seed=123)...\n",
      "  [QuantumNoEnt] Epoch 1/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  1 | loss=1.7089 | test_acc=0.6353 | time=478.7s\n",
      "  [QuantumNoEnt] Epoch 2/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  2 | loss=1.1375 | test_acc=0.7387 | time=947.5s\n",
      "  [QuantumNoEnt] Epoch 3/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  3 | loss=0.9024 | test_acc=0.7807 | time=1425.3s\n",
      "  [QuantumNoEnt] Epoch 4/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  4 | loss=0.7707 | test_acc=0.7883 | time=1895.5s\n",
      "  [QuantumNoEnt] Epoch 5/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  5 | loss=0.6883 | test_acc=0.7990 | time=2352.6s\n",
      "  [QuantumNoEnt] Epoch 6/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  6 | loss=0.6343 | test_acc=0.8027 | time=2812.0s\n",
      "  [QuantumNoEnt] Epoch 7/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  7 | loss=0.5941 | test_acc=0.8183 | time=3276.7s\n",
      "  [QuantumNoEnt] Epoch 8/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  8 | loss=0.5615 | test_acc=0.8280 | time=3758.3s\n",
      "  [QuantumNoEnt] Epoch 9/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  9 | loss=0.5388 | test_acc=0.8337 | time=4262.8s\n",
      "  [QuantumNoEnt] Epoch 10/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 10 | loss=0.5244 | test_acc=0.8340 | time=4766.2s\n",
      "  [QuantumNoEnt] Epoch 11/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 11 | loss=0.5153 | test_acc=0.8303 | time=5254.8s\n",
      "  [QuantumNoEnt] Epoch 12/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 12 | loss=0.5073 | test_acc=0.8363 | time=5753.8s\n",
      "  [QuantumNoEnt] Epoch 13/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 13 | loss=0.5007 | test_acc=0.8397 | time=6217.0s\n",
      "  [QuantumNoEnt] Epoch 14/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 14 | loss=0.4953 | test_acc=0.8360 | time=6676.8s\n",
      "  [QuantumNoEnt] Epoch 15/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 15 | loss=0.4906 | test_acc=0.8427 | time=7156.8s\n",
      "  [QuantumNoEnt] Epoch 16/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 16 | loss=0.4871 | test_acc=0.8397 | time=7638.0s\n",
      "  [QuantumNoEnt] Epoch 17/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 17 | loss=0.4837 | test_acc=0.8417 | time=8115.4s\n",
      "  [QuantumNoEnt] Epoch 18/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 18 | loss=0.4808 | test_acc=0.8400 | time=8597.2s\n",
      "  [QuantumNoEnt] Epoch 19/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 19 | loss=0.4783 | test_acc=0.8423 | time=9083.3s\n",
      "  [QuantumNoEnt] Epoch 20/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 20 | loss=0.4763 | test_acc=0.8387 | time=9565.6s\n",
      "  [QuantumNoEnt] Epoch 21/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 21 | loss=0.4734 | test_acc=0.8420 | time=10040.1s\n",
      "  [QuantumNoEnt] Epoch 22/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 22 | loss=0.4709 | test_acc=0.8447 | time=10532.3s\n",
      "  [QuantumNoEnt] Epoch 23/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 23 | loss=0.4698 | test_acc=0.8473 | time=11017.5s\n",
      "  [QuantumNoEnt] Epoch 24/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 24 | loss=0.4678 | test_acc=0.8440 | time=11489.2s\n",
      "  [QuantumNoEnt] Epoch 25/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 25 | loss=0.4660 | test_acc=0.8450 | time=11958.2s\n",
      "  [QuantumNoEnt] Epoch 26/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 26 | loss=0.4643 | test_acc=0.8483 | time=12417.3s\n",
      "  [QuantumNoEnt] Epoch 27/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 27 | loss=0.4631 | test_acc=0.8463 | time=12893.1s\n",
      "  [QuantumNoEnt] Epoch 28/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 28 | loss=0.4613 | test_acc=0.8470 | time=13374.7s\n",
      "  [QuantumNoEnt] Epoch 29/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 29 | loss=0.4602 | test_acc=0.8493 | time=13842.0s\n",
      "  [QuantumNoEnt] Epoch 30/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 30 | loss=0.4580 | test_acc=0.8483 | time=14300.7s\n",
      "  [QuantumNoEnt] Epoch 31/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 31 | loss=0.4570 | test_acc=0.8480 | time=14775.4s\n",
      "  [QuantumNoEnt] Epoch 32/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 32 | loss=0.4554 | test_acc=0.8520 | time=15253.5s\n",
      "  [QuantumNoEnt] Epoch 33/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 33 | loss=0.4535 | test_acc=0.8497 | time=15729.4s\n",
      "  [QuantumNoEnt] Epoch 34/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 34 | loss=0.4520 | test_acc=0.8500 | time=16211.7s\n",
      "  [QuantumNoEnt] Epoch 35/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 35 | loss=0.4504 | test_acc=0.8523 | time=16689.2s\n",
      "  [QuantumNoEnt] Epoch 36/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 36 | loss=0.4495 | test_acc=0.8523 | time=17170.5s\n",
      "  [QuantumNoEnt] Epoch 37/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 37 | loss=0.4466 | test_acc=0.8520 | time=17682.7s\n",
      "  [QuantumNoEnt] Epoch 38/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 38 | loss=0.4448 | test_acc=0.8527 | time=18172.2s\n",
      "  [QuantumNoEnt] Epoch 39/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 39 | loss=0.4424 | test_acc=0.8533 | time=18642.5s\n",
      "  [QuantumNoEnt] Epoch 40/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 40 | loss=0.4406 | test_acc=0.8540 | time=19141.7s\n",
      "  [QuantumNoEnt] Epoch 41/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 41 | loss=0.4377 | test_acc=0.8547 | time=19603.7s\n",
      "  [QuantumNoEnt] Epoch 42/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 42 | loss=0.4355 | test_acc=0.8560 | time=20069.7s\n",
      "  [QuantumNoEnt] Epoch 43/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 43 | loss=0.4317 | test_acc=0.8540 | time=20563.3s\n",
      "  [QuantumNoEnt] Epoch 44/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 44 | loss=0.4308 | test_acc=0.8577 | time=21031.6s\n",
      "  [QuantumNoEnt] Epoch 45/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 45 | loss=0.4270 | test_acc=0.8580 | time=21495.2s\n",
      "  [QuantumNoEnt] Epoch 46/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 46 | loss=0.4247 | test_acc=0.8620 | time=21978.4s\n",
      "  [QuantumNoEnt] Epoch 47/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 47 | loss=0.4229 | test_acc=0.8593 | time=22454.7s\n",
      "  [QuantumNoEnt] Epoch 48/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 48 | loss=0.4202 | test_acc=0.8600 | time=22931.2s\n",
      "  [QuantumNoEnt] Epoch 49/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 49 | loss=0.4174 | test_acc=0.8630 | time=23398.4s\n",
      "  [QuantumNoEnt] Epoch 50/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 50 | loss=0.4157 | test_acc=0.8593 | time=23860.0s\n",
      "  [QuantumNoEnt] Epoch 51/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 51 | loss=0.4142 | test_acc=0.8583 | time=24329.5s\n",
      "  [QuantumNoEnt] Epoch 52/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 52 | loss=0.4133 | test_acc=0.8640 | time=24795.1s\n",
      "  [QuantumNoEnt] Epoch 53/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 53 | loss=0.4114 | test_acc=0.8617 | time=25277.8s\n",
      "  [QuantumNoEnt] Epoch 54/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 54 | loss=0.4096 | test_acc=0.8643 | time=25757.8s\n",
      "  [QuantumNoEnt] Epoch 55/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 55 | loss=0.4090 | test_acc=0.8637 | time=26238.7s\n",
      "  [QuantumNoEnt] Epoch 56/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 56 | loss=0.4072 | test_acc=0.8630 | time=26735.9s\n",
      "  [QuantumNoEnt] Epoch 57/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 57 | loss=0.4061 | test_acc=0.8650 | time=27216.1s\n",
      "  [QuantumNoEnt] Epoch 58/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 58 | loss=0.4053 | test_acc=0.8670 | time=27715.8s\n",
      "  [QuantumNoEnt] Epoch 59/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 59 | loss=0.4042 | test_acc=0.8633 | time=28186.5s\n",
      "  [QuantumNoEnt] Epoch 60/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 60 | loss=0.4027 | test_acc=0.8647 | time=28672.9s\n",
      "  [QuantumNoEnt] Epoch 61/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 61 | loss=0.4024 | test_acc=0.8643 | time=29146.0s\n",
      "  [QuantumNoEnt] Epoch 62/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 62 | loss=0.4015 | test_acc=0.8640 | time=29626.2s\n",
      "  [QuantumNoEnt] Epoch 63/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 63 | loss=0.4002 | test_acc=0.8643 | time=30109.9s\n",
      "  [QuantumNoEnt] Epoch 64/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 64 | loss=0.3997 | test_acc=0.8650 | time=30586.2s\n",
      "  [QuantumNoEnt] Epoch 65/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 65 | loss=0.3986 | test_acc=0.8637 | time=31058.6s\n",
      "  [QuantumNoEnt] Epoch 66/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 66 | loss=0.3991 | test_acc=0.8650 | time=31528.3s\n",
      "  [QuantumNoEnt] Epoch 67/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 67 | loss=0.3974 | test_acc=0.8633 | time=31999.4s\n",
      "  [QuantumNoEnt] Epoch 68/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 68 | loss=0.3972 | test_acc=0.8663 | time=32479.8s\n",
      "  [QuantumNoEnt] Early stop at epoch 68 (no improvement for 10 epochs)\n",
      "\n",
      "======================================================================\n",
      "SEED 456\n",
      "======================================================================\n",
      "\n",
      "  Training QuantumNet (NO entanglement, seed=456)...\n",
      "  [QuantumNoEnt] Epoch 1/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  1 | loss=1.7560 | test_acc=0.6743 | time=476.7s\n",
      "  [QuantumNoEnt] Epoch 2/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  2 | loss=0.9905 | test_acc=0.7923 | time=958.5s\n",
      "  [QuantumNoEnt] Epoch 3/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  3 | loss=0.6978 | test_acc=0.8397 | time=1436.1s\n",
      "  [QuantumNoEnt] Epoch 4/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  4 | loss=0.5624 | test_acc=0.8547 | time=1912.4s\n",
      "  [QuantumNoEnt] Epoch 5/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  5 | loss=0.4909 | test_acc=0.8593 | time=2382.5s\n",
      "  [QuantumNoEnt] Epoch 6/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  6 | loss=0.4520 | test_acc=0.8623 | time=2867.7s\n",
      "  [QuantumNoEnt] Epoch 7/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  7 | loss=0.4272 | test_acc=0.8607 | time=3356.6s\n",
      "  [QuantumNoEnt] Epoch 8/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  8 | loss=0.4102 | test_acc=0.8630 | time=3853.9s\n",
      "  [QuantumNoEnt] Epoch 9/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  9 | loss=0.3984 | test_acc=0.8633 | time=4345.5s\n",
      "  [QuantumNoEnt] Epoch 10/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 10 | loss=0.3886 | test_acc=0.8647 | time=4827.1s\n",
      "  [QuantumNoEnt] Epoch 11/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 11 | loss=0.3815 | test_acc=0.8623 | time=5302.1s\n",
      "  [QuantumNoEnt] Epoch 12/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 12 | loss=0.3750 | test_acc=0.8610 | time=5795.0s\n",
      "  [QuantumNoEnt] Epoch 13/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 13 | loss=0.3700 | test_acc=0.8650 | time=6270.0s\n",
      "  [QuantumNoEnt] Epoch 14/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 14 | loss=0.3645 | test_acc=0.8643 | time=6747.3s\n",
      "  [QuantumNoEnt] Epoch 15/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 15 | loss=0.3610 | test_acc=0.8653 | time=7222.7s\n",
      "  [QuantumNoEnt] Epoch 16/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 16 | loss=0.3565 | test_acc=0.8653 | time=7696.6s\n",
      "  [QuantumNoEnt] Epoch 17/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 17 | loss=0.3527 | test_acc=0.8653 | time=8180.1s\n",
      "  [QuantumNoEnt] Epoch 18/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 18 | loss=0.3503 | test_acc=0.8677 | time=8647.1s\n",
      "  [QuantumNoEnt] Epoch 19/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 19 | loss=0.3474 | test_acc=0.8690 | time=9113.2s\n",
      "  [QuantumNoEnt] Epoch 20/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 20 | loss=0.3443 | test_acc=0.8650 | time=9582.8s\n",
      "  [QuantumNoEnt] Epoch 21/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 21 | loss=0.3414 | test_acc=0.8660 | time=10064.3s\n",
      "  [QuantumNoEnt] Epoch 22/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 22 | loss=0.3401 | test_acc=0.8663 | time=10544.5s\n",
      "  [QuantumNoEnt] Epoch 23/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 23 | loss=0.3366 | test_acc=0.8633 | time=11025.4s\n",
      "  [QuantumNoEnt] Epoch 24/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 24 | loss=0.3348 | test_acc=0.8673 | time=11506.0s\n",
      "  [QuantumNoEnt] Epoch 25/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 25 | loss=0.3334 | test_acc=0.8660 | time=11971.8s\n",
      "  [QuantumNoEnt] Epoch 26/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 26 | loss=0.3318 | test_acc=0.8690 | time=12430.5s\n",
      "  [QuantumNoEnt] Epoch 27/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 27 | loss=0.3292 | test_acc=0.8653 | time=12884.3s\n",
      "  [QuantumNoEnt] Epoch 28/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 28 | loss=0.3276 | test_acc=0.8677 | time=13345.3s\n",
      "  [QuantumNoEnt] Epoch 29/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 29 | loss=0.3262 | test_acc=0.8663 | time=13816.6s\n",
      "  [QuantumNoEnt] Early stop at epoch 29 (no improvement for 10 epochs)\n",
      "\n",
      "======================================================================\n",
      "QUANTUM (NO ENTANGLEMENT) SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Accuracy:   0.8752 ± 0.0102\n",
      "Time:       24601.3s ± 7891.5s\n",
      "Epochs:     51.3 ± 16.4\n",
      "Parameters: 162 trainable (head), 12,722 total\n",
      "\n",
      "Per-seed results:\n",
      "  Seed 42: acc=0.8897, time=27507.6s, epochs=57\n",
      "  Seed 123: acc=0.8670, time=32479.8s, epochs=68\n",
      "  Seed 456: acc=0.8690, time=13816.6s, epochs=29\n",
      "\n",
      "✓ Saved results to: quantum_noent_results.pt\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Block 3: Quantum (No Entanglement) Training\n",
    "============================================\n",
    "Loads frozen preprocessor from Block 1 and trains quantum head WITHOUT entanglement.\n",
    "Uses Lightning-GPU acceleration.\n",
    "\n",
    "Requirements:\n",
    "- realnet_results.pt (from Block 1)\n",
    "- pennylane, pennylane-lightning-gpu\n",
    "\n",
    "Outputs:\n",
    "- quantum_noent_results.pt: Contains quantum (no ent) results\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from collections import defaultdict\n",
    "\n",
    "try:\n",
    "    import pennylane as qml\n",
    "    QUANTUM_DEVICE = \"lightning.gpu\"\n",
    "    PENNYLANE_AVAILABLE = True\n",
    "    print(\"✓ Using lightning.gpu device\")\n",
    "except ImportError:\n",
    "    PENNYLANE_AVAILABLE = False\n",
    "    print(\"✗ PennyLane not installed\")\n",
    "    exit(1)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "print(f\"Using PyTorch device: {device}\")\n",
    "\n",
    "# ============================================\n",
    "# Comprehensive seed setting for reproducibility\n",
    "# ============================================\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    \"\"\"Set seeds for all RNG sources for reproducibility\"\"\"\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    # Python\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # NumPy\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # PyTorch deterministic mode\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # CuPy (if available)\n",
    "    try:\n",
    "        import cupy as cp\n",
    "        cp.random.seed(seed)\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    # PennyLane (if available)\n",
    "    try:\n",
    "        import pennylane as qml\n",
    "        qml.numpy.random.seed(seed)\n",
    "    except (ImportError, AttributeError):\n",
    "        pass\n",
    "\n",
    "# ============================================\n",
    "# Shared Preprocessor (must match Block 1)\n",
    "# ============================================\n",
    "\n",
    "class SharedPreprocessor(nn.Module):\n",
    "    \"\"\"Shared classical feature extractor: 784 → 16\"\"\"\n",
    "    def __init__(self, input_dim=784, bottleneck_dim=16):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, bottleneck_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.tanh(self.fc(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Quantum Head (No Entanglement)\n",
    "# ============================================\n",
    "\n",
    "class QuantumHead(nn.Module):\n",
    "    \"\"\"\n",
    "    VQC with 4 qubits, 3 layers, NO entanglement → 10 classes.\n",
    "    Uses Lightning acceleration (GPU).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_qubits=4, n_layers=3, num_classes=10, device_name=None):\n",
    "        super().__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Map 16 features → n_qubits\n",
    "        self.feature_select = nn.Linear(16, n_qubits)\n",
    "\n",
    "        # Use specified device or default\n",
    "        if device_name is None:\n",
    "            device_name = QUANTUM_DEVICE\n",
    "        \n",
    "        # Quantum device\n",
    "        self.dev = qml.device(device_name, wires=n_qubits)\n",
    "\n",
    "        # Use adjoint differentiation for lightning (much faster)\n",
    "        diff_method = \"adjoint\" if \"lightning\" in device_name else \"parameter-shift\"\n",
    "\n",
    "        @qml.qnode(self.dev, interface=\"torch\", diff_method=diff_method)\n",
    "        def quantum_circuit(inputs, weights):\n",
    "            \"\"\"\n",
    "            Single-sample circuit WITHOUT entanglement.\n",
    "            inputs: (n_qubits,)\n",
    "            weights: (n_layers, n_qubits, 2)\n",
    "            \"\"\"\n",
    "            for layer in range(n_layers):\n",
    "                # Data re-uploading\n",
    "                for i in range(n_qubits):\n",
    "                    qml.RY(inputs[i], wires=i)\n",
    "\n",
    "                # Trainable rotations\n",
    "                for i in range(n_qubits):\n",
    "                    qml.RY(weights[layer, i, 0], wires=i)\n",
    "                    qml.RZ(weights[layer, i, 1], wires=i)\n",
    "\n",
    "                # NO ENTANGLEMENT\n",
    "\n",
    "            return [\n",
    "                qml.expval(qml.PauliZ(0)),\n",
    "                qml.expval(qml.PauliZ(1)),\n",
    "                qml.expval(qml.PauliZ(2)),\n",
    "                qml.expval(qml.PauliZ(3)),\n",
    "                qml.expval(qml.PauliZ(0) @ qml.PauliZ(1)),\n",
    "                qml.expval(qml.PauliZ(2) @ qml.PauliZ(3))\n",
    "            ]\n",
    "\n",
    "        self.quantum_circuit = quantum_circuit\n",
    "\n",
    "        weight_shape = (n_layers, n_qubits, 2)\n",
    "        self.q_weights = nn.Parameter(torch.randn(weight_shape) * 0.1)\n",
    "        self.fc_out = nn.Linear(6, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Process samples with progress tracking.\n",
    "        x: (batch, 16) real bottleneck features\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        x = torch.tanh(self.feature_select(x))\n",
    "\n",
    "        # Process in chunks for memory management\n",
    "        chunk_size = 32\n",
    "        quantum_outputs = []\n",
    "\n",
    "        for start_idx in range(0, batch_size, chunk_size):\n",
    "            end_idx = min(start_idx + chunk_size, batch_size)\n",
    "            chunk = x[start_idx:end_idx]\n",
    "\n",
    "            chunk_outputs = []\n",
    "            for i in range(chunk.size(0)):\n",
    "                q_raw = self.quantum_circuit(chunk[i], self.q_weights)\n",
    "                if isinstance(q_raw, (list, tuple)):\n",
    "                    q_out = torch.stack(q_raw)\n",
    "                else:\n",
    "                    q_out = q_raw\n",
    "                chunk_outputs.append(q_out)\n",
    "\n",
    "            quantum_outputs.extend(chunk_outputs)\n",
    "\n",
    "        # Convert to tensor (cast to float32)\n",
    "        quantum_outputs = torch.stack(quantum_outputs).float()\n",
    "        quantum_outputs = quantum_outputs.to(self.fc_out.weight.dtype)\n",
    "\n",
    "        output = self.fc_out(quantum_outputs)\n",
    "        return output\n",
    "\n",
    "\n",
    "class QuantumNet(nn.Module):\n",
    "    \"\"\"Complete Quantum network: Preprocessor + QuantumHead (no ent)\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.preprocessor = SharedPreprocessor(784, 16)\n",
    "        self.head = QuantumHead(n_qubits=4, n_layers=3, num_classes=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.preprocessor(x)\n",
    "        return self.head(features)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Training and Evaluation\n",
    "# ============================================\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, show_progress=True):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(loader):\n",
    "        # Keep data on CPU for quantum models\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        total_samples += x.size(0)\n",
    "\n",
    "        if show_progress and batch_idx % 20 == 0:\n",
    "            print(f\"    Batch {batch_idx}/{len(loader)}, samples: {total_samples}\", end=\"\\r\")\n",
    "\n",
    "    if show_progress:\n",
    "        print()\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            # Keep data on CPU\n",
    "            logits = model(x)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += x.size(0)\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "def train_with_early_stopping(model, train_loader, test_loader, optimizer,\n",
    "                              max_epochs=200, patience=10, name=\"Model\"):\n",
    "    best_acc = 0.0\n",
    "    epochs_without_improvement = 0\n",
    "    start = time.time()\n",
    "    last_acc = 0.0\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        print(f\"  [{name}] Epoch {epoch}/{max_epochs}\")\n",
    "        loss = train_one_epoch(model, train_loader, optimizer, show_progress=True)\n",
    "        acc = evaluate(model, test_loader)\n",
    "        last_acc = acc\n",
    "\n",
    "        elapsed = time.time() - start\n",
    "        print(f\"  [{name}] Epoch {epoch:2d} | loss={loss:.4f} \"\n",
    "              f\"| test_acc={acc:.4f} | time={elapsed:.1f}s\")\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"  [{name}] Early stop at epoch {epoch} \"\n",
    "                  f\"(no improvement for {patience} epochs)\")\n",
    "            break\n",
    "\n",
    "    total_time = time.time() - start\n",
    "    return {\n",
    "        \"best_acc\": best_acc,\n",
    "        \"final_acc\": last_acc,\n",
    "        \"time\": total_time,\n",
    "        \"epochs\": epoch,\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Data utilities\n",
    "# ============================================\n",
    "\n",
    "def stratified_sample(dataset, n_samples_per_class):\n",
    "    \"\"\"\n",
    "    Create a stratified sample with n_samples_per_class from each class.\n",
    "    Maintains class balance.\n",
    "    \"\"\"\n",
    "    # Group indices by class\n",
    "    class_indices = defaultdict(list)\n",
    "    for idx, (_, label) in enumerate(dataset):\n",
    "        class_indices[label].append(idx)\n",
    "    \n",
    "    # Sample from each class\n",
    "    sampled_indices = []\n",
    "    for class_label in sorted(class_indices.keys()):\n",
    "        indices = class_indices[class_label]\n",
    "        # Use fixed random seed for reproducibility\n",
    "        rng = np.random.RandomState(42)\n",
    "        selected = rng.choice(indices, size=min(n_samples_per_class, len(indices)), \n",
    "                             replace=False)\n",
    "        sampled_indices.extend(selected)\n",
    "    \n",
    "    return sampled_indices\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Main\n",
    "# ============================================\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"BLOCK 3: Quantum (NO Entanglement) Training\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nConfiguration:\")\n",
    "    print(\"  • Batch size: 32\")\n",
    "    print(\"  • Patience: 10\")\n",
    "    print(\"  • Max epochs: 200\")\n",
    "    print(\"  • Seeds: [42, 123, 456]\")\n",
    "    print(\"  • Architecture: [frozen 784→16] → 4 qubits (3 layers, NO ent) → 10\")\n",
    "    print(f\"  • Quantum device: {QUANTUM_DEVICE}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Load frozen preprocessor from Block 1\n",
    "    print(\"\\nLoading frozen preprocessor from Block 1...\")\n",
    "    try:\n",
    "        realnet_data = torch.load(\"realnet_results.pt\", weights_only=False)\n",
    "        preprocessor_state = realnet_data[\"preprocessor_state\"]\n",
    "        print(\"✓ Loaded preprocessor state from realnet_results.pt\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"✗ ERROR: realnet_results.pt not found!\")\n",
    "        print(\"  Run Block 1 first to train RealNet and save preprocessor.\")\n",
    "        return\n",
    "\n",
    "    # Create frozen preprocessor (CPU for quantum)\n",
    "    shared_preprocessor = SharedPreprocessor(784, 16)\n",
    "    shared_preprocessor.load_state_dict(preprocessor_state)\n",
    "    for p in shared_preprocessor.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    preprocessor_params = sum(p.numel() for p in shared_preprocessor.parameters())\n",
    "    print(f\"  Preprocessor frozen with {preprocessor_params:,} params\")\n",
    "\n",
    "    # Load data\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    full_train_ds = datasets.MNIST(root=\"./data\", train=True,\n",
    "                                   download=True, transform=transform)\n",
    "    full_test_ds = datasets.MNIST(root=\"./data\", train=False,\n",
    "                                  download=True, transform=transform)\n",
    "\n",
    "    # Create stratified samples (same as Block 1)\n",
    "    print(\"\\nCreating stratified samples...\")\n",
    "    train_indices = stratified_sample(full_train_ds, n_samples_per_class=1500)\n",
    "    test_indices = stratified_sample(full_test_ds, n_samples_per_class=300)\n",
    "    \n",
    "    train_ds = Subset(full_train_ds, train_indices)\n",
    "    test_ds = Subset(full_test_ds, test_indices)\n",
    "    \n",
    "    print(f\"  Train samples: {len(train_ds)} (stratified, 1500 per class)\")\n",
    "    print(f\"  Test samples:  {len(test_ds)} (stratified, 300 per class)\")\n",
    "\n",
    "    # Smaller batches for quantum\n",
    "    train_loader = DataLoader(train_ds, batch_size=32,\n",
    "                              shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_ds, batch_size=64,\n",
    "                             shuffle=False, num_workers=0)\n",
    "\n",
    "    seeds = [42, 123, 456]\n",
    "    all_results = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(f\"SEED {seed}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        set_all_seeds(seed)\n",
    "\n",
    "        print(f\"\\n  Training QuantumNet (NO entanglement, seed={seed})...\")\n",
    "        quantum_model = QuantumNet()\n",
    "        quantum_model.preprocessor = shared_preprocessor  # Use frozen preprocessor\n",
    "        \n",
    "        quantum_opt = torch.optim.Adam(quantum_model.head.parameters(), lr=1e-3)\n",
    "        \n",
    "        result = train_with_early_stopping(\n",
    "            quantum_model, train_loader, test_loader, quantum_opt,\n",
    "            max_epochs=200, patience=10, name=\"QuantumNoEnt\"\n",
    "        )\n",
    "        \n",
    "        result[\"trainable_params\"] = sum(p.numel() for p in quantum_model.parameters()\n",
    "                                         if p.requires_grad)\n",
    "        result[\"total_params\"] = sum(p.numel() for p in quantum_model.parameters())\n",
    "        result[\"seed\"] = seed\n",
    "        \n",
    "        all_results.append(result)\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"QUANTUM (NO ENTANGLEMENT) SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    accs = [r[\"best_acc\"] for r in all_results]\n",
    "    times = [r[\"time\"] for r in all_results]\n",
    "    epochs = [r[\"epochs\"] for r in all_results]\n",
    "    trainable = all_results[0][\"trainable_params\"]\n",
    "    total = all_results[0][\"total_params\"]\n",
    "    \n",
    "    print(f\"\\nAccuracy:   {np.mean(accs):.4f} ± {np.std(accs):.4f}\")\n",
    "    print(f\"Time:       {np.mean(times):.1f}s ± {np.std(times):.1f}s\")\n",
    "    print(f\"Epochs:     {np.mean(epochs):.1f} ± {np.std(epochs):.1f}\")\n",
    "    print(f\"Parameters: {trainable:,} trainable (head), {total:,} total\")\n",
    "    \n",
    "    print(\"\\nPer-seed results:\")\n",
    "    for r in all_results:\n",
    "        print(f\"  Seed {r['seed']}: acc={r['best_acc']:.4f}, \"\n",
    "              f\"time={r['time']:.1f}s, epochs={r['epochs']}\")\n",
    "\n",
    "    # Save results\n",
    "    save_dict = {\n",
    "        \"results\": all_results,\n",
    "        \"summary\": {\n",
    "            \"mean_acc\": np.mean(accs),\n",
    "            \"std_acc\": np.std(accs),\n",
    "            \"mean_time\": np.mean(times),\n",
    "            \"trainable_params\": trainable,\n",
    "            \"total_params\": total\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    torch.save(save_dict, \"quantum_noent_results.pt\")\n",
    "    print(f\"\\n✓ Saved results to: quantum_noent_results.pt\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c44b123-8a8c-4509-95cb-cde611fed48d",
   "metadata": {},
   "source": [
    "# Quantum Entanglement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c211251-6de5-436f-9eaf-874bfede69c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using lightning.gpu device\n",
      "Using PyTorch device: cuda\n",
      "======================================================================\n",
      "BLOCK 4: Quantum (WITH Entanglement) Training\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "  • Batch size: 32\n",
      "  • Patience: 10\n",
      "  • Max epochs: 200\n",
      "  • Seeds: [42, 123, 456]\n",
      "  • Architecture: [frozen 784→16] → 4 qubits (3 layers, WITH ent) → 10\n",
      "  • Quantum device: lightning.gpu\n",
      "======================================================================\n",
      "\n",
      "Loading frozen preprocessor from Block 1...\n",
      "✓ Loaded preprocessor state from realnet_results.pt\n",
      "  Preprocessor frozen with 12,560 params\n",
      "\n",
      "Creating stratified samples...\n",
      "  Train samples: 15000 (stratified, 1500 per class)\n",
      "  Test samples:  3000 (stratified, 300 per class)\n",
      "\n",
      "======================================================================\n",
      "SEED 42\n",
      "======================================================================\n",
      "\n",
      "  Training QuantumNet (WITH entanglement, seed=42)...\n",
      "  [QuantumEnt] Epoch 1/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  1 | loss=1.9970 | test_acc=0.5143 | time=520.5s\n",
      "  [QuantumEnt] Epoch 2/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  2 | loss=1.5236 | test_acc=0.5957 | time=1033.7s\n",
      "  [QuantumEnt] Epoch 3/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  3 | loss=1.2381 | test_acc=0.7057 | time=1564.2s\n",
      "  [QuantumEnt] Epoch 4/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  4 | loss=1.0609 | test_acc=0.7567 | time=2077.4s\n",
      "  [QuantumEnt] Epoch 5/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  5 | loss=0.9364 | test_acc=0.7900 | time=2603.8s\n",
      "  [QuantumEnt] Epoch 6/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  6 | loss=0.8376 | test_acc=0.8163 | time=3138.8s\n",
      "  [QuantumEnt] Epoch 7/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  7 | loss=0.7523 | test_acc=0.8270 | time=3684.2s\n",
      "  [QuantumEnt] Epoch 8/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  8 | loss=0.6782 | test_acc=0.8423 | time=4227.3s\n",
      "  [QuantumEnt] Epoch 9/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  9 | loss=0.6187 | test_acc=0.8413 | time=4775.2s\n",
      "  [QuantumEnt] Epoch 10/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 10 | loss=0.5750 | test_acc=0.8470 | time=5324.7s\n",
      "  [QuantumEnt] Epoch 11/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 11 | loss=0.5424 | test_acc=0.8547 | time=5862.4s\n",
      "  [QuantumEnt] Epoch 12/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 12 | loss=0.5173 | test_acc=0.8567 | time=6406.1s\n",
      "  [QuantumEnt] Epoch 13/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 13 | loss=0.4982 | test_acc=0.8513 | time=6937.6s\n",
      "  [QuantumEnt] Epoch 14/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 14 | loss=0.4828 | test_acc=0.8533 | time=7479.5s\n",
      "  [QuantumEnt] Epoch 15/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 15 | loss=0.4691 | test_acc=0.8567 | time=8017.1s\n",
      "  [QuantumEnt] Epoch 16/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 16 | loss=0.4595 | test_acc=0.8597 | time=8567.7s\n",
      "  [QuantumEnt] Epoch 17/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 17 | loss=0.4508 | test_acc=0.8603 | time=9097.4s\n",
      "  [QuantumEnt] Epoch 18/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 18 | loss=0.4423 | test_acc=0.8627 | time=9639.6s\n",
      "  [QuantumEnt] Epoch 19/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 19 | loss=0.4364 | test_acc=0.8627 | time=10187.6s\n",
      "  [QuantumEnt] Epoch 20/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 20 | loss=0.4301 | test_acc=0.8617 | time=10736.8s\n",
      "  [QuantumEnt] Epoch 21/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 21 | loss=0.4245 | test_acc=0.8627 | time=11297.2s\n",
      "  [QuantumEnt] Epoch 22/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 22 | loss=0.4193 | test_acc=0.8650 | time=11836.5s\n",
      "  [QuantumEnt] Epoch 23/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 23 | loss=0.4151 | test_acc=0.8637 | time=12389.3s\n",
      "  [QuantumEnt] Epoch 24/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 24 | loss=0.4111 | test_acc=0.8653 | time=12921.4s\n",
      "  [QuantumEnt] Epoch 25/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 25 | loss=0.4072 | test_acc=0.8657 | time=13480.7s\n",
      "  [QuantumEnt] Epoch 26/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 26 | loss=0.4040 | test_acc=0.8667 | time=14026.0s\n",
      "  [QuantumEnt] Epoch 27/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 27 | loss=0.4000 | test_acc=0.8687 | time=14573.2s\n",
      "  [QuantumEnt] Epoch 28/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 28 | loss=0.3973 | test_acc=0.8677 | time=15094.7s\n",
      "  [QuantumEnt] Epoch 29/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 29 | loss=0.3938 | test_acc=0.8693 | time=15625.9s\n",
      "  [QuantumEnt] Epoch 30/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 30 | loss=0.3915 | test_acc=0.8703 | time=16165.3s\n",
      "  [QuantumEnt] Epoch 31/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 31 | loss=0.3886 | test_acc=0.8737 | time=16711.1s\n",
      "  [QuantumEnt] Epoch 32/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 32 | loss=0.3857 | test_acc=0.8703 | time=17262.1s\n",
      "  [QuantumEnt] Epoch 33/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 33 | loss=0.3837 | test_acc=0.8710 | time=17797.6s\n",
      "  [QuantumEnt] Epoch 34/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 34 | loss=0.3813 | test_acc=0.8733 | time=18351.7s\n",
      "  [QuantumEnt] Epoch 35/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 35 | loss=0.3784 | test_acc=0.8733 | time=18893.8s\n",
      "  [QuantumEnt] Epoch 36/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 36 | loss=0.3767 | test_acc=0.8767 | time=19421.7s\n",
      "  [QuantumEnt] Epoch 37/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 37 | loss=0.3743 | test_acc=0.8770 | time=19932.1s\n",
      "  [QuantumEnt] Epoch 38/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 38 | loss=0.3724 | test_acc=0.8763 | time=20472.8s\n",
      "  [QuantumEnt] Epoch 39/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 39 | loss=0.3710 | test_acc=0.8737 | time=21001.0s\n",
      "  [QuantumEnt] Epoch 40/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 40 | loss=0.3684 | test_acc=0.8763 | time=21527.0s\n",
      "  [QuantumEnt] Epoch 41/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 41 | loss=0.3676 | test_acc=0.8763 | time=22052.8s\n",
      "  [QuantumEnt] Epoch 42/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 42 | loss=0.3661 | test_acc=0.8743 | time=22577.0s\n",
      "  [QuantumEnt] Epoch 43/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 43 | loss=0.3643 | test_acc=0.8763 | time=23098.4s\n",
      "  [QuantumEnt] Epoch 44/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 44 | loss=0.3631 | test_acc=0.8760 | time=23617.2s\n",
      "  [QuantumEnt] Epoch 45/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 45 | loss=0.3609 | test_acc=0.8730 | time=24135.3s\n",
      "  [QuantumEnt] Epoch 46/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 46 | loss=0.3590 | test_acc=0.8740 | time=24651.6s\n",
      "  [QuantumEnt] Epoch 47/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 47 | loss=0.3594 | test_acc=0.8763 | time=25185.9s\n",
      "  [QuantumEnt] Early stop at epoch 47 (no improvement for 10 epochs)\n",
      "\n",
      "======================================================================\n",
      "SEED 123\n",
      "======================================================================\n",
      "\n",
      "  Training QuantumNet (WITH entanglement, seed=123)...\n",
      "  [QuantumEnt] Epoch 1/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  1 | loss=2.0376 | test_acc=0.3743 | time=535.2s\n",
      "  [QuantumEnt] Epoch 2/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  2 | loss=1.6369 | test_acc=0.5010 | time=1079.7s\n",
      "  [QuantumEnt] Epoch 3/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  3 | loss=1.3871 | test_acc=0.5950 | time=1617.2s\n",
      "  [QuantumEnt] Epoch 4/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  4 | loss=1.1848 | test_acc=0.7070 | time=2165.5s\n",
      "  [QuantumEnt] Epoch 5/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  5 | loss=1.0324 | test_acc=0.7207 | time=2699.1s\n",
      "  [QuantumEnt] Epoch 6/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  6 | loss=0.9249 | test_acc=0.7753 | time=3260.6s\n",
      "  [QuantumEnt] Epoch 7/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  7 | loss=0.8493 | test_acc=0.8047 | time=3802.1s\n",
      "  [QuantumEnt] Epoch 8/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  8 | loss=0.7902 | test_acc=0.8050 | time=4314.4s\n",
      "  [QuantumEnt] Epoch 9/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  9 | loss=0.7387 | test_acc=0.8230 | time=4840.7s\n",
      "  [QuantumEnt] Epoch 10/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 10 | loss=0.6880 | test_acc=0.8347 | time=5382.0s\n",
      "  [QuantumEnt] Epoch 11/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 11 | loss=0.6436 | test_acc=0.8390 | time=5923.6s\n",
      "  [QuantumEnt] Epoch 12/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 12 | loss=0.6080 | test_acc=0.8407 | time=6458.2s\n",
      "  [QuantumEnt] Epoch 13/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 13 | loss=0.5796 | test_acc=0.8403 | time=6995.6s\n",
      "  [QuantumEnt] Epoch 14/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 14 | loss=0.5552 | test_acc=0.8463 | time=7539.1s\n",
      "  [QuantumEnt] Epoch 15/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 15 | loss=0.5351 | test_acc=0.8470 | time=8057.8s\n",
      "  [QuantumEnt] Epoch 16/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 16 | loss=0.5178 | test_acc=0.8483 | time=8599.2s\n",
      "  [QuantumEnt] Epoch 17/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 17 | loss=0.5025 | test_acc=0.8497 | time=9139.4s\n",
      "  [QuantumEnt] Epoch 18/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 18 | loss=0.4891 | test_acc=0.8483 | time=9676.1s\n",
      "  [QuantumEnt] Epoch 19/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 19 | loss=0.4778 | test_acc=0.8560 | time=10227.8s\n",
      "  [QuantumEnt] Epoch 20/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 20 | loss=0.4676 | test_acc=0.8557 | time=10770.1s\n",
      "  [QuantumEnt] Epoch 21/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 21 | loss=0.4580 | test_acc=0.8573 | time=11328.2s\n",
      "  [QuantumEnt] Epoch 22/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 22 | loss=0.4499 | test_acc=0.8593 | time=11871.7s\n",
      "  [QuantumEnt] Epoch 23/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 23 | loss=0.4423 | test_acc=0.8540 | time=12418.5s\n",
      "  [QuantumEnt] Epoch 24/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 24 | loss=0.4359 | test_acc=0.8593 | time=12965.0s\n",
      "  [QuantumEnt] Epoch 25/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 25 | loss=0.4295 | test_acc=0.8590 | time=13524.2s\n",
      "  [QuantumEnt] Epoch 26/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 26 | loss=0.4239 | test_acc=0.8617 | time=14057.6s\n",
      "  [QuantumEnt] Epoch 27/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 27 | loss=0.4181 | test_acc=0.8610 | time=14609.4s\n",
      "  [QuantumEnt] Epoch 28/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 28 | loss=0.4140 | test_acc=0.8600 | time=15163.5s\n",
      "  [QuantumEnt] Epoch 29/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 29 | loss=0.4087 | test_acc=0.8637 | time=15700.6s\n",
      "  [QuantumEnt] Epoch 30/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 30 | loss=0.4042 | test_acc=0.8623 | time=16263.8s\n",
      "  [QuantumEnt] Epoch 31/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 31 | loss=0.4005 | test_acc=0.8640 | time=16805.0s\n",
      "  [QuantumEnt] Epoch 32/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 32 | loss=0.3981 | test_acc=0.8647 | time=17335.9s\n",
      "  [QuantumEnt] Epoch 33/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 33 | loss=0.3933 | test_acc=0.8647 | time=17880.8s\n",
      "  [QuantumEnt] Epoch 34/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 34 | loss=0.3908 | test_acc=0.8657 | time=18436.5s\n",
      "  [QuantumEnt] Epoch 35/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 35 | loss=0.3875 | test_acc=0.8637 | time=18961.0s\n",
      "  [QuantumEnt] Epoch 36/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 36 | loss=0.3852 | test_acc=0.8673 | time=19510.5s\n",
      "  [QuantumEnt] Epoch 37/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 37 | loss=0.3827 | test_acc=0.8660 | time=20033.4s\n",
      "  [QuantumEnt] Epoch 38/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 38 | loss=0.3804 | test_acc=0.8653 | time=20585.5s\n",
      "  [QuantumEnt] Epoch 39/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 39 | loss=0.3767 | test_acc=0.8690 | time=21129.5s\n",
      "  [QuantumEnt] Epoch 40/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 40 | loss=0.3756 | test_acc=0.8677 | time=21695.8s\n",
      "  [QuantumEnt] Epoch 41/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 41 | loss=0.3732 | test_acc=0.8700 | time=22246.2s\n",
      "  [QuantumEnt] Epoch 42/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 42 | loss=0.3713 | test_acc=0.8683 | time=22786.3s\n",
      "  [QuantumEnt] Epoch 43/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 43 | loss=0.3692 | test_acc=0.8667 | time=23332.8s\n",
      "  [QuantumEnt] Epoch 44/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 44 | loss=0.3679 | test_acc=0.8730 | time=23886.3s\n",
      "  [QuantumEnt] Epoch 45/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 45 | loss=0.3655 | test_acc=0.8723 | time=24417.6s\n",
      "  [QuantumEnt] Epoch 46/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 46 | loss=0.3641 | test_acc=0.8703 | time=24971.3s\n",
      "  [QuantumEnt] Epoch 47/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 47 | loss=0.3635 | test_acc=0.8690 | time=25516.8s\n",
      "  [QuantumEnt] Epoch 48/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 48 | loss=0.3616 | test_acc=0.8707 | time=26062.3s\n",
      "  [QuantumEnt] Epoch 49/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 49 | loss=0.3591 | test_acc=0.8727 | time=26593.6s\n",
      "  [QuantumEnt] Epoch 50/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 50 | loss=0.3587 | test_acc=0.8730 | time=27153.3s\n",
      "  [QuantumEnt] Epoch 51/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 51 | loss=0.3575 | test_acc=0.8720 | time=27687.8s\n",
      "  [QuantumEnt] Epoch 52/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 52 | loss=0.3558 | test_acc=0.8757 | time=28226.3s\n",
      "  [QuantumEnt] Epoch 53/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 53 | loss=0.3545 | test_acc=0.8737 | time=28778.2s\n",
      "  [QuantumEnt] Epoch 54/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 54 | loss=0.3533 | test_acc=0.8727 | time=29345.5s\n",
      "  [QuantumEnt] Epoch 55/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 55 | loss=0.3525 | test_acc=0.8760 | time=29903.2s\n",
      "  [QuantumEnt] Epoch 56/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 56 | loss=0.3518 | test_acc=0.8743 | time=30430.5s\n",
      "  [QuantumEnt] Epoch 57/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 57 | loss=0.3503 | test_acc=0.8737 | time=30966.3s\n",
      "  [QuantumEnt] Epoch 58/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 58 | loss=0.3482 | test_acc=0.8747 | time=31501.7s\n",
      "  [QuantumEnt] Epoch 59/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 59 | loss=0.3485 | test_acc=0.8763 | time=32059.4s\n",
      "  [QuantumEnt] Epoch 60/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 60 | loss=0.3466 | test_acc=0.8773 | time=32587.9s\n",
      "  [QuantumEnt] Epoch 61/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 61 | loss=0.3456 | test_acc=0.8747 | time=33122.1s\n",
      "  [QuantumEnt] Epoch 62/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 62 | loss=0.3454 | test_acc=0.8757 | time=33660.5s\n",
      "  [QuantumEnt] Epoch 63/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 63 | loss=0.3441 | test_acc=0.8743 | time=34214.8s\n",
      "  [QuantumEnt] Epoch 64/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 64 | loss=0.3429 | test_acc=0.8767 | time=34750.7s\n",
      "  [QuantumEnt] Epoch 65/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 65 | loss=0.3432 | test_acc=0.8757 | time=35293.2s\n",
      "  [QuantumEnt] Epoch 66/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 66 | loss=0.3411 | test_acc=0.8763 | time=35826.8s\n",
      "  [QuantumEnt] Epoch 67/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 67 | loss=0.3408 | test_acc=0.8763 | time=36378.2s\n",
      "  [QuantumEnt] Epoch 68/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 68 | loss=0.3396 | test_acc=0.8770 | time=36931.9s\n",
      "  [QuantumEnt] Epoch 69/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 69 | loss=0.3387 | test_acc=0.8770 | time=37485.1s\n",
      "  [QuantumEnt] Epoch 70/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 70 | loss=0.3384 | test_acc=0.8750 | time=38034.2s\n",
      "  [QuantumEnt] Early stop at epoch 70 (no improvement for 10 epochs)\n",
      "\n",
      "======================================================================\n",
      "SEED 456\n",
      "======================================================================\n",
      "\n",
      "  Training QuantumNet (WITH entanglement, seed=456)...\n",
      "  [QuantumEnt] Epoch 1/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  1 | loss=2.0081 | test_acc=0.4987 | time=547.1s\n",
      "  [QuantumEnt] Epoch 2/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  2 | loss=1.4869 | test_acc=0.6170 | time=1078.1s\n",
      "  [QuantumEnt] Epoch 3/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  3 | loss=1.2132 | test_acc=0.7277 | time=1617.7s\n",
      "  [QuantumEnt] Epoch 4/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  4 | loss=1.0601 | test_acc=0.7923 | time=2153.3s\n",
      "  [QuantumEnt] Epoch 5/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  5 | loss=0.9475 | test_acc=0.8117 | time=2693.1s\n",
      "  [QuantumEnt] Epoch 6/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  6 | loss=0.8490 | test_acc=0.8247 | time=3233.5s\n",
      "  [QuantumEnt] Epoch 7/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  7 | loss=0.7600 | test_acc=0.8347 | time=3783.6s\n",
      "  [QuantumEnt] Epoch 8/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  8 | loss=0.6870 | test_acc=0.8440 | time=4316.6s\n",
      "  [QuantumEnt] Epoch 9/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  9 | loss=0.6299 | test_acc=0.8390 | time=4879.3s\n",
      "  [QuantumEnt] Epoch 10/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 10 | loss=0.5854 | test_acc=0.8477 | time=5413.3s\n",
      "  [QuantumEnt] Epoch 11/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 11 | loss=0.5496 | test_acc=0.8503 | time=5983.9s\n",
      "  [QuantumEnt] Epoch 12/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 12 | loss=0.5218 | test_acc=0.8540 | time=6522.3s\n",
      "  [QuantumEnt] Epoch 13/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 13 | loss=0.4982 | test_acc=0.8587 | time=7089.3s\n",
      "  [QuantumEnt] Epoch 14/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 14 | loss=0.4789 | test_acc=0.8603 | time=7627.8s\n",
      "  [QuantumEnt] Epoch 15/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 15 | loss=0.4625 | test_acc=0.8597 | time=8171.6s\n",
      "  [QuantumEnt] Epoch 16/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 16 | loss=0.4480 | test_acc=0.8633 | time=8735.2s\n",
      "  [QuantumEnt] Epoch 17/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 17 | loss=0.4370 | test_acc=0.8683 | time=9299.1s\n",
      "  [QuantumEnt] Epoch 18/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 18 | loss=0.4261 | test_acc=0.8667 | time=9837.0s\n",
      "  [QuantumEnt] Epoch 19/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 19 | loss=0.4169 | test_acc=0.8707 | time=10376.5s\n",
      "  [QuantumEnt] Epoch 20/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 20 | loss=0.4093 | test_acc=0.8657 | time=10910.7s\n",
      "  [QuantumEnt] Epoch 21/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 21 | loss=0.4030 | test_acc=0.8713 | time=11453.3s\n",
      "  [QuantumEnt] Epoch 22/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 22 | loss=0.3983 | test_acc=0.8687 | time=12003.8s\n",
      "  [QuantumEnt] Epoch 23/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 23 | loss=0.3923 | test_acc=0.8713 | time=12561.7s\n",
      "  [QuantumEnt] Epoch 24/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 24 | loss=0.3876 | test_acc=0.8763 | time=13103.4s\n",
      "  [QuantumEnt] Epoch 25/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 25 | loss=0.3838 | test_acc=0.8750 | time=13641.8s\n",
      "  [QuantumEnt] Epoch 26/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 26 | loss=0.3805 | test_acc=0.8680 | time=14189.4s\n",
      "  [QuantumEnt] Epoch 27/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 27 | loss=0.3765 | test_acc=0.8727 | time=14720.7s\n",
      "  [QuantumEnt] Epoch 28/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 28 | loss=0.3734 | test_acc=0.8727 | time=15271.6s\n",
      "  [QuantumEnt] Epoch 29/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 29 | loss=0.3705 | test_acc=0.8740 | time=15808.5s\n",
      "  [QuantumEnt] Epoch 30/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 30 | loss=0.3670 | test_acc=0.8777 | time=16362.5s\n",
      "  [QuantumEnt] Epoch 31/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 31 | loss=0.3648 | test_acc=0.8753 | time=16890.8s\n",
      "  [QuantumEnt] Epoch 32/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 32 | loss=0.3621 | test_acc=0.8757 | time=17419.7s\n",
      "  [QuantumEnt] Epoch 33/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 33 | loss=0.3598 | test_acc=0.8777 | time=17941.2s\n",
      "  [QuantumEnt] Epoch 34/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 34 | loss=0.3582 | test_acc=0.8733 | time=18457.1s\n",
      "  [QuantumEnt] Epoch 35/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 35 | loss=0.3550 | test_acc=0.8747 | time=18989.6s\n",
      "  [QuantumEnt] Epoch 36/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 36 | loss=0.3541 | test_acc=0.8787 | time=19529.6s\n",
      "  [QuantumEnt] Epoch 37/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 37 | loss=0.3515 | test_acc=0.8760 | time=20062.7s\n",
      "  [QuantumEnt] Epoch 38/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 38 | loss=0.3509 | test_acc=0.8803 | time=20602.6s\n",
      "  [QuantumEnt] Epoch 39/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 39 | loss=0.3480 | test_acc=0.8787 | time=21138.7s\n",
      "  [QuantumEnt] Epoch 40/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 40 | loss=0.3471 | test_acc=0.8783 | time=21674.9s\n",
      "  [QuantumEnt] Epoch 41/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 41 | loss=0.3452 | test_acc=0.8797 | time=22200.9s\n",
      "  [QuantumEnt] Epoch 42/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 42 | loss=0.3425 | test_acc=0.8810 | time=22716.9s\n",
      "  [QuantumEnt] Epoch 43/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 43 | loss=0.3421 | test_acc=0.8837 | time=23245.8s\n",
      "  [QuantumEnt] Epoch 44/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 44 | loss=0.3399 | test_acc=0.8800 | time=23772.4s\n",
      "  [QuantumEnt] Epoch 45/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 45 | loss=0.3381 | test_acc=0.8800 | time=24300.7s\n",
      "  [QuantumEnt] Epoch 46/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 46 | loss=0.3368 | test_acc=0.8803 | time=24817.6s\n",
      "  [QuantumEnt] Epoch 47/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 47 | loss=0.3355 | test_acc=0.8827 | time=25339.8s\n",
      "  [QuantumEnt] Epoch 48/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 48 | loss=0.3336 | test_acc=0.8837 | time=25858.0s\n",
      "  [QuantumEnt] Epoch 49/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 49 | loss=0.3330 | test_acc=0.8820 | time=26386.0s\n",
      "  [QuantumEnt] Epoch 50/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 50 | loss=0.3316 | test_acc=0.8790 | time=26899.7s\n",
      "  [QuantumEnt] Epoch 51/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 51 | loss=0.3299 | test_acc=0.8817 | time=27419.2s\n",
      "  [QuantumEnt] Epoch 52/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 52 | loss=0.3281 | test_acc=0.8867 | time=27940.9s\n",
      "  [QuantumEnt] Epoch 53/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 53 | loss=0.3284 | test_acc=0.8850 | time=28458.3s\n",
      "  [QuantumEnt] Epoch 54/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 54 | loss=0.3270 | test_acc=0.8813 | time=28975.5s\n",
      "  [QuantumEnt] Epoch 55/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 55 | loss=0.3252 | test_acc=0.8840 | time=29509.5s\n",
      "  [QuantumEnt] Epoch 56/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 56 | loss=0.3243 | test_acc=0.8850 | time=30038.0s\n",
      "  [QuantumEnt] Epoch 57/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 57 | loss=0.3230 | test_acc=0.8837 | time=30572.3s\n",
      "  [QuantumEnt] Epoch 58/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 58 | loss=0.3212 | test_acc=0.8850 | time=31107.3s\n",
      "  [QuantumEnt] Epoch 59/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 59 | loss=0.3206 | test_acc=0.8883 | time=31635.3s\n",
      "  [QuantumEnt] Epoch 60/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 60 | loss=0.3194 | test_acc=0.8833 | time=32155.9s\n",
      "  [QuantumEnt] Epoch 61/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 61 | loss=0.3182 | test_acc=0.8830 | time=32671.7s\n",
      "  [QuantumEnt] Epoch 62/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 62 | loss=0.3174 | test_acc=0.8867 | time=33193.1s\n",
      "  [QuantumEnt] Epoch 63/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 63 | loss=0.3164 | test_acc=0.8850 | time=33711.1s\n",
      "  [QuantumEnt] Epoch 64/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 64 | loss=0.3156 | test_acc=0.8843 | time=34232.3s\n",
      "  [QuantumEnt] Epoch 65/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 65 | loss=0.3152 | test_acc=0.8847 | time=34755.6s\n",
      "  [QuantumEnt] Epoch 66/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 66 | loss=0.3128 | test_acc=0.8883 | time=35282.9s\n",
      "  [QuantumEnt] Epoch 67/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 67 | loss=0.3127 | test_acc=0.8843 | time=35810.0s\n",
      "  [QuantumEnt] Epoch 68/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 68 | loss=0.3122 | test_acc=0.8823 | time=36378.5s\n",
      "  [QuantumEnt] Epoch 69/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 69 | loss=0.3112 | test_acc=0.8893 | time=36962.7s\n",
      "  [QuantumEnt] Epoch 70/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 70 | loss=0.3102 | test_acc=0.8877 | time=37524.0s\n",
      "  [QuantumEnt] Epoch 71/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 71 | loss=0.3094 | test_acc=0.8863 | time=38093.0s\n",
      "  [QuantumEnt] Epoch 72/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 72 | loss=0.3092 | test_acc=0.8883 | time=38660.5s\n",
      "  [QuantumEnt] Epoch 73/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 73 | loss=0.3076 | test_acc=0.8827 | time=39240.2s\n",
      "  [QuantumEnt] Epoch 74/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 74 | loss=0.3076 | test_acc=0.8890 | time=39813.0s\n",
      "  [QuantumEnt] Epoch 75/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 75 | loss=0.3055 | test_acc=0.8877 | time=40344.2s\n",
      "  [QuantumEnt] Epoch 76/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 76 | loss=0.3062 | test_acc=0.8903 | time=40896.1s\n",
      "  [QuantumEnt] Epoch 77/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 77 | loss=0.3045 | test_acc=0.8853 | time=41440.0s\n",
      "  [QuantumEnt] Epoch 78/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 78 | loss=0.3040 | test_acc=0.8873 | time=41988.5s\n",
      "  [QuantumEnt] Epoch 79/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 79 | loss=0.3043 | test_acc=0.8890 | time=42536.4s\n",
      "  [QuantumEnt] Epoch 80/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 80 | loss=0.3030 | test_acc=0.8873 | time=43080.0s\n",
      "  [QuantumEnt] Epoch 81/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 81 | loss=0.3023 | test_acc=0.8883 | time=43628.0s\n",
      "  [QuantumEnt] Epoch 82/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 82 | loss=0.3016 | test_acc=0.8890 | time=44158.6s\n",
      "  [QuantumEnt] Epoch 83/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 83 | loss=0.3007 | test_acc=0.8873 | time=44687.5s\n",
      "  [QuantumEnt] Epoch 84/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 84 | loss=0.3001 | test_acc=0.8867 | time=45207.5s\n",
      "  [QuantumEnt] Epoch 85/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 85 | loss=0.2993 | test_acc=0.8880 | time=45732.0s\n",
      "  [QuantumEnt] Epoch 86/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 86 | loss=0.2989 | test_acc=0.8877 | time=46255.2s\n",
      "  [QuantumEnt] Early stop at epoch 86 (no improvement for 10 epochs)\n",
      "\n",
      "======================================================================\n",
      "QUANTUM (WITH ENTANGLEMENT) SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Accuracy:   0.8816 ± 0.0062\n",
      "Time:       36491.7s ± 8670.4s\n",
      "Epochs:     67.7 ± 16.0\n",
      "Parameters: 162 trainable (head), 12,722 total\n",
      "\n",
      "Per-seed results:\n",
      "  Seed 42: acc=0.8770, time=25185.9s, epochs=47\n",
      "  Seed 123: acc=0.8773, time=38034.2s, epochs=70\n",
      "  Seed 456: acc=0.8903, time=46255.2s, epochs=86\n",
      "\n",
      "✓ Saved results to: quantum_ent_results.pt\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Block 4: Quantum (WITH Entanglement) Training\n",
    "==============================================\n",
    "Loads frozen preprocessor from Block 1 and trains quantum head WITH entanglement.\n",
    "Uses Lightning-GPU acceleration.\n",
    "\n",
    "Requirements:\n",
    "- realnet_results.pt (from Block 1)\n",
    "- pennylane, pennylane-lightning-gpu\n",
    "\n",
    "Outputs:\n",
    "- quantum_ent_results.pt: Contains quantum (with ent) results\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from collections import defaultdict\n",
    "\n",
    "try:\n",
    "    import pennylane as qml\n",
    "    QUANTUM_DEVICE = \"lightning.gpu\"\n",
    "    PENNYLANE_AVAILABLE = True\n",
    "    print(\"✓ Using lightning.gpu device\")\n",
    "except ImportError:\n",
    "    PENNYLANE_AVAILABLE = False\n",
    "    print(\"✗ PennyLane not installed\")\n",
    "    exit(1)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "print(f\"Using PyTorch device: {device}\")\n",
    "\n",
    "# ============================================\n",
    "# Comprehensive seed setting for reproducibility\n",
    "# ============================================\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    \"\"\"Set seeds for all RNG sources for reproducibility\"\"\"\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    # Python\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # NumPy\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # PyTorch deterministic mode\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # CuPy (if available)\n",
    "    try:\n",
    "        import cupy as cp\n",
    "        cp.random.seed(seed)\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    # PennyLane (if available)\n",
    "    try:\n",
    "        import pennylane as qml\n",
    "        qml.numpy.random.seed(seed)\n",
    "    except (ImportError, AttributeError):\n",
    "        pass\n",
    "\n",
    "# ============================================\n",
    "# Shared Preprocessor (must match Block 1)\n",
    "# ============================================\n",
    "\n",
    "class SharedPreprocessor(nn.Module):\n",
    "    \"\"\"Shared classical feature extractor: 784 → 16\"\"\"\n",
    "    def __init__(self, input_dim=784, bottleneck_dim=16):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, bottleneck_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.tanh(self.fc(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Quantum Head (WITH Entanglement)\n",
    "# ============================================\n",
    "\n",
    "class QuantumHead(nn.Module):\n",
    "    \"\"\"\n",
    "    VQC with 4 qubits, 3 layers, WITH entanglement → 10 classes.\n",
    "    Uses Lightning acceleration (GPU).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_qubits=4, n_layers=3, num_classes=10, device_name=None):\n",
    "        super().__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Map 16 features → n_qubits\n",
    "        self.feature_select = nn.Linear(16, n_qubits)\n",
    "\n",
    "        # Use specified device or default\n",
    "        if device_name is None:\n",
    "            device_name = QUANTUM_DEVICE\n",
    "        \n",
    "        # Quantum device\n",
    "        self.dev = qml.device(device_name, wires=n_qubits)\n",
    "\n",
    "        # Use adjoint differentiation for lightning (much faster)\n",
    "        diff_method = \"adjoint\" if \"lightning\" in device_name else \"parameter-shift\"\n",
    "\n",
    "        @qml.qnode(self.dev, interface=\"torch\", diff_method=diff_method)\n",
    "        def quantum_circuit(inputs, weights):\n",
    "            \"\"\"\n",
    "            Single-sample circuit WITH entanglement.\n",
    "            inputs: (n_qubits,)\n",
    "            weights: (n_layers, n_qubits, 2)\n",
    "            \"\"\"\n",
    "            for layer in range(n_layers):\n",
    "                # Data re-uploading\n",
    "                for i in range(n_qubits):\n",
    "                    qml.RY(inputs[i], wires=i)\n",
    "\n",
    "                # Trainable rotations\n",
    "                for i in range(n_qubits):\n",
    "                    qml.RY(weights[layer, i, 0], wires=i)\n",
    "                    qml.RZ(weights[layer, i, 1], wires=i)\n",
    "\n",
    "                # ENTANGLEMENT: CNOT ring\n",
    "                for i in range(n_qubits - 1):\n",
    "                    qml.CNOT(wires=[i, i + 1])\n",
    "                if n_qubits > 2:\n",
    "                    qml.CNOT(wires=[n_qubits - 1, 0])\n",
    "\n",
    "            return [\n",
    "                qml.expval(qml.PauliZ(0)),\n",
    "                qml.expval(qml.PauliZ(1)),\n",
    "                qml.expval(qml.PauliZ(2)),\n",
    "                qml.expval(qml.PauliZ(3)),\n",
    "                qml.expval(qml.PauliZ(0) @ qml.PauliZ(1)),\n",
    "                qml.expval(qml.PauliZ(2) @ qml.PauliZ(3))\n",
    "            ]\n",
    "\n",
    "        self.quantum_circuit = quantum_circuit\n",
    "\n",
    "        weight_shape = (n_layers, n_qubits, 2)\n",
    "        self.q_weights = nn.Parameter(torch.randn(weight_shape) * 0.1)\n",
    "        self.fc_out = nn.Linear(6, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Process samples with progress tracking.\n",
    "        x: (batch, 16) real bottleneck features\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        x = torch.tanh(self.feature_select(x))\n",
    "\n",
    "        # Process in chunks for memory management\n",
    "        chunk_size = 32\n",
    "        quantum_outputs = []\n",
    "\n",
    "        for start_idx in range(0, batch_size, chunk_size):\n",
    "            end_idx = min(start_idx + chunk_size, batch_size)\n",
    "            chunk = x[start_idx:end_idx]\n",
    "\n",
    "            chunk_outputs = []\n",
    "            for i in range(chunk.size(0)):\n",
    "                q_raw = self.quantum_circuit(chunk[i], self.q_weights)\n",
    "                if isinstance(q_raw, (list, tuple)):\n",
    "                    q_out = torch.stack(q_raw)\n",
    "                else:\n",
    "                    q_out = q_raw\n",
    "                chunk_outputs.append(q_out)\n",
    "\n",
    "            quantum_outputs.extend(chunk_outputs)\n",
    "\n",
    "        # Convert to tensor (cast to float32)\n",
    "        quantum_outputs = torch.stack(quantum_outputs).float()\n",
    "        quantum_outputs = quantum_outputs.to(self.fc_out.weight.dtype)\n",
    "\n",
    "        output = self.fc_out(quantum_outputs)\n",
    "        return output\n",
    "\n",
    "\n",
    "class QuantumNet(nn.Module):\n",
    "    \"\"\"Complete Quantum network: Preprocessor + QuantumHead (with ent)\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.preprocessor = SharedPreprocessor(784, 16)\n",
    "        self.head = QuantumHead(n_qubits=4, n_layers=3, num_classes=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.preprocessor(x)\n",
    "        return self.head(features)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Training and Evaluation\n",
    "# ============================================\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, show_progress=True):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(loader):\n",
    "        # Keep data on CPU for quantum models\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        total_samples += x.size(0)\n",
    "\n",
    "        if show_progress and batch_idx % 20 == 0:\n",
    "            print(f\"    Batch {batch_idx}/{len(loader)}, samples: {total_samples}\", end=\"\\r\")\n",
    "\n",
    "    if show_progress:\n",
    "        print()\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            # Keep data on CPU\n",
    "            logits = model(x)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += x.size(0)\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "def train_with_early_stopping(model, train_loader, test_loader, optimizer,\n",
    "                              max_epochs=200, patience=10, name=\"Model\"):\n",
    "    best_acc = 0.0\n",
    "    epochs_without_improvement = 0\n",
    "    start = time.time()\n",
    "    last_acc = 0.0\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        print(f\"  [{name}] Epoch {epoch}/{max_epochs}\")\n",
    "        loss = train_one_epoch(model, train_loader, optimizer, show_progress=True)\n",
    "        acc = evaluate(model, test_loader)\n",
    "        last_acc = acc\n",
    "\n",
    "        elapsed = time.time() - start\n",
    "        print(f\"  [{name}] Epoch {epoch:2d} | loss={loss:.4f} \"\n",
    "              f\"| test_acc={acc:.4f} | time={elapsed:.1f}s\")\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"  [{name}] Early stop at epoch {epoch} \"\n",
    "                  f\"(no improvement for {patience} epochs)\")\n",
    "            break\n",
    "\n",
    "    total_time = time.time() - start\n",
    "    return {\n",
    "        \"best_acc\": best_acc,\n",
    "        \"final_acc\": last_acc,\n",
    "        \"time\": total_time,\n",
    "        \"epochs\": epoch,\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Data utilities\n",
    "# ============================================\n",
    "\n",
    "def stratified_sample(dataset, n_samples_per_class):\n",
    "    \"\"\"\n",
    "    Create a stratified sample with n_samples_per_class from each class.\n",
    "    Maintains class balance.\n",
    "    \"\"\"\n",
    "    # Group indices by class\n",
    "    class_indices = defaultdict(list)\n",
    "    for idx, (_, label) in enumerate(dataset):\n",
    "        class_indices[label].append(idx)\n",
    "    \n",
    "    # Sample from each class\n",
    "    sampled_indices = []\n",
    "    for class_label in sorted(class_indices.keys()):\n",
    "        indices = class_indices[class_label]\n",
    "        # Use fixed random seed for reproducibility\n",
    "        rng = np.random.RandomState(42)\n",
    "        selected = rng.choice(indices, size=min(n_samples_per_class, len(indices)), \n",
    "                             replace=False)\n",
    "        sampled_indices.extend(selected)\n",
    "    \n",
    "    return sampled_indices\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Main\n",
    "# ============================================\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"BLOCK 4: Quantum (WITH Entanglement) Training\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nConfiguration:\")\n",
    "    print(\"  • Batch size: 32\")\n",
    "    print(\"  • Patience: 10\")\n",
    "    print(\"  • Max epochs: 200\")\n",
    "    print(\"  • Seeds: [42, 123, 456]\")\n",
    "    print(\"  • Architecture: [frozen 784→16] → 4 qubits (3 layers, WITH ent) → 10\")\n",
    "    print(f\"  • Quantum device: {QUANTUM_DEVICE}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Load frozen preprocessor from Block 1\n",
    "    print(\"\\nLoading frozen preprocessor from Block 1...\")\n",
    "    try:\n",
    "        realnet_data = torch.load(\"realnet_results.pt\", weights_only=False)\n",
    "        preprocessor_state = realnet_data[\"preprocessor_state\"]\n",
    "        print(\"✓ Loaded preprocessor state from realnet_results.pt\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"✗ ERROR: realnet_results.pt not found!\")\n",
    "        print(\"  Run Block 1 first to train RealNet and save preprocessor.\")\n",
    "        return\n",
    "\n",
    "    # Create frozen preprocessor (CPU for quantum)\n",
    "    shared_preprocessor = SharedPreprocessor(784, 16)\n",
    "    shared_preprocessor.load_state_dict(preprocessor_state)\n",
    "    for p in shared_preprocessor.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    preprocessor_params = sum(p.numel() for p in shared_preprocessor.parameters())\n",
    "    print(f\"  Preprocessor frozen with {preprocessor_params:,} params\")\n",
    "\n",
    "    # Load data\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    full_train_ds = datasets.MNIST(root=\"./data\", train=True,\n",
    "                                   download=True, transform=transform)\n",
    "    full_test_ds = datasets.MNIST(root=\"./data\", train=False,\n",
    "                                  download=True, transform=transform)\n",
    "\n",
    "    # Create stratified samples (same as Block 1)\n",
    "    print(\"\\nCreating stratified samples...\")\n",
    "    train_indices = stratified_sample(full_train_ds, n_samples_per_class=1500)\n",
    "    test_indices = stratified_sample(full_test_ds, n_samples_per_class=300)\n",
    "    \n",
    "    train_ds = Subset(full_train_ds, train_indices)\n",
    "    test_ds = Subset(full_test_ds, test_indices)\n",
    "    \n",
    "    print(f\"  Train samples: {len(train_ds)} (stratified, 1500 per class)\")\n",
    "    print(f\"  Test samples:  {len(test_ds)} (stratified, 300 per class)\")\n",
    "\n",
    "    # Smaller batches for quantum\n",
    "    train_loader = DataLoader(train_ds, batch_size=32,\n",
    "                              shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_ds, batch_size=64,\n",
    "                             shuffle=False, num_workers=0)\n",
    "\n",
    "    seeds = [42, 123, 456]\n",
    "    all_results = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(f\"SEED {seed}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        set_all_seeds(seed)\n",
    "\n",
    "        print(f\"\\n  Training QuantumNet (WITH entanglement, seed={seed})...\")\n",
    "        quantum_model = QuantumNet()\n",
    "        quantum_model.preprocessor = shared_preprocessor  # Use frozen preprocessor\n",
    "        \n",
    "        quantum_opt = torch.optim.Adam(quantum_model.head.parameters(), lr=1e-3)\n",
    "        \n",
    "        result = train_with_early_stopping(\n",
    "            quantum_model, train_loader, test_loader, quantum_opt,\n",
    "            max_epochs=200, patience=10, name=\"QuantumEnt\"\n",
    "        )\n",
    "        \n",
    "        result[\"trainable_params\"] = sum(p.numel() for p in quantum_model.parameters()\n",
    "                                         if p.requires_grad)\n",
    "        result[\"total_params\"] = sum(p.numel() for p in quantum_model.parameters())\n",
    "        result[\"seed\"] = seed\n",
    "        \n",
    "        all_results.append(result)\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"QUANTUM (WITH ENTANGLEMENT) SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    accs = [r[\"best_acc\"] for r in all_results]\n",
    "    times = [r[\"time\"] for r in all_results]\n",
    "    epochs = [r[\"epochs\"] for r in all_results]\n",
    "    trainable = all_results[0][\"trainable_params\"]\n",
    "    total = all_results[0][\"total_params\"]\n",
    "    \n",
    "    print(f\"\\nAccuracy:   {np.mean(accs):.4f} ± {np.std(accs):.4f}\")\n",
    "    print(f\"Time:       {np.mean(times):.1f}s ± {np.std(times):.1f}s\")\n",
    "    print(f\"Epochs:     {np.mean(epochs):.1f} ± {np.std(epochs):.1f}\")\n",
    "    print(f\"Parameters: {trainable:,} trainable (head), {total:,} total\")\n",
    "    \n",
    "    print(\"\\nPer-seed results:\")\n",
    "    for r in all_results:\n",
    "        print(f\"  Seed {r['seed']}: acc={r['best_acc']:.4f}, \"\n",
    "              f\"time={r['time']:.1f}s, epochs={r['epochs']}\")\n",
    "\n",
    "    # Save results\n",
    "    save_dict = {\n",
    "        \"results\": all_results,\n",
    "        \"summary\": {\n",
    "            \"mean_acc\": np.mean(accs),\n",
    "            \"std_acc\": np.std(accs),\n",
    "            \"mean_time\": np.mean(times),\n",
    "            \"trainable_params\": trainable,\n",
    "            \"total_params\": total\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    torch.save(save_dict, \"quantum_ent_results.pt\")\n",
    "    print(f\"\\n✓ Saved results to: quantum_ent_results.pt\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99552865-c4be-4e4e-894f-0a06ea8c9020",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59110581-a0ef-4b42-ac34-ba8cfc5c98ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "BLOCK 5: AGGREGATE RESULTS AND COMPARATIVE ANALYSIS\n",
      "==========================================================================================\n",
      "✓ Loaded RealNet results\n",
      "✓ Loaded QuatNet results\n",
      "✓ Loaded Quantum (no ent) results\n",
      "✓ Loaded Quantum (with ent) results\n",
      "\n",
      "==========================================================================================\n",
      "AGGREGATED RESULTS (mean ± std over 3 seeds)\n",
      "==========================================================================================\n",
      "Model           Accuracy             Time (s)             Epochs          Parameters          \n",
      "------------------------------------------------------------------------------------------\n",
      "Real            0.9354 ± 0.0052      30.0 ± 5.9           21.3 ± 4.5      14,298              \n",
      "Quat            0.9364 ± 0.0015      77.4 ± 6.5           31.7 ± 2.4      1,000 (head)        \n",
      "QNoEnt          0.8752 ± 0.0102      24601.3 ± 7891.5     51.3 ± 16.4     162 (head)          \n",
      "QEnt            0.8816 ± 0.0062      36491.7 ± 8670.4     67.7 ± 16.0     162 (head)          \n",
      "==========================================================================================\n",
      "\n",
      "======================================================================\n",
      "PER-SEED RESULTS\n",
      "======================================================================\n",
      "\n",
      "Seed 42:\n",
      "Model           Accuracy     Time (s)     Epochs    \n",
      "--------------------------------------------------\n",
      "Real            0.9293       21.7         15        \n",
      "Quat            0.9383       74.6         30        \n",
      "QNoEnt          0.8897       27507.6      57        \n",
      "QEnt            0.8770       25185.9      47        \n",
      "\n",
      "Seed 123:\n",
      "Model           Accuracy     Time (s)     Epochs    \n",
      "--------------------------------------------------\n",
      "Real            0.9420       33.1         24        \n",
      "Quat            0.9347       86.4         35        \n",
      "QNoEnt          0.8670       32479.8      68        \n",
      "QEnt            0.8773       38034.2      70        \n",
      "\n",
      "Seed 456:\n",
      "Model           Accuracy     Time (s)     Epochs    \n",
      "--------------------------------------------------\n",
      "Real            0.9350       35.3         25        \n",
      "Quat            0.9363       71.4         30        \n",
      "QNoEnt          0.8690       13816.6      29        \n",
      "QEnt            0.8903       46255.2      86        \n",
      "\n",
      "==========================================================================================\n",
      "COMPARATIVE ANALYSIS: Answering Research Questions\n",
      "==========================================================================================\n",
      "\n",
      "1. QUATERNION vs REAL MLP:\n",
      "   ----------------------------------------------------------------------\n",
      "   Real MLP accuracy:        0.9354 ± 0.0052\n",
      "   Quaternion accuracy:      0.9364 ± 0.0015\n",
      "   Gap:                      -0.10 percentage points\n",
      "   Performance retention:    100.1%\n",
      "\n",
      "   → Classical SU(2) (quaternions) captures 100.1% of\n",
      "     standard MLP performance with structured algebraic constraints.\n",
      "\n",
      "2. QUANTUM (no entanglement) vs REAL MLP:\n",
      "   ----------------------------------------------------------------------\n",
      "   Real MLP accuracy:        0.9354 ± 0.0052\n",
      "   Quantum (no ent) accuracy:0.8752 ± 0.0102\n",
      "   Gap:                      6.02 percentage points\n",
      "   Performance retention:    93.6%\n",
      "\n",
      "   → Quantum circuits WITHOUT entanglement capture 93.6%\n",
      "     of MLP performance, suggesting limited benefit over classical rotation gates.\n",
      "\n",
      "3. QUANTUM (with entanglement) vs REAL MLP:\n",
      "   ----------------------------------------------------------------------\n",
      "   Real MLP accuracy:        0.9354 ± 0.0052\n",
      "   Quantum (with ent) accuracy:0.8816 ± 0.0062\n",
      "   Gap:                      5.39 percentage points\n",
      "   Performance retention:    94.2%\n",
      "\n",
      "   → Quantum circuits WITH entanglement capture 94.2%\n",
      "     of MLP performance.\n",
      "\n",
      "4. ENTANGLEMENT EFFECT:\n",
      "   ----------------------------------------------------------------------\n",
      "   Quantum (no ent) accuracy:0.8752 ± 0.0102\n",
      "   Quantum (with ent) accuracy:0.8816 ± 0.0062\n",
      "   Improvement:              0.63 percentage points\n",
      "\n",
      "   → Entanglement IMPROVES performance by 0.7%\n",
      "     (relative improvement over non-entangled baseline)\n",
      "\n",
      "5. QUATERNION vs QUANTUM (no entanglement) - Core Research Question:\n",
      "   ----------------------------------------------------------------------\n",
      "   Quaternion accuracy:      0.9364 ± 0.0015\n",
      "   Quantum (no ent) accuracy:0.8752 ± 0.0102\n",
      "   Gap:                      6.12 percentage points\n",
      "\n",
      "   → Quaternion networks OUTPERFORM quantum circuits without entanglement\n",
      "     by 6.12 percentage points, suggesting classical SU(2)\n",
      "     provides a more effective representation on this task.\n",
      "\n",
      "6. QUATERNION vs QUANTUM (with entanglement):\n",
      "   ----------------------------------------------------------------------\n",
      "   Quaternion accuracy:      0.9364 ± 0.0015\n",
      "   Quantum (with ent) accuracy:0.8816 ± 0.0062\n",
      "   Gap:                      5.49 percentage points\n",
      "\n",
      "   → Quaternions match or exceed entangled quantum performance,\n",
      "     suggesting entanglement provides limited benefit on this task.\n",
      "\n",
      "==========================================================================================\n",
      "KEY TAKEAWAYS FOR PAPER\n",
      "==========================================================================================\n",
      "\n",
      "✓ EMPIRICAL FINDINGS:\n",
      "  1. Quaternion networks achieve 100.1% of Real MLP performance\n",
      "     with structured SU(2) algebraic constraints\n",
      "\n",
      "  2. Quaternions vs Quantum (no ent): 6.12 percentage point gap\n",
      "\n",
      "  3. Entanglement provides minimal benefit (0.7%) on MNIST\n",
      "     → Single-qubit rotations sufficient for this classification task\n",
      "\n",
      "✓ IMPLICATIONS:\n",
      "  • Quaternion networks provide an efficient classical surrogate for\n",
      "    single-qubit quantum models (SU(2) rotations without entanglement)\n",
      "  • For tasks where entanglement is unnecessary, classical quaternion\n",
      "    algebra may be preferable (faster training, no quantum hardware)\n",
      "  • When entanglement provides measurable benefit, quantum advantage emerges\n",
      "\n",
      "✓ COMPUTATIONAL EFFICIENCY:\n",
      "  • Real MLP:  30.0s (baseline)\n",
      "  • Quaternion: 77.4s (2.58x Real)\n",
      "  • Quantum (no ent): 24601.3s (818.8x Real)\n",
      "    → Even with lightning.gpu, quantum is 317.7x slower than quaternions\n",
      "==========================================================================================\n",
      "\n",
      "==========================================================================================\n",
      "ANALYSIS COMPLETE\n",
      "==========================================================================================\n",
      "\n",
      "All results saved in:\n",
      "  • realnet_results.pt\n",
      "  • quatnet_results.pt\n",
      "  • quantum_noent_results.pt\n",
      "  • quantum_ent_results.pt\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Block 5: Aggregate Results and Comparative Analysis\n",
    "====================================================\n",
    "Loads results from Blocks 1-4 and performs comprehensive comparison.\n",
    "\n",
    "Requirements:\n",
    "- realnet_results.pt (from Block 1)\n",
    "- quatnet_results.pt (from Block 2)\n",
    "- quantum_noent_results.pt (from Block 3)\n",
    "- quantum_ent_results.pt (from Block 4)\n",
    "\n",
    "Outputs:\n",
    "- Comprehensive comparison tables\n",
    "- Statistical analysis\n",
    "- Research question answers\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def load_results():\n",
    "    \"\"\"Load all results files\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    try:\n",
    "        real_data = torch.load(\"realnet_results.pt\", weights_only=False)\n",
    "        results[\"Real\"] = real_data[\"results\"]\n",
    "        print(\"✓ Loaded RealNet results\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"✗ realnet_results.pt not found\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        quat_data = torch.load(\"quatnet_results.pt\", weights_only=False)\n",
    "        results[\"Quat\"] = quat_data[\"results\"]\n",
    "        print(\"✓ Loaded QuatNet results\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"✗ quatnet_results.pt not found\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        qno_data = torch.load(\"quantum_noent_results.pt\", weights_only=False)\n",
    "        results[\"QNoEnt\"] = qno_data[\"results\"]\n",
    "        print(\"✓ Loaded Quantum (no ent) results\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"⚠ quantum_noent_results.pt not found (skipping)\")\n",
    "        results[\"QNoEnt\"] = []\n",
    "    \n",
    "    try:\n",
    "        qent_data = torch.load(\"quantum_ent_results.pt\", weights_only=False)\n",
    "        results[\"QEnt\"] = qent_data[\"results\"]\n",
    "        print(\"✓ Loaded Quantum (with ent) results\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"⚠ quantum_ent_results.pt not found (skipping)\")\n",
    "        results[\"QEnt\"] = []\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def print_summary_table(results):\n",
    "    \"\"\"Print aggregated summary table\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(\"AGGREGATED RESULTS (mean ± std over 3 seeds)\")\n",
    "    print(\"=\" * 90)\n",
    "    print(f\"{'Model':<15} {'Accuracy':<20} {'Time (s)':<20} {'Epochs':<15} {'Parameters':<20}\")\n",
    "    print(\"-\" * 90)\n",
    "    \n",
    "    for name in [\"Real\", \"Quat\", \"QNoEnt\", \"QEnt\"]:\n",
    "        if not results[name]:\n",
    "            continue\n",
    "            \n",
    "        accs = [r[\"best_acc\"] for r in results[name]]\n",
    "        times = [r[\"time\"] for r in results[name]]\n",
    "        epochs = [r[\"epochs\"] for r in results[name]]\n",
    "        \n",
    "        acc_str = f\"{np.mean(accs):.4f} ± {np.std(accs):.4f}\"\n",
    "        time_str = f\"{np.mean(times):.1f} ± {np.std(times):.1f}\"\n",
    "        epoch_str = f\"{np.mean(epochs):.1f} ± {np.std(epochs):.1f}\"\n",
    "        \n",
    "        if name == \"Real\":\n",
    "            params = results[name][0][\"params\"]\n",
    "            param_str = f\"{params:,}\"\n",
    "        else:\n",
    "            trainable = results[name][0][\"trainable_params\"]\n",
    "            total = results[name][0][\"total_params\"]\n",
    "            param_str = f\"{trainable:,} (head)\"\n",
    "        \n",
    "        print(f\"{name:<15} {acc_str:<20} {time_str:<20} {epoch_str:<15} {param_str:<20}\")\n",
    "    \n",
    "    print(\"=\" * 90)\n",
    "\n",
    "\n",
    "def print_per_seed_table(results):\n",
    "    \"\"\"Print detailed per-seed results\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"PER-SEED RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    seeds = [42, 123, 456]\n",
    "    \n",
    "    for seed_idx, seed in enumerate(seeds):\n",
    "        print(f\"\\nSeed {seed}:\")\n",
    "        print(f\"{'Model':<15} {'Accuracy':<12} {'Time (s)':<12} {'Epochs':<10}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for name in [\"Real\", \"Quat\", \"QNoEnt\", \"QEnt\"]:\n",
    "            if not results[name]:\n",
    "                continue\n",
    "            \n",
    "            r = results[name][seed_idx]\n",
    "            print(f\"{name:<15} {r['best_acc']:<12.4f} {r['time']:<12.1f} {r['epochs']:<10}\")\n",
    "\n",
    "\n",
    "def comparative_analysis(results):\n",
    "    \"\"\"Perform comparative analysis answering research questions\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(\"COMPARATIVE ANALYSIS: Answering Research Questions\")\n",
    "    print(\"=\" * 90)\n",
    "    \n",
    "    real_accs = [r[\"best_acc\"] for r in results[\"Real\"]]\n",
    "    quat_accs = [r[\"best_acc\"] for r in results[\"Quat\"]]\n",
    "    \n",
    "    def pct_gap(a, b):\n",
    "        \"\"\"Percentage point gap (a - b)\"\"\"\n",
    "        return (np.mean(a) - np.mean(b)) * 100.0\n",
    "    \n",
    "    def retention(a, b):\n",
    "        \"\"\"Percentage of performance retained\"\"\"\n",
    "        return (np.mean(a) / np.mean(b)) * 100.0\n",
    "    \n",
    "    print(\"\\n1. QUATERNION vs REAL MLP:\")\n",
    "    print(\"   \" + \"-\" * 70)\n",
    "    print(f\"   Real MLP accuracy:        {np.mean(real_accs):.4f} ± {np.std(real_accs):.4f}\")\n",
    "    print(f\"   Quaternion accuracy:      {np.mean(quat_accs):.4f} ± {np.std(quat_accs):.4f}\")\n",
    "    print(f\"   Gap:                      {pct_gap(real_accs, quat_accs):.2f} percentage points\")\n",
    "    print(f\"   Performance retention:    {retention(quat_accs, real_accs):.1f}%\")\n",
    "    print(f\"\\n   → Classical SU(2) (quaternions) captures {retention(quat_accs, real_accs):.1f}% of\")\n",
    "    print(f\"     standard MLP performance with structured algebraic constraints.\")\n",
    "    \n",
    "    if results[\"QNoEnt\"]:\n",
    "        qno_accs = [r[\"best_acc\"] for r in results[\"QNoEnt\"]]\n",
    "        \n",
    "        print(\"\\n2. QUANTUM (no entanglement) vs REAL MLP:\")\n",
    "        print(\"   \" + \"-\" * 70)\n",
    "        print(f\"   Real MLP accuracy:        {np.mean(real_accs):.4f} ± {np.std(real_accs):.4f}\")\n",
    "        print(f\"   Quantum (no ent) accuracy:{np.mean(qno_accs):.4f} ± {np.std(qno_accs):.4f}\")\n",
    "        print(f\"   Gap:                      {pct_gap(real_accs, qno_accs):.2f} percentage points\")\n",
    "        print(f\"   Performance retention:    {retention(qno_accs, real_accs):.1f}%\")\n",
    "        print(f\"\\n   → Quantum circuits WITHOUT entanglement capture {retention(qno_accs, real_accs):.1f}%\")\n",
    "        print(f\"     of MLP performance, suggesting limited benefit over classical rotation gates.\")\n",
    "    \n",
    "    if results[\"QEnt\"]:\n",
    "        qent_accs = [r[\"best_acc\"] for r in results[\"QEnt\"]]\n",
    "        \n",
    "        print(\"\\n3. QUANTUM (with entanglement) vs REAL MLP:\")\n",
    "        print(\"   \" + \"-\" * 70)\n",
    "        print(f\"   Real MLP accuracy:        {np.mean(real_accs):.4f} ± {np.std(real_accs):.4f}\")\n",
    "        print(f\"   Quantum (with ent) accuracy:{np.mean(qent_accs):.4f} ± {np.std(qent_accs):.4f}\")\n",
    "        print(f\"   Gap:                      {pct_gap(real_accs, qent_accs):.2f} percentage points\")\n",
    "        print(f\"   Performance retention:    {retention(qent_accs, real_accs):.1f}%\")\n",
    "        print(f\"\\n   → Quantum circuits WITH entanglement capture {retention(qent_accs, real_accs):.1f}%\")\n",
    "        print(f\"     of MLP performance.\")\n",
    "    \n",
    "    if results[\"QNoEnt\"] and results[\"QEnt\"]:\n",
    "        print(\"\\n4. ENTANGLEMENT EFFECT:\")\n",
    "        print(\"   \" + \"-\" * 70)\n",
    "        print(f\"   Quantum (no ent) accuracy:{np.mean(qno_accs):.4f} ± {np.std(qno_accs):.4f}\")\n",
    "        print(f\"   Quantum (with ent) accuracy:{np.mean(qent_accs):.4f} ± {np.std(qent_accs):.4f}\")\n",
    "        print(f\"   Improvement:              {pct_gap(qent_accs, qno_accs):.2f} percentage points\")\n",
    "        \n",
    "        if np.mean(qent_accs) > np.mean(qno_accs):\n",
    "            improvement_pct = ((np.mean(qent_accs) - np.mean(qno_accs)) / np.mean(qno_accs)) * 100\n",
    "            print(f\"\\n   → Entanglement IMPROVES performance by {improvement_pct:.1f}%\")\n",
    "            print(f\"     (relative improvement over non-entangled baseline)\")\n",
    "        else:\n",
    "            print(f\"\\n   → Entanglement DOES NOT improve performance on this task\")\n",
    "    \n",
    "    if results[\"QNoEnt\"]:\n",
    "        print(\"\\n5. QUATERNION vs QUANTUM (no entanglement) - Core Research Question:\")\n",
    "        print(\"   \" + \"-\" * 70)\n",
    "        print(f\"   Quaternion accuracy:      {np.mean(quat_accs):.4f} ± {np.std(quat_accs):.4f}\")\n",
    "        print(f\"   Quantum (no ent) accuracy:{np.mean(qno_accs):.4f} ± {np.std(qno_accs):.4f}\")\n",
    "        print(f\"   Gap:                      {pct_gap(quat_accs, qno_accs):.2f} percentage points\")\n",
    "        \n",
    "        gap_abs = abs(pct_gap(quat_accs, qno_accs))\n",
    "        if gap_abs < 2.0:\n",
    "            print(f\"\\n   → Quaternion networks (classical SU(2)) CLOSELY APPROXIMATE quantum circuits\")\n",
    "            print(f\"     without entanglement (quantum SU(2)). Gap < 2 percentage points.\")\n",
    "            print(f\"\\n   → This supports the hypothesis that classical quaternion algebra can serve\")\n",
    "            print(f\"     as an effective surrogate for single-qubit quantum models on MNIST.\")\n",
    "        elif np.mean(quat_accs) > np.mean(qno_accs):\n",
    "            print(f\"\\n   → Quaternion networks OUTPERFORM quantum circuits without entanglement\")\n",
    "            print(f\"     by {gap_abs:.2f} percentage points, suggesting classical SU(2)\")\n",
    "            print(f\"     provides a more effective representation on this task.\")\n",
    "        else:\n",
    "            print(f\"\\n   → Quantum circuits without entanglement OUTPERFORM quaternions\")\n",
    "            print(f\"     by {gap_abs:.2f} percentage points.\")\n",
    "    \n",
    "    if results[\"QEnt\"]:\n",
    "        print(\"\\n6. QUATERNION vs QUANTUM (with entanglement):\")\n",
    "        print(\"   \" + \"-\" * 70)\n",
    "        print(f\"   Quaternion accuracy:      {np.mean(quat_accs):.4f} ± {np.std(quat_accs):.4f}\")\n",
    "        print(f\"   Quantum (with ent) accuracy:{np.mean(qent_accs):.4f} ± {np.std(qent_accs):.4f}\")\n",
    "        print(f\"   Gap:                      {pct_gap(quat_accs, qent_accs):.2f} percentage points\")\n",
    "        \n",
    "        if np.mean(qent_accs) > np.mean(quat_accs):\n",
    "            print(f\"\\n   → Entangled quantum circuits outperform quaternions, demonstrating\")\n",
    "            print(f\"     the value of quantum correlations beyond classical SU(2) rotations.\")\n",
    "        else:\n",
    "            print(f\"\\n   → Quaternions match or exceed entangled quantum performance,\")\n",
    "            print(f\"     suggesting entanglement provides limited benefit on this task.\")\n",
    "\n",
    "\n",
    "def key_takeaways(results):\n",
    "    \"\"\"Summarize key takeaways for paper\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(\"KEY TAKEAWAYS FOR PAPER\")\n",
    "    print(\"=\" * 90)\n",
    "    \n",
    "    real_accs = [r[\"best_acc\"] for r in results[\"Real\"]]\n",
    "    quat_accs = [r[\"best_acc\"] for r in results[\"Quat\"]]\n",
    "    \n",
    "    def retention(a, b):\n",
    "        return (np.mean(a) / np.mean(b)) * 100.0\n",
    "    \n",
    "    print(\"\\n✓ EMPIRICAL FINDINGS:\")\n",
    "    print(f\"  1. Quaternion networks achieve {retention(quat_accs, real_accs):.1f}% of Real MLP performance\")\n",
    "    print(f\"     with structured SU(2) algebraic constraints\")\n",
    "    \n",
    "    if results[\"QNoEnt\"]:\n",
    "        qno_accs = [r[\"best_acc\"] for r in results[\"QNoEnt\"]]\n",
    "        gap = abs((np.mean(quat_accs) - np.mean(qno_accs)) * 100.0)\n",
    "        print(f\"\\n  2. Quaternions vs Quantum (no ent): {gap:.2f} percentage point gap\")\n",
    "        if gap < 2.0:\n",
    "            print(f\"     → Classical SU(2) effectively approximates quantum SU(2) without entanglement\")\n",
    "        \n",
    "    if results[\"QEnt\"]:\n",
    "        qent_accs = [r[\"best_acc\"] for r in results[\"QEnt\"]]\n",
    "        qno_accs = [r[\"best_acc\"] for r in results[\"QNoEnt\"]]\n",
    "        ent_improvement = ((np.mean(qent_accs) - np.mean(qno_accs)) / np.mean(qno_accs)) * 100\n",
    "        if ent_improvement > 1.0:\n",
    "            print(f\"\\n  3. Entanglement improves quantum performance by {ent_improvement:.1f}%\")\n",
    "            print(f\"     → Demonstrates measurable value of quantum correlations on MNIST\")\n",
    "        else:\n",
    "            print(f\"\\n  3. Entanglement provides minimal benefit ({ent_improvement:.1f}%) on MNIST\")\n",
    "            print(f\"     → Single-qubit rotations sufficient for this classification task\")\n",
    "    \n",
    "    print(\"\\n✓ IMPLICATIONS:\")\n",
    "    print(\"  • Quaternion networks provide an efficient classical surrogate for\")\n",
    "    print(\"    single-qubit quantum models (SU(2) rotations without entanglement)\")\n",
    "    print(\"  • For tasks where entanglement is unnecessary, classical quaternion\")\n",
    "    print(\"    algebra may be preferable (faster training, no quantum hardware)\")\n",
    "    print(\"  • When entanglement provides measurable benefit, quantum advantage emerges\")\n",
    "    \n",
    "    print(\"\\n✓ COMPUTATIONAL EFFICIENCY:\")\n",
    "    real_time = np.mean([r[\"time\"] for r in results[\"Real\"]])\n",
    "    quat_time = np.mean([r[\"time\"] for r in results[\"Quat\"]])\n",
    "    print(f\"  • Real MLP:  {real_time:.1f}s (baseline)\")\n",
    "    print(f\"  • Quaternion: {quat_time:.1f}s ({quat_time/real_time:.2f}x Real)\")\n",
    "    \n",
    "    if results[\"QNoEnt\"]:\n",
    "        qno_time = np.mean([r[\"time\"] for r in results[\"QNoEnt\"]])\n",
    "        print(f\"  • Quantum (no ent): {qno_time:.1f}s ({qno_time/real_time:.1f}x Real)\")\n",
    "        print(f\"    → Even with lightning.gpu, quantum is {qno_time/quat_time:.1f}x slower than quaternions\")\n",
    "    \n",
    "    print(\"=\" * 90)\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 90)\n",
    "    print(\"BLOCK 5: AGGREGATE RESULTS AND COMPARATIVE ANALYSIS\")\n",
    "    print(\"=\" * 90)\n",
    "    \n",
    "    results = load_results()\n",
    "    \n",
    "    if results is None:\n",
    "        print(\"\\n✗ ERROR: Required results files not found.\")\n",
    "        print(\"Run Blocks 1 and 2 at minimum (Real and Quat).\")\n",
    "        return\n",
    "    \n",
    "    # Summary table\n",
    "    print_summary_table(results)\n",
    "    \n",
    "    # Per-seed details\n",
    "    print_per_seed_table(results)\n",
    "    \n",
    "    # Comparative analysis\n",
    "    comparative_analysis(results)\n",
    "    \n",
    "    # Key takeaways\n",
    "    key_takeaways(results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(\"ANALYSIS COMPLETE\")\n",
    "    print(\"=\" * 90)\n",
    "    print(\"\\nAll results saved in:\")\n",
    "    print(\"  • realnet_results.pt\")\n",
    "    print(\"  • quatnet_results.pt\")\n",
    "    if results[\"QNoEnt\"]:\n",
    "        print(\"  • quantum_noent_results.pt\")\n",
    "    if results[\"QEnt\"]:\n",
    "        print(\"  • quantum_ent_results.pt\")\n",
    "    print(\"=\" * 90)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a4ca24-8d0f-42cc-bc0e-4cb3162ca51c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
