{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb827bdd-8e7a-4529-b171-bfe39f21755e",
   "metadata": {},
   "source": [
    "# RealNet FashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44ea5659-ba60-47df-99d3-b04c1f55ea06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch device: cuda\n",
      "======================================================================\n",
      "BLOCK 1: RealNet Training\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "  • Batch size: 32\n",
      "  • Patience: 10\n",
      "  • Seeds: [42, 123, 456]\n",
      "  • Architecture: 784 → 16 → 64 → 10\n",
      "======================================================================\n",
      "\n",
      "Creating stratified samples...\n",
      "  Sampling took 0.01s\n",
      "  Train samples: 15000 (stratified, 1500 per class)\n",
      "  Test samples:  3000 (stratified, 300 per class)\n",
      "\n",
      "======================================================================\n",
      "SEED 42\n",
      "======================================================================\n",
      "\n",
      "  Training RealNet (seed=42)...\n",
      "  [Real] Epoch  1 | loss=0.8276 | test_acc=0.7903 | time=1.6s\n",
      "  [Real] Epoch  2 | loss=0.4974 | test_acc=0.8260 | time=2.9s\n",
      "  [Real] Epoch  3 | loss=0.4442 | test_acc=0.8333 | time=4.1s\n",
      "  [Real] Epoch  4 | loss=0.4149 | test_acc=0.8270 | time=5.4s\n",
      "  [Real] Epoch  5 | loss=0.3965 | test_acc=0.8373 | time=6.8s\n",
      "  [Real] Epoch  6 | loss=0.3760 | test_acc=0.8383 | time=8.1s\n",
      "  [Real] Epoch  7 | loss=0.3641 | test_acc=0.8303 | time=9.4s\n",
      "  [Real] Epoch  8 | loss=0.3509 | test_acc=0.8377 | time=11.7s\n",
      "  [Real] Epoch  9 | loss=0.3429 | test_acc=0.8390 | time=13.0s\n",
      "  [Real] Epoch 10 | loss=0.3271 | test_acc=0.8440 | time=14.3s\n",
      "  [Real] Epoch 11 | loss=0.3186 | test_acc=0.8397 | time=15.6s\n",
      "  [Real] Epoch 12 | loss=0.3115 | test_acc=0.8313 | time=16.9s\n",
      "  [Real] Epoch 13 | loss=0.3069 | test_acc=0.8327 | time=18.2s\n",
      "  [Real] Epoch 14 | loss=0.3008 | test_acc=0.8453 | time=19.6s\n",
      "  [Real] Epoch 15 | loss=0.3007 | test_acc=0.8420 | time=20.9s\n",
      "  [Real] Epoch 16 | loss=0.2891 | test_acc=0.8383 | time=22.2s\n",
      "  [Real] Epoch 17 | loss=0.2797 | test_acc=0.8373 | time=23.6s\n",
      "  [Real] Epoch 18 | loss=0.2733 | test_acc=0.8270 | time=24.9s\n",
      "  [Real] Epoch 19 | loss=0.2711 | test_acc=0.8343 | time=26.3s\n",
      "  [Real] Epoch 20 | loss=0.2649 | test_acc=0.8357 | time=27.7s\n",
      "  [Real] Epoch 21 | loss=0.2621 | test_acc=0.8410 | time=29.1s\n",
      "  [Real] Epoch 22 | loss=0.2619 | test_acc=0.8383 | time=30.5s\n",
      "  [Real] Epoch 23 | loss=0.2506 | test_acc=0.8390 | time=31.9s\n",
      "  [Real] Epoch 24 | loss=0.2493 | test_acc=0.8393 | time=33.3s\n",
      "  [Real] Early stop at epoch 24 (no improvement for 10 epochs)\n",
      "\n",
      "  → Saved preprocessor state from seed 42\n",
      "\n",
      "======================================================================\n",
      "SEED 123\n",
      "======================================================================\n",
      "\n",
      "  Training RealNet (seed=123)...\n",
      "  [Real] Epoch  1 | loss=0.8076 | test_acc=0.8080 | time=1.4s\n",
      "  [Real] Epoch  2 | loss=0.4696 | test_acc=0.8267 | time=2.8s\n",
      "  [Real] Epoch  3 | loss=0.4312 | test_acc=0.8317 | time=4.1s\n",
      "  [Real] Epoch  4 | loss=0.4032 | test_acc=0.8373 | time=5.5s\n",
      "  [Real] Epoch  5 | loss=0.3861 | test_acc=0.8373 | time=7.0s\n",
      "  [Real] Epoch  6 | loss=0.3691 | test_acc=0.8343 | time=8.5s\n",
      "  [Real] Epoch  7 | loss=0.3598 | test_acc=0.8343 | time=10.9s\n",
      "  [Real] Epoch  8 | loss=0.3523 | test_acc=0.8453 | time=12.4s\n",
      "  [Real] Epoch  9 | loss=0.3355 | test_acc=0.8430 | time=13.9s\n",
      "  [Real] Epoch 10 | loss=0.3230 | test_acc=0.8323 | time=15.4s\n",
      "  [Real] Epoch 11 | loss=0.3165 | test_acc=0.8373 | time=16.8s\n",
      "  [Real] Epoch 12 | loss=0.3143 | test_acc=0.8427 | time=18.1s\n",
      "  [Real] Epoch 13 | loss=0.3049 | test_acc=0.8440 | time=19.5s\n",
      "  [Real] Epoch 14 | loss=0.2938 | test_acc=0.8380 | time=20.9s\n",
      "  [Real] Epoch 15 | loss=0.2918 | test_acc=0.8477 | time=22.1s\n",
      "  [Real] Epoch 16 | loss=0.2835 | test_acc=0.8457 | time=23.4s\n",
      "  [Real] Epoch 17 | loss=0.2795 | test_acc=0.8437 | time=24.7s\n",
      "  [Real] Epoch 18 | loss=0.2771 | test_acc=0.8413 | time=26.0s\n",
      "  [Real] Epoch 19 | loss=0.2728 | test_acc=0.8413 | time=27.4s\n",
      "  [Real] Epoch 20 | loss=0.2681 | test_acc=0.8380 | time=28.7s\n",
      "  [Real] Epoch 21 | loss=0.2642 | test_acc=0.8420 | time=30.0s\n",
      "  [Real] Epoch 22 | loss=0.2676 | test_acc=0.8340 | time=31.3s\n",
      "  [Real] Epoch 23 | loss=0.2504 | test_acc=0.8447 | time=32.6s\n",
      "  [Real] Epoch 24 | loss=0.2487 | test_acc=0.8347 | time=33.9s\n",
      "  [Real] Epoch 25 | loss=0.2438 | test_acc=0.8403 | time=35.2s\n",
      "  [Real] Early stop at epoch 25 (no improvement for 10 epochs)\n",
      "\n",
      "======================================================================\n",
      "SEED 456\n",
      "======================================================================\n",
      "\n",
      "  Training RealNet (seed=456)...\n",
      "  [Real] Epoch  1 | loss=0.8361 | test_acc=0.8007 | time=1.3s\n",
      "  [Real] Epoch  2 | loss=0.5010 | test_acc=0.8153 | time=2.6s\n",
      "  [Real] Epoch  3 | loss=0.4518 | test_acc=0.8187 | time=3.9s\n",
      "  [Real] Epoch  4 | loss=0.4273 | test_acc=0.8337 | time=5.3s\n",
      "  [Real] Epoch  5 | loss=0.4044 | test_acc=0.8357 | time=6.6s\n",
      "  [Real] Epoch  6 | loss=0.3824 | test_acc=0.8267 | time=9.0s\n",
      "  [Real] Epoch  7 | loss=0.3673 | test_acc=0.8277 | time=10.3s\n",
      "  [Real] Epoch  8 | loss=0.3606 | test_acc=0.8323 | time=11.7s\n",
      "  [Real] Epoch  9 | loss=0.3492 | test_acc=0.8377 | time=13.0s\n",
      "  [Real] Epoch 10 | loss=0.3366 | test_acc=0.8450 | time=14.3s\n",
      "  [Real] Epoch 11 | loss=0.3289 | test_acc=0.8440 | time=15.6s\n",
      "  [Real] Epoch 12 | loss=0.3256 | test_acc=0.8400 | time=16.9s\n",
      "  [Real] Epoch 13 | loss=0.3118 | test_acc=0.8387 | time=18.3s\n",
      "  [Real] Epoch 14 | loss=0.3115 | test_acc=0.8367 | time=19.6s\n",
      "  [Real] Epoch 15 | loss=0.3035 | test_acc=0.8403 | time=20.9s\n",
      "  [Real] Epoch 16 | loss=0.2936 | test_acc=0.8393 | time=22.1s\n",
      "  [Real] Epoch 17 | loss=0.2924 | test_acc=0.8387 | time=23.5s\n",
      "  [Real] Epoch 18 | loss=0.2804 | test_acc=0.8350 | time=24.8s\n",
      "  [Real] Epoch 19 | loss=0.2789 | test_acc=0.8327 | time=26.1s\n",
      "  [Real] Epoch 20 | loss=0.2802 | test_acc=0.8430 | time=27.5s\n",
      "  [Real] Early stop at epoch 20 (no improvement for 10 epochs)\n",
      "\n",
      "======================================================================\n",
      "REALNET SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Accuracy:   0.8460 ± 0.0012\n",
      "Time:       32.0s ± 3.3s\n",
      "Epochs:     23.0 ± 2.2\n",
      "Parameters: 14,298\n",
      "\n",
      "Per-seed results:\n",
      "  Seed 42: acc=0.8453, time=33.3s, epochs=24\n",
      "  Seed 123: acc=0.8477, time=35.2s, epochs=25\n",
      "  Seed 456: acc=0.8450, time=27.5s, epochs=20\n",
      "\n",
      "✓ Saved results to: realnet_results.pt\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Block 1: RealNet Training\n",
    "==========================\n",
    "Trains the baseline Real MLP on 3 seeds.\n",
    "Saves the trained preprocessor for reuse in subsequent blocks.\n",
    "\n",
    "Outputs:\n",
    "- realnet_results.pt: Contains results dict and trained preprocessor state\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "print(f\"Using PyTorch device: {device}\")\n",
    "\n",
    "# ============================================\n",
    "# Comprehensive seed setting for reproducibility\n",
    "# ============================================\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    \"\"\"Set seeds for all RNG sources for reproducibility\"\"\"\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    # Python\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # NumPy\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # PyTorch deterministic mode\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # CuPy (if available)\n",
    "    try:\n",
    "        import cupy as cp\n",
    "        cp.random.seed(seed)\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    # PennyLane (if available)\n",
    "    try:\n",
    "        import pennylane as qml\n",
    "        qml.numpy.random.seed(seed)\n",
    "    except (ImportError, AttributeError):\n",
    "        pass\n",
    "\n",
    "# ============================================\n",
    "# Shared Bottleneck Preprocessor: 784 → 16\n",
    "# ============================================\n",
    "\n",
    "class SharedPreprocessor(nn.Module):\n",
    "    \"\"\"Shared classical feature extractor: 784 → 16\"\"\"\n",
    "    def __init__(self, input_dim=784, bottleneck_dim=16):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, bottleneck_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.tanh(self.fc(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Real-valued Head and Network\n",
    "# ============================================\n",
    "\n",
    "class RealHead(nn.Module):\n",
    "    \"\"\"Standard MLP: 16 → 64 → 10\"\"\"\n",
    "    def __init__(self, bottleneck_dim=16, hidden_dim=64, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(bottleneck_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RealNet(nn.Module):\n",
    "    \"\"\"Complete Real network: Preprocessor + RealHead\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.preprocessor = SharedPreprocessor(784, 16)\n",
    "        self.head = RealHead(16, 64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.preprocessor(x)\n",
    "        return self.head(features)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Training and Evaluation\n",
    "# ============================================\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, device, show_progress=False):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        total_samples += x.size(0)\n",
    "\n",
    "        if show_progress and batch_idx % 50 == 0:\n",
    "            print(f\"    Batch {batch_idx}/{len(loader)}, samples: {total_samples}\", end=\"\\r\")\n",
    "\n",
    "    if show_progress:\n",
    "        print()\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += x.size(0)\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "def train_with_early_stopping(model, train_loader, test_loader, optimizer,\n",
    "                              device, max_epochs=40, patience=10, name=\"Model\"):\n",
    "    if isinstance(device, str):\n",
    "        device = torch.device(device)\n",
    "\n",
    "    best_acc = 0.0\n",
    "    epochs_without_improvement = 0\n",
    "    start = time.time()\n",
    "    last_acc = 0.0\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        loss = train_one_epoch(model, train_loader, optimizer, device, show_progress=False)\n",
    "        acc = evaluate(model, test_loader, device)\n",
    "        last_acc = acc\n",
    "\n",
    "        elapsed = time.time() - start\n",
    "        print(f\"  [{name}] Epoch {epoch:2d} | loss={loss:.4f} \"\n",
    "              f\"| test_acc={acc:.4f} | time={elapsed:.1f}s\")\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"  [{name}] Early stop at epoch {epoch} \"\n",
    "                  f\"(no improvement for {patience} epochs)\")\n",
    "            break\n",
    "\n",
    "    total_time = time.time() - start\n",
    "    return {\n",
    "        \"best_acc\": best_acc,\n",
    "        \"final_acc\": last_acc,\n",
    "        \"time\": total_time,\n",
    "        \"epochs\": epoch,\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Data utilities\n",
    "# ============================================\n",
    "\n",
    "def stratified_sample_from_targets(dataset, n_samples_per_class, seed=42):\n",
    "    \"\"\"\n",
    "    Create a stratified sample with n_samples_per_class from each class.\n",
    "    Uses dataset.targets directly - NO image loading during sampling.\n",
    "    \"\"\"\n",
    "    # FashionMNIST and MNIST expose targets as a tensor\n",
    "    targets = dataset.targets\n",
    "    rng = np.random.RandomState(seed)\n",
    "    \n",
    "    sampled_indices = []\n",
    "    for c in range(10):\n",
    "        # Find all indices for class c\n",
    "        idx_c = (targets == c).nonzero(as_tuple=False).view(-1).cpu().numpy()\n",
    "        k = min(n_samples_per_class, len(idx_c))\n",
    "        # Sample without replacement\n",
    "        selected = rng.choice(idx_c, size=k, replace=False).tolist()\n",
    "        sampled_indices.extend(selected)\n",
    "    \n",
    "    # Shuffle the combined indices\n",
    "    rng.shuffle(sampled_indices)\n",
    "    return sampled_indices\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Main\n",
    "# ============================================\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"BLOCK 1: RealNet Training\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nConfiguration:\")\n",
    "    print(\"  • Batch size: 32\")\n",
    "    print(\"  • Patience: 10\")\n",
    "    print(\"  • Seeds: [42, 123, 456]\")\n",
    "    print(\"  • Architecture: 784 → 16 → 64 → 10\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.2860,), (0.3530,))\n",
    "    ])\n",
    "\n",
    "    # Load full datasets\n",
    "    full_train_ds = datasets.FashionMNIST(root=\"./data\", train=True,\n",
    "                                         download=True, transform=transform)\n",
    "    full_test_ds = datasets.FashionMNIST(root=\"./data\", train=False,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "    # Create stratified samples (fast - uses targets only, no image loading)\n",
    "    print(\"\\nCreating stratified samples...\")\n",
    "    t0 = time.time()\n",
    "    train_indices = stratified_sample_from_targets(full_train_ds, n_samples_per_class=1500, seed=42)\n",
    "    test_indices = stratified_sample_from_targets(full_test_ds, n_samples_per_class=300, seed=42)\n",
    "    print(f\"  Sampling took {time.time()-t0:.2f}s\")\n",
    "    \n",
    "    train_ds = Subset(full_train_ds, train_indices)\n",
    "    test_ds = Subset(full_test_ds, test_indices)\n",
    "    \n",
    "    print(f\"  Train samples: {len(train_ds)} (stratified, 1500 per class)\")\n",
    "    print(f\"  Test samples:  {len(test_ds)} (stratified, 300 per class)\")\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=32,\n",
    "                              shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_ds, batch_size=64,\n",
    "                             shuffle=False, num_workers=0)\n",
    "\n",
    "    seeds = [42, 123, 456]\n",
    "    all_results = []\n",
    "    trained_preprocessor_state = None\n",
    "\n",
    "    for seed in seeds:\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(f\"SEED {seed}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        set_all_seeds(seed)\n",
    "\n",
    "        print(f\"\\n  Training RealNet (seed={seed})...\")\n",
    "        real_model = RealNet().to(device)\n",
    "        real_opt = torch.optim.Adam(real_model.parameters(), lr=1e-3)\n",
    "        \n",
    "        result = train_with_early_stopping(\n",
    "            real_model, train_loader, test_loader, real_opt, device,\n",
    "            max_epochs=40, patience=10, name=\"Real\"\n",
    "        )\n",
    "        result[\"params\"] = sum(p.numel() for p in real_model.parameters())\n",
    "        result[\"seed\"] = seed\n",
    "        \n",
    "        all_results.append(result)\n",
    "        \n",
    "        # Save preprocessor from first seed\n",
    "        if trained_preprocessor_state is None:\n",
    "            trained_preprocessor_state = real_model.preprocessor.state_dict()\n",
    "            print(f\"\\n  → Saved preprocessor state from seed {seed}\")\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"REALNET SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    accs = [r[\"best_acc\"] for r in all_results]\n",
    "    times = [r[\"time\"] for r in all_results]\n",
    "    epochs = [r[\"epochs\"] for r in all_results]\n",
    "    params = all_results[0][\"params\"]\n",
    "    \n",
    "    print(f\"\\nAccuracy:   {np.mean(accs):.4f} ± {np.std(accs):.4f}\")\n",
    "    print(f\"Time:       {np.mean(times):.1f}s ± {np.std(times):.1f}s\")\n",
    "    print(f\"Epochs:     {np.mean(epochs):.1f} ± {np.std(epochs):.1f}\")\n",
    "    print(f\"Parameters: {params:,}\")\n",
    "    \n",
    "    print(\"\\nPer-seed results:\")\n",
    "    for r in all_results:\n",
    "        print(f\"  Seed {r['seed']}: acc={r['best_acc']:.4f}, \"\n",
    "              f\"time={r['time']:.1f}s, epochs={r['epochs']}\")\n",
    "\n",
    "    # Save results and preprocessor\n",
    "    save_dict = {\n",
    "        \"results\": all_results,\n",
    "        \"preprocessor_state\": trained_preprocessor_state,\n",
    "        \"summary\": {\n",
    "            \"mean_acc\": np.mean(accs),\n",
    "            \"std_acc\": np.std(accs),\n",
    "            \"mean_time\": np.mean(times),\n",
    "            \"params\": params\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    torch.save(save_dict, \"realnet_results.pt\")\n",
    "    print(f\"\\n✓ Saved results to: realnet_results.pt\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11453524-4ec0-4e95-9621-0c6ea601172d",
   "metadata": {},
   "source": [
    "# QuatNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e099062a-f242-4ca1-a8a7-2645b2dedcd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch device: cuda\n",
      "======================================================================\n",
      "BLOCK 2: QuatNet Training (Frozen Preprocessor)\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "  • Batch size: 32\n",
      "  • Patience: 10\n",
      "  • Seeds: [42, 123, 456]\n",
      "  • Architecture: [frozen 784→16] → 4 quats → 16 quats → 10\n",
      "======================================================================\n",
      "\n",
      "Loading frozen preprocessor from Block 1...\n",
      "✓ Loaded preprocessor state from realnet_results.pt\n",
      "  Preprocessor frozen with 12,560 params\n",
      "\n",
      "Creating stratified samples...\n",
      "  Sampling took 0.01s\n",
      "  Train samples: 15000 (stratified, 1500 per class)\n",
      "  Test samples:  3000 (stratified, 300 per class)\n",
      "\n",
      "======================================================================\n",
      "SEED 42\n",
      "======================================================================\n",
      "\n",
      "  Training QuatNet (seed=42)...\n",
      "  [Quat] Epoch  1 | loss=1.3058 | test_acc=0.8073 | time=2.3s\n",
      "  [Quat] Epoch  2 | loss=0.4225 | test_acc=0.8323 | time=4.4s\n",
      "  [Quat] Epoch  3 | loss=0.3442 | test_acc=0.8357 | time=6.7s\n",
      "  [Quat] Epoch  4 | loss=0.3137 | test_acc=0.8377 | time=8.9s\n",
      "  [Quat] Epoch  5 | loss=0.2970 | test_acc=0.8380 | time=11.1s\n",
      "  [Quat] Epoch  6 | loss=0.2856 | test_acc=0.8423 | time=13.2s\n",
      "  [Quat] Epoch  7 | loss=0.2778 | test_acc=0.8413 | time=15.5s\n",
      "  [Quat] Epoch  8 | loss=0.2716 | test_acc=0.8417 | time=17.6s\n",
      "  [Quat] Epoch  9 | loss=0.2663 | test_acc=0.8413 | time=19.8s\n",
      "  [Quat] Epoch 10 | loss=0.2626 | test_acc=0.8393 | time=22.0s\n",
      "  [Quat] Epoch 11 | loss=0.2594 | test_acc=0.8430 | time=24.3s\n",
      "  [Quat] Epoch 12 | loss=0.2566 | test_acc=0.8417 | time=26.5s\n",
      "  [Quat] Epoch 13 | loss=0.2541 | test_acc=0.8400 | time=30.2s\n",
      "  [Quat] Epoch 14 | loss=0.2521 | test_acc=0.8450 | time=32.5s\n",
      "  [Quat] Epoch 15 | loss=0.2501 | test_acc=0.8420 | time=34.8s\n",
      "  [Quat] Epoch 16 | loss=0.2483 | test_acc=0.8430 | time=37.1s\n",
      "  [Quat] Epoch 17 | loss=0.2470 | test_acc=0.8430 | time=39.3s\n",
      "  [Quat] Epoch 18 | loss=0.2455 | test_acc=0.8423 | time=41.6s\n",
      "  [Quat] Epoch 19 | loss=0.2447 | test_acc=0.8450 | time=43.8s\n",
      "  [Quat] Epoch 20 | loss=0.2434 | test_acc=0.8410 | time=46.0s\n",
      "  [Quat] Epoch 21 | loss=0.2423 | test_acc=0.8423 | time=48.3s\n",
      "  [Quat] Epoch 22 | loss=0.2416 | test_acc=0.8450 | time=50.6s\n",
      "  [Quat] Epoch 23 | loss=0.2403 | test_acc=0.8440 | time=52.9s\n",
      "  [Quat] Epoch 24 | loss=0.2396 | test_acc=0.8433 | time=55.1s\n",
      "  [Quat] Early stop at epoch 24 (no improvement for 10 epochs)\n",
      "\n",
      "======================================================================\n",
      "SEED 123\n",
      "======================================================================\n",
      "\n",
      "  Training QuatNet (seed=123)...\n",
      "  [Quat] Epoch  1 | loss=1.1336 | test_acc=0.8143 | time=2.2s\n",
      "  [Quat] Epoch  2 | loss=0.4049 | test_acc=0.8337 | time=4.4s\n",
      "  [Quat] Epoch  3 | loss=0.3371 | test_acc=0.8393 | time=8.1s\n",
      "  [Quat] Epoch  4 | loss=0.3093 | test_acc=0.8377 | time=10.2s\n",
      "  [Quat] Epoch  5 | loss=0.2937 | test_acc=0.8437 | time=12.5s\n",
      "  [Quat] Epoch  6 | loss=0.2834 | test_acc=0.8440 | time=14.7s\n",
      "  [Quat] Epoch  7 | loss=0.2761 | test_acc=0.8447 | time=16.9s\n",
      "  [Quat] Epoch  8 | loss=0.2704 | test_acc=0.8437 | time=19.2s\n",
      "  [Quat] Epoch  9 | loss=0.2660 | test_acc=0.8447 | time=21.6s\n",
      "  [Quat] Epoch 10 | loss=0.2626 | test_acc=0.8433 | time=23.9s\n",
      "  [Quat] Epoch 11 | loss=0.2594 | test_acc=0.8450 | time=26.2s\n",
      "  [Quat] Epoch 12 | loss=0.2569 | test_acc=0.8423 | time=28.5s\n",
      "  [Quat] Epoch 13 | loss=0.2543 | test_acc=0.8417 | time=30.8s\n",
      "  [Quat] Epoch 14 | loss=0.2529 | test_acc=0.8420 | time=33.1s\n",
      "  [Quat] Epoch 15 | loss=0.2513 | test_acc=0.8447 | time=35.4s\n",
      "  [Quat] Epoch 16 | loss=0.2496 | test_acc=0.8430 | time=37.7s\n",
      "  [Quat] Epoch 17 | loss=0.2484 | test_acc=0.8417 | time=41.5s\n",
      "  [Quat] Epoch 18 | loss=0.2470 | test_acc=0.8433 | time=43.7s\n",
      "  [Quat] Epoch 19 | loss=0.2462 | test_acc=0.8440 | time=45.8s\n",
      "  [Quat] Epoch 20 | loss=0.2445 | test_acc=0.8430 | time=48.0s\n",
      "  [Quat] Epoch 21 | loss=0.2434 | test_acc=0.8417 | time=50.2s\n",
      "  [Quat] Early stop at epoch 21 (no improvement for 10 epochs)\n",
      "\n",
      "======================================================================\n",
      "SEED 456\n",
      "======================================================================\n",
      "\n",
      "  Training QuatNet (seed=456)...\n",
      "  [Quat] Epoch  1 | loss=1.0780 | test_acc=0.8103 | time=2.3s\n",
      "  [Quat] Epoch  2 | loss=0.3897 | test_acc=0.8270 | time=4.4s\n",
      "  [Quat] Epoch  3 | loss=0.3263 | test_acc=0.8310 | time=6.6s\n",
      "  [Quat] Epoch  4 | loss=0.3018 | test_acc=0.8380 | time=8.9s\n",
      "  [Quat] Epoch  5 | loss=0.2883 | test_acc=0.8420 | time=11.1s\n",
      "  [Quat] Epoch  6 | loss=0.2795 | test_acc=0.8397 | time=13.3s\n",
      "  [Quat] Epoch  7 | loss=0.2733 | test_acc=0.8427 | time=15.4s\n",
      "  [Quat] Epoch  8 | loss=0.2684 | test_acc=0.8410 | time=17.5s\n",
      "  [Quat] Epoch  9 | loss=0.2643 | test_acc=0.8407 | time=19.7s\n",
      "  [Quat] Epoch 10 | loss=0.2613 | test_acc=0.8407 | time=22.1s\n",
      "  [Quat] Epoch 11 | loss=0.2579 | test_acc=0.8403 | time=25.9s\n",
      "  [Quat] Epoch 12 | loss=0.2563 | test_acc=0.8390 | time=28.2s\n",
      "  [Quat] Epoch 13 | loss=0.2540 | test_acc=0.8377 | time=30.3s\n",
      "  [Quat] Epoch 14 | loss=0.2527 | test_acc=0.8393 | time=32.5s\n",
      "  [Quat] Epoch 15 | loss=0.2507 | test_acc=0.8400 | time=34.7s\n",
      "  [Quat] Epoch 16 | loss=0.2491 | test_acc=0.8393 | time=36.9s\n",
      "  [Quat] Epoch 17 | loss=0.2479 | test_acc=0.8440 | time=39.0s\n",
      "  [Quat] Epoch 18 | loss=0.2467 | test_acc=0.8393 | time=41.2s\n",
      "  [Quat] Epoch 19 | loss=0.2455 | test_acc=0.8387 | time=43.3s\n",
      "  [Quat] Epoch 20 | loss=0.2447 | test_acc=0.8400 | time=45.5s\n",
      "  [Quat] Epoch 21 | loss=0.2438 | test_acc=0.8427 | time=47.8s\n",
      "  [Quat] Epoch 22 | loss=0.2424 | test_acc=0.8413 | time=50.3s\n",
      "  [Quat] Epoch 23 | loss=0.2418 | test_acc=0.8423 | time=52.9s\n",
      "  [Quat] Epoch 24 | loss=0.2407 | test_acc=0.8420 | time=55.5s\n",
      "  [Quat] Epoch 25 | loss=0.2399 | test_acc=0.8423 | time=59.8s\n",
      "  [Quat] Epoch 26 | loss=0.2397 | test_acc=0.8423 | time=62.5s\n",
      "  [Quat] Epoch 27 | loss=0.2385 | test_acc=0.8390 | time=65.3s\n",
      "  [Quat] Early stop at epoch 27 (no improvement for 10 epochs)\n",
      "\n",
      "======================================================================\n",
      "QUATNET SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Accuracy:   0.8447 ± 0.0005\n",
      "Time:       56.9s ± 6.3s\n",
      "Epochs:     24.0 ± 2.4\n",
      "Parameters: 1,000 trainable (head), 13,560 total\n",
      "\n",
      "Per-seed results:\n",
      "  Seed 42: acc=0.8450, time=55.1s, epochs=24\n",
      "  Seed 123: acc=0.8450, time=50.2s, epochs=21\n",
      "  Seed 456: acc=0.8440, time=65.3s, epochs=27\n",
      "\n",
      "✓ Saved results to: quatnet_results.pt\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Block 2: QuatNet Training\n",
    "==========================\n",
    "Loads frozen preprocessor from Block 1 and trains quaternion head on 3 seeds.\n",
    "\n",
    "Requirements:\n",
    "- realnet_results.pt (from Block 1)\n",
    "\n",
    "Outputs:\n",
    "- quatnet_results.pt: Contains quaternion head results\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "print(f\"Using PyTorch device: {device}\")\n",
    "\n",
    "# ============================================\n",
    "# Comprehensive seed setting for reproducibility\n",
    "# ============================================\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    \"\"\"Set seeds for all RNG sources for reproducibility\"\"\"\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    # Python\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # NumPy\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # PyTorch deterministic mode\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # CuPy (if available)\n",
    "    try:\n",
    "        import cupy as cp\n",
    "        cp.random.seed(seed)\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    # PennyLane (if available)\n",
    "    try:\n",
    "        import pennylane as qml\n",
    "        qml.numpy.random.seed(seed)\n",
    "    except (ImportError, AttributeError):\n",
    "        pass\n",
    "\n",
    "# ============================================\n",
    "# Quaternion utilities (PyTorch tensors)\n",
    "# ============================================\n",
    "\n",
    "def q_normalize(q):\n",
    "    norm = torch.linalg.norm(q, dim=-1, keepdim=True) + 1e-8\n",
    "    return q / norm\n",
    "\n",
    "def q_conj(q):\n",
    "    w, x, y, z = torch.unbind(q, dim=-1)\n",
    "    return torch.stack([w, -x, -y, -z], dim=-1)\n",
    "\n",
    "def q_mul(a, b):\n",
    "    \"\"\"Hamilton product of two quaternions\"\"\"\n",
    "    aw, ax, ay, az = torch.unbind(a, dim=-1)\n",
    "    bw, bx, by, bz = torch.unbind(b, dim=-1)\n",
    "\n",
    "    w = aw * bw - ax * bx - ay * by - az * bz\n",
    "    x = aw * bx + ax * bw + ay * bz - az * by\n",
    "    y = aw * by - ax * bz + ay * bw + az * bx\n",
    "    z = aw * bz + ax * by - ay * bx + az * bw\n",
    "\n",
    "    return torch.stack([w, x, y, z], dim=-1)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Shared Preprocessor (must match Block 1)\n",
    "# ============================================\n",
    "\n",
    "class SharedPreprocessor(nn.Module):\n",
    "    \"\"\"Shared classical feature extractor: 784 → 16\"\"\"\n",
    "    def __init__(self, input_dim=784, bottleneck_dim=16):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, bottleneck_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.tanh(self.fc(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Quaternion Head and Network\n",
    "# ============================================\n",
    "\n",
    "class QuaternionLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        \"\"\"\n",
    "        in_features, out_features are in \"quaternion units\".\n",
    "        Internally weight: (out_features, in_features, 4)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.empty(out_features, in_features, 4))\n",
    "        self.bias = nn.Parameter(torch.empty(out_features, 4))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.normal_(self.weight, mean=0.0, std=0.1)\n",
    "        with torch.no_grad():\n",
    "            self.weight[:] = q_normalize(self.weight)\n",
    "            nn.init.constant_(self.bias[..., 0], 1.0)\n",
    "            nn.init.constant_(self.bias[..., 1:], 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, in_features, 4)\n",
    "        Returns: (B, out_features, 4)\n",
    "        \"\"\"\n",
    "        w = self.weight.unsqueeze(0)\n",
    "        x_exp = x.unsqueeze(1)\n",
    "        prod = q_mul(w, x_exp)\n",
    "        out = prod.sum(dim=2) + self.bias\n",
    "        return out\n",
    "\n",
    "\n",
    "class QuatHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Quaternion head: 4 quats → 16 quats → 10 quats → 10 logits.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.quat_fc1 = QuaternionLinear(4, 16)\n",
    "        self.quat_fc2 = QuaternionLinear(16, num_classes)\n",
    "\n",
    "    def real_to_quat(self, x):\n",
    "        \"\"\"Convert 16 real features to 4 quaternions\"\"\"\n",
    "        B = x.size(0)\n",
    "        return x.view(B, 4, 4)\n",
    "\n",
    "    def quat_to_real(self, q):\n",
    "        \"\"\"Extract real part of quaternions for classification\"\"\"\n",
    "        return q[..., 0]\n",
    "\n",
    "    def forward(self, x):\n",
    "        q_in = self.real_to_quat(x)\n",
    "        hq = self.quat_fc1(q_in)\n",
    "        hq = q_normalize(hq)\n",
    "        hq = torch.tanh(hq)\n",
    "        q_out = self.quat_fc2(hq)\n",
    "        logits = self.quat_to_real(q_out)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class QuatNet(nn.Module):\n",
    "    \"\"\"Complete Quaternion network: Preprocessor + QuatHead\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.preprocessor = SharedPreprocessor(784, 16)\n",
    "        self.head = QuatHead(10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.preprocessor(x)\n",
    "        return self.head(features)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Training and Evaluation\n",
    "# ============================================\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, device, show_progress=False):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        total_samples += x.size(0)\n",
    "\n",
    "        if show_progress and batch_idx % 50 == 0:\n",
    "            print(f\"    Batch {batch_idx}/{len(loader)}, samples: {total_samples}\", end=\"\\r\")\n",
    "\n",
    "    if show_progress:\n",
    "        print()\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += x.size(0)\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "def train_with_early_stopping(model, train_loader, test_loader, optimizer,\n",
    "                              device, max_epochs=40, patience=10, name=\"Model\"):\n",
    "    if isinstance(device, str):\n",
    "        device = torch.device(device)\n",
    "\n",
    "    best_acc = 0.0\n",
    "    epochs_without_improvement = 0\n",
    "    start = time.time()\n",
    "    last_acc = 0.0\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        loss = train_one_epoch(model, train_loader, optimizer, device, show_progress=False)\n",
    "        acc = evaluate(model, test_loader, device)\n",
    "        last_acc = acc\n",
    "\n",
    "        elapsed = time.time() - start\n",
    "        print(f\"  [{name}] Epoch {epoch:2d} | loss={loss:.4f} \"\n",
    "              f\"| test_acc={acc:.4f} | time={elapsed:.1f}s\")\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"  [{name}] Early stop at epoch {epoch} \"\n",
    "                  f\"(no improvement for {patience} epochs)\")\n",
    "            break\n",
    "\n",
    "    total_time = time.time() - start\n",
    "    return {\n",
    "        \"best_acc\": best_acc,\n",
    "        \"final_acc\": last_acc,\n",
    "        \"time\": total_time,\n",
    "        \"epochs\": epoch,\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Data utilities\n",
    "# ============================================\n",
    "\n",
    "def stratified_sample_from_targets(dataset, n_samples_per_class, seed=42):\n",
    "    \"\"\"\n",
    "    Create a stratified sample with n_samples_per_class from each class.\n",
    "    Uses dataset.targets directly - NO image loading during sampling.\n",
    "    \"\"\"\n",
    "    # FashionMNIST and MNIST expose targets as a tensor\n",
    "    targets = dataset.targets\n",
    "    rng = np.random.RandomState(seed)\n",
    "    \n",
    "    sampled_indices = []\n",
    "    for c in range(10):\n",
    "        # Find all indices for class c\n",
    "        idx_c = (targets == c).nonzero(as_tuple=False).view(-1).cpu().numpy()\n",
    "        k = min(n_samples_per_class, len(idx_c))\n",
    "        # Sample without replacement\n",
    "        selected = rng.choice(idx_c, size=k, replace=False).tolist()\n",
    "        sampled_indices.extend(selected)\n",
    "    \n",
    "    # Shuffle the combined indices\n",
    "    rng.shuffle(sampled_indices)\n",
    "    return sampled_indices\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Main\n",
    "# ============================================\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"BLOCK 2: QuatNet Training (Frozen Preprocessor)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nConfiguration:\")\n",
    "    print(\"  • Batch size: 32\")\n",
    "    print(\"  • Patience: 10\")\n",
    "    print(\"  • Seeds: [42, 123, 456]\")\n",
    "    print(\"  • Architecture: [frozen 784→16] → 4 quats → 16 quats → 10\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Load frozen preprocessor from Block 1\n",
    "    print(\"\\nLoading frozen preprocessor from Block 1...\")\n",
    "    try:\n",
    "        realnet_data = torch.load(\"realnet_results.pt\", weights_only=False)\n",
    "        preprocessor_state = realnet_data[\"preprocessor_state\"]\n",
    "        print(\"✓ Loaded preprocessor state from realnet_results.pt\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"✗ ERROR: realnet_results.pt not found!\")\n",
    "        print(\"  Run Block 1 first to train RealNet and save preprocessor.\")\n",
    "        return\n",
    "\n",
    "    # Create frozen preprocessor\n",
    "    shared_preprocessor = SharedPreprocessor(784, 16).to(device)\n",
    "    shared_preprocessor.load_state_dict(preprocessor_state)\n",
    "    for p in shared_preprocessor.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    preprocessor_params = sum(p.numel() for p in shared_preprocessor.parameters())\n",
    "    print(f\"  Preprocessor frozen with {preprocessor_params:,} params\")\n",
    "\n",
    "    # Load data\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.2860,), (0.3530,))\n",
    "    ])\n",
    "\n",
    "    full_train_ds = datasets.FashionMNIST(root=\"./data\", train=True,\n",
    "                                         download=True, transform=transform)\n",
    "    full_test_ds = datasets.FashionMNIST(root=\"./data\", train=False,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "    # Create stratified samples (fast - uses targets only, no image loading)\n",
    "    print(\"\\nCreating stratified samples...\")\n",
    "    t0 = time.time()\n",
    "    train_indices = stratified_sample_from_targets(full_train_ds, n_samples_per_class=1500, seed=42)\n",
    "    test_indices = stratified_sample_from_targets(full_test_ds, n_samples_per_class=300, seed=42)\n",
    "    print(f\"  Sampling took {time.time()-t0:.2f}s\")\n",
    "    \n",
    "    train_ds = Subset(full_train_ds, train_indices)\n",
    "    test_ds = Subset(full_test_ds, test_indices)\n",
    "    \n",
    "    print(f\"  Train samples: {len(train_ds)} (stratified, 1500 per class)\")\n",
    "    print(f\"  Test samples:  {len(test_ds)} (stratified, 300 per class)\")\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=32,\n",
    "                              shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_ds, batch_size=64,\n",
    "                             shuffle=False, num_workers=0)\n",
    "\n",
    "    seeds = [42, 123, 456]\n",
    "    all_results = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(f\"SEED {seed}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        set_all_seeds(seed)\n",
    "\n",
    "        print(f\"\\n  Training QuatNet (seed={seed})...\")\n",
    "        quat_model = QuatNet().to(device)\n",
    "        quat_model.preprocessor = shared_preprocessor  # Use frozen preprocessor\n",
    "        \n",
    "        quat_opt = torch.optim.Adam(quat_model.head.parameters(), lr=1e-3)\n",
    "        \n",
    "        result = train_with_early_stopping(\n",
    "            quat_model, train_loader, test_loader, quat_opt, device,\n",
    "            max_epochs=40, patience=10, name=\"Quat\"\n",
    "        )\n",
    "        \n",
    "        result[\"trainable_params\"] = sum(p.numel() for p in quat_model.parameters()\n",
    "                                         if p.requires_grad)\n",
    "        result[\"total_params\"] = sum(p.numel() for p in quat_model.parameters())\n",
    "        result[\"seed\"] = seed\n",
    "        \n",
    "        all_results.append(result)\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"QUATNET SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    accs = [r[\"best_acc\"] for r in all_results]\n",
    "    times = [r[\"time\"] for r in all_results]\n",
    "    epochs = [r[\"epochs\"] for r in all_results]\n",
    "    trainable = all_results[0][\"trainable_params\"]\n",
    "    total = all_results[0][\"total_params\"]\n",
    "    \n",
    "    print(f\"\\nAccuracy:   {np.mean(accs):.4f} ± {np.std(accs):.4f}\")\n",
    "    print(f\"Time:       {np.mean(times):.1f}s ± {np.std(times):.1f}s\")\n",
    "    print(f\"Epochs:     {np.mean(epochs):.1f} ± {np.std(epochs):.1f}\")\n",
    "    print(f\"Parameters: {trainable:,} trainable (head), {total:,} total\")\n",
    "    \n",
    "    print(\"\\nPer-seed results:\")\n",
    "    for r in all_results:\n",
    "        print(f\"  Seed {r['seed']}: acc={r['best_acc']:.4f}, \"\n",
    "              f\"time={r['time']:.1f}s, epochs={r['epochs']}\")\n",
    "\n",
    "    # Save results\n",
    "    save_dict = {\n",
    "        \"results\": all_results,\n",
    "        \"summary\": {\n",
    "            \"mean_acc\": np.mean(accs),\n",
    "            \"std_acc\": np.std(accs),\n",
    "            \"mean_time\": np.mean(times),\n",
    "            \"trainable_params\": trainable,\n",
    "            \"total_params\": total\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    torch.save(save_dict, \"quatnet_results.pt\")\n",
    "    print(f\"\\n✓ Saved results to: quatnet_results.pt\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c266c5-3d08-4449-9f9d-905366d51b71",
   "metadata": {},
   "source": [
    "# Quant No Ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3848f591-dc2d-4283-a3cb-72f381cef549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using lightning.gpu device\n",
      "Using PyTorch device: cuda\n",
      "======================================================================\n",
      "BLOCK 3: Quantum (NO Entanglement) Training\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "  • Batch size: 32\n",
      "  • Patience: 10\n",
      "  • Max epochs: 200\n",
      "  • Seeds: [42, 123, 456]\n",
      "  • Architecture: [frozen 784→16] → 4 qubits (3 layers, NO ent) → 10\n",
      "  • Quantum device: lightning.gpu\n",
      "======================================================================\n",
      "\n",
      "Loading frozen preprocessor from Block 1...\n",
      "✓ Loaded preprocessor state from realnet_results.pt\n",
      "  Preprocessor frozen with 12,560 params\n",
      "\n",
      "Creating stratified samples...\n",
      "  Sampling took 0.00s\n",
      "  Train samples: 15000 (stratified, 1500 per class)\n",
      "  Test samples:  3000 (stratified, 300 per class)\n",
      "\n",
      "======================================================================\n",
      "SEED 42\n",
      "======================================================================\n",
      "\n",
      "  Training QuantumNet (NO entanglement, seed=42)...\n",
      "  [QuantumNoEnt] Epoch 1/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  1 | loss=1.7367 | test_acc=0.5983 | time=440.0s\n",
      "  [QuantumNoEnt] Epoch 2/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  2 | loss=1.0365 | test_acc=0.7277 | time=890.4s\n",
      "  [QuantumNoEnt] Epoch 3/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  3 | loss=0.7860 | test_acc=0.7513 | time=1341.5s\n",
      "  [QuantumNoEnt] Epoch 4/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  4 | loss=0.6637 | test_acc=0.7570 | time=1781.3s\n",
      "  [QuantumNoEnt] Epoch 5/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  5 | loss=0.5991 | test_acc=0.7650 | time=2243.2s\n",
      "  [QuantumNoEnt] Epoch 6/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  6 | loss=0.5570 | test_acc=0.7873 | time=2687.1s\n",
      "  [QuantumNoEnt] Epoch 7/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  7 | loss=0.5269 | test_acc=0.7887 | time=3146.0s\n",
      "  [QuantumNoEnt] Epoch 8/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  8 | loss=0.5027 | test_acc=0.7910 | time=3613.0s\n",
      "  [QuantumNoEnt] Epoch 9/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  9 | loss=0.4835 | test_acc=0.7950 | time=4056.5s\n",
      "  [QuantumNoEnt] Epoch 10/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 10 | loss=0.4672 | test_acc=0.7973 | time=4520.2s\n",
      "  [QuantumNoEnt] Epoch 11/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 11 | loss=0.4552 | test_acc=0.7973 | time=4989.3s\n",
      "  [QuantumNoEnt] Epoch 12/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 12 | loss=0.4437 | test_acc=0.8043 | time=5466.9s\n",
      "  [QuantumNoEnt] Epoch 13/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 13 | loss=0.4356 | test_acc=0.8060 | time=5937.5s\n",
      "  [QuantumNoEnt] Epoch 14/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 14 | loss=0.4283 | test_acc=0.8047 | time=6408.1s\n",
      "  [QuantumNoEnt] Epoch 15/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 15 | loss=0.4228 | test_acc=0.8113 | time=6877.3s\n",
      "  [QuantumNoEnt] Epoch 16/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 16 | loss=0.4172 | test_acc=0.8083 | time=7344.7s\n",
      "  [QuantumNoEnt] Epoch 17/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 17 | loss=0.4132 | test_acc=0.8070 | time=7807.6s\n",
      "  [QuantumNoEnt] Epoch 18/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 18 | loss=0.4087 | test_acc=0.8107 | time=8255.6s\n",
      "  [QuantumNoEnt] Epoch 19/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 19 | loss=0.4052 | test_acc=0.8070 | time=8722.0s\n",
      "  [QuantumNoEnt] Epoch 20/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 20 | loss=0.4022 | test_acc=0.8097 | time=9203.5s\n",
      "  [QuantumNoEnt] Epoch 21/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 21 | loss=0.4007 | test_acc=0.8120 | time=9668.6s\n",
      "  [QuantumNoEnt] Epoch 22/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 22 | loss=0.3972 | test_acc=0.8083 | time=10137.0s\n",
      "  [QuantumNoEnt] Epoch 23/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 23 | loss=0.3969 | test_acc=0.8090 | time=10591.8s\n",
      "  [QuantumNoEnt] Epoch 24/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 24 | loss=0.3930 | test_acc=0.8127 | time=11053.8s\n",
      "  [QuantumNoEnt] Epoch 25/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 25 | loss=0.3906 | test_acc=0.8107 | time=11511.6s\n",
      "  [QuantumNoEnt] Epoch 26/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 26 | loss=0.3904 | test_acc=0.8117 | time=11961.4s\n",
      "  [QuantumNoEnt] Epoch 27/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 27 | loss=0.3875 | test_acc=0.8060 | time=12416.2s\n",
      "  [QuantumNoEnt] Epoch 28/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 28 | loss=0.3881 | test_acc=0.8157 | time=12878.8s\n",
      "  [QuantumNoEnt] Epoch 29/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 29 | loss=0.3858 | test_acc=0.8133 | time=13329.9s\n",
      "  [QuantumNoEnt] Epoch 30/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 30 | loss=0.3851 | test_acc=0.8133 | time=13793.9s\n",
      "  [QuantumNoEnt] Epoch 31/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 31 | loss=0.3826 | test_acc=0.8160 | time=14269.9s\n",
      "  [QuantumNoEnt] Epoch 32/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 32 | loss=0.3824 | test_acc=0.8160 | time=14735.6s\n",
      "  [QuantumNoEnt] Epoch 33/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 33 | loss=0.3815 | test_acc=0.8157 | time=15200.9s\n",
      "  [QuantumNoEnt] Epoch 34/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 34 | loss=0.3801 | test_acc=0.8173 | time=15677.5s\n",
      "  [QuantumNoEnt] Epoch 35/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 35 | loss=0.3792 | test_acc=0.8133 | time=16148.0s\n",
      "  [QuantumNoEnt] Epoch 36/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 36 | loss=0.3782 | test_acc=0.8123 | time=16621.6s\n",
      "  [QuantumNoEnt] Epoch 37/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 37 | loss=0.3772 | test_acc=0.8143 | time=17083.7s\n",
      "  [QuantumNoEnt] Epoch 38/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 38 | loss=0.3764 | test_acc=0.8147 | time=17542.4s\n",
      "  [QuantumNoEnt] Epoch 39/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 39 | loss=0.3759 | test_acc=0.8150 | time=18004.7s\n",
      "  [QuantumNoEnt] Epoch 40/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 40 | loss=0.3733 | test_acc=0.8173 | time=18469.4s\n",
      "  [QuantumNoEnt] Epoch 41/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 41 | loss=0.3747 | test_acc=0.8173 | time=18938.7s\n",
      "  [QuantumNoEnt] Epoch 42/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 42 | loss=0.3725 | test_acc=0.8150 | time=19399.0s\n",
      "  [QuantumNoEnt] Epoch 43/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 43 | loss=0.3724 | test_acc=0.8167 | time=19856.3s\n",
      "  [QuantumNoEnt] Epoch 44/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 44 | loss=0.3708 | test_acc=0.8150 | time=20324.1s\n",
      "  [QuantumNoEnt] Early stop at epoch 44 (no improvement for 10 epochs)\n",
      "\n",
      "======================================================================\n",
      "SEED 123\n",
      "======================================================================\n",
      "\n",
      "  Training QuantumNet (NO entanglement, seed=123)...\n",
      "  [QuantumNoEnt] Epoch 1/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  1 | loss=1.7326 | test_acc=0.6617 | time=466.4s\n",
      "  [QuantumNoEnt] Epoch 2/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  2 | loss=1.0887 | test_acc=0.7007 | time=923.3s\n",
      "  [QuantumNoEnt] Epoch 3/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  3 | loss=0.8099 | test_acc=0.7163 | time=1379.6s\n",
      "  [QuantumNoEnt] Epoch 4/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  4 | loss=0.7036 | test_acc=0.7557 | time=1840.9s\n",
      "  [QuantumNoEnt] Epoch 5/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  5 | loss=0.6483 | test_acc=0.7730 | time=2295.1s\n",
      "  [QuantumNoEnt] Epoch 6/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  6 | loss=0.6030 | test_acc=0.7970 | time=2742.3s\n",
      "  [QuantumNoEnt] Epoch 7/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  7 | loss=0.5604 | test_acc=0.7970 | time=3220.7s\n",
      "  [QuantumNoEnt] Epoch 8/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  8 | loss=0.5294 | test_acc=0.7940 | time=3682.3s\n",
      "  [QuantumNoEnt] Epoch 9/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  9 | loss=0.5095 | test_acc=0.8053 | time=4137.6s\n",
      "  [QuantumNoEnt] Epoch 10/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 10 | loss=0.4955 | test_acc=0.8000 | time=4591.5s\n",
      "  [QuantumNoEnt] Epoch 11/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 11 | loss=0.4855 | test_acc=0.7953 | time=5049.0s\n",
      "  [QuantumNoEnt] Epoch 12/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 12 | loss=0.4784 | test_acc=0.8013 | time=5512.5s\n",
      "  [QuantumNoEnt] Epoch 13/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 13 | loss=0.4721 | test_acc=0.7960 | time=5973.8s\n",
      "  [QuantumNoEnt] Epoch 14/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 14 | loss=0.4671 | test_acc=0.8040 | time=6443.5s\n",
      "  [QuantumNoEnt] Epoch 15/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 15 | loss=0.4640 | test_acc=0.8017 | time=6909.0s\n",
      "  [QuantumNoEnt] Epoch 16/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 16 | loss=0.4600 | test_acc=0.8020 | time=7374.5s\n",
      "  [QuantumNoEnt] Epoch 17/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 17 | loss=0.4564 | test_acc=0.8027 | time=7847.9s\n",
      "  [QuantumNoEnt] Epoch 18/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 18 | loss=0.4547 | test_acc=0.8057 | time=8306.3s\n",
      "  [QuantumNoEnt] Epoch 19/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 19 | loss=0.4524 | test_acc=0.8017 | time=8758.7s\n",
      "  [QuantumNoEnt] Epoch 20/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 20 | loss=0.4506 | test_acc=0.8030 | time=9214.3s\n",
      "  [QuantumNoEnt] Epoch 21/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 21 | loss=0.4480 | test_acc=0.8047 | time=9677.6s\n",
      "  [QuantumNoEnt] Epoch 22/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 22 | loss=0.4471 | test_acc=0.8023 | time=10122.5s\n",
      "  [QuantumNoEnt] Epoch 23/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 23 | loss=0.4447 | test_acc=0.8057 | time=10574.6s\n",
      "  [QuantumNoEnt] Epoch 24/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 24 | loss=0.4441 | test_acc=0.8060 | time=11030.3s\n",
      "  [QuantumNoEnt] Epoch 25/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 25 | loss=0.4424 | test_acc=0.8037 | time=11486.9s\n",
      "  [QuantumNoEnt] Epoch 26/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 26 | loss=0.4411 | test_acc=0.8057 | time=11946.7s\n",
      "  [QuantumNoEnt] Epoch 27/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 27 | loss=0.4405 | test_acc=0.8047 | time=12400.6s\n",
      "  [QuantumNoEnt] Epoch 28/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 28 | loss=0.4386 | test_acc=0.8060 | time=12872.4s\n",
      "  [QuantumNoEnt] Epoch 29/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 29 | loss=0.4381 | test_acc=0.8047 | time=13353.9s\n",
      "  [QuantumNoEnt] Epoch 30/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 30 | loss=0.4358 | test_acc=0.8093 | time=13824.7s\n",
      "  [QuantumNoEnt] Epoch 31/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 31 | loss=0.4352 | test_acc=0.8070 | time=14283.1s\n",
      "  [QuantumNoEnt] Epoch 32/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 32 | loss=0.4342 | test_acc=0.8003 | time=14756.9s\n",
      "  [QuantumNoEnt] Epoch 33/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 33 | loss=0.4335 | test_acc=0.8043 | time=15215.4s\n",
      "  [QuantumNoEnt] Epoch 34/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 34 | loss=0.4318 | test_acc=0.8083 | time=15675.0s\n",
      "  [QuantumNoEnt] Epoch 35/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 35 | loss=0.4304 | test_acc=0.8107 | time=16140.1s\n",
      "  [QuantumNoEnt] Epoch 36/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 36 | loss=0.4283 | test_acc=0.8037 | time=16612.6s\n",
      "  [QuantumNoEnt] Epoch 37/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 37 | loss=0.4282 | test_acc=0.8043 | time=17061.4s\n",
      "  [QuantumNoEnt] Epoch 38/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 38 | loss=0.4258 | test_acc=0.8063 | time=17523.2s\n",
      "  [QuantumNoEnt] Epoch 39/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 39 | loss=0.4243 | test_acc=0.8063 | time=17999.1s\n",
      "  [QuantumNoEnt] Epoch 40/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 40 | loss=0.4226 | test_acc=0.8097 | time=18463.3s\n",
      "  [QuantumNoEnt] Epoch 41/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 41 | loss=0.4209 | test_acc=0.8100 | time=18934.2s\n",
      "  [QuantumNoEnt] Epoch 42/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 42 | loss=0.4187 | test_acc=0.8080 | time=19411.9s\n",
      "  [QuantumNoEnt] Epoch 43/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 43 | loss=0.4166 | test_acc=0.8073 | time=19888.3s\n",
      "  [QuantumNoEnt] Epoch 44/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 44 | loss=0.4148 | test_acc=0.8087 | time=20362.0s\n",
      "  [QuantumNoEnt] Epoch 45/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 45 | loss=0.4128 | test_acc=0.8070 | time=20844.4s\n",
      "  [QuantumNoEnt] Early stop at epoch 45 (no improvement for 10 epochs)\n",
      "\n",
      "======================================================================\n",
      "SEED 456\n",
      "======================================================================\n",
      "\n",
      "  Training QuantumNet (NO entanglement, seed=456)...\n",
      "  [QuantumNoEnt] Epoch 1/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  1 | loss=1.6683 | test_acc=0.6633 | time=478.1s\n",
      "  [QuantumNoEnt] Epoch 2/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  2 | loss=1.0043 | test_acc=0.7507 | time=940.0s\n",
      "  [QuantumNoEnt] Epoch 3/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  3 | loss=0.7363 | test_acc=0.7537 | time=1417.5s\n",
      "  [QuantumNoEnt] Epoch 4/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  4 | loss=0.6406 | test_acc=0.7667 | time=1913.6s\n",
      "  [QuantumNoEnt] Epoch 5/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  5 | loss=0.5941 | test_acc=0.7597 | time=2400.6s\n",
      "  [QuantumNoEnt] Epoch 6/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  6 | loss=0.5665 | test_acc=0.7610 | time=2886.0s\n",
      "  [QuantumNoEnt] Epoch 7/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  7 | loss=0.5469 | test_acc=0.7647 | time=3364.3s\n",
      "  [QuantumNoEnt] Epoch 8/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  8 | loss=0.5314 | test_acc=0.7717 | time=3850.1s\n",
      "  [QuantumNoEnt] Epoch 9/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch  9 | loss=0.5205 | test_acc=0.7793 | time=4358.5s\n",
      "  [QuantumNoEnt] Epoch 10/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 10 | loss=0.5094 | test_acc=0.7813 | time=4857.9s\n",
      "  [QuantumNoEnt] Epoch 11/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 11 | loss=0.5011 | test_acc=0.7853 | time=5330.1s\n",
      "  [QuantumNoEnt] Epoch 12/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 12 | loss=0.4929 | test_acc=0.7940 | time=5839.1s\n",
      "  [QuantumNoEnt] Epoch 13/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 13 | loss=0.4858 | test_acc=0.7977 | time=6323.2s\n",
      "  [QuantumNoEnt] Epoch 14/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 14 | loss=0.4774 | test_acc=0.7987 | time=6816.1s\n",
      "  [QuantumNoEnt] Epoch 15/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 15 | loss=0.4702 | test_acc=0.8023 | time=7311.1s\n",
      "  [QuantumNoEnt] Epoch 16/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 16 | loss=0.4625 | test_acc=0.8027 | time=7795.6s\n",
      "  [QuantumNoEnt] Epoch 17/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 17 | loss=0.4547 | test_acc=0.8067 | time=8279.8s\n",
      "  [QuantumNoEnt] Epoch 18/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 18 | loss=0.4488 | test_acc=0.8113 | time=8759.1s\n",
      "  [QuantumNoEnt] Epoch 19/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 19 | loss=0.4436 | test_acc=0.8093 | time=9233.5s\n",
      "  [QuantumNoEnt] Epoch 20/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 20 | loss=0.4382 | test_acc=0.8090 | time=9732.3s\n",
      "  [QuantumNoEnt] Epoch 21/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 21 | loss=0.4337 | test_acc=0.8153 | time=10231.2s\n",
      "  [QuantumNoEnt] Epoch 22/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 22 | loss=0.4296 | test_acc=0.8133 | time=10704.7s\n",
      "  [QuantumNoEnt] Epoch 23/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 23 | loss=0.4259 | test_acc=0.8143 | time=11200.2s\n",
      "  [QuantumNoEnt] Epoch 24/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 24 | loss=0.4227 | test_acc=0.8150 | time=11675.3s\n",
      "  [QuantumNoEnt] Epoch 25/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 25 | loss=0.4189 | test_acc=0.8170 | time=12162.5s\n",
      "  [QuantumNoEnt] Epoch 26/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 26 | loss=0.4149 | test_acc=0.8143 | time=12663.9s\n",
      "  [QuantumNoEnt] Epoch 27/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 27 | loss=0.4131 | test_acc=0.8170 | time=13155.2s\n",
      "  [QuantumNoEnt] Epoch 28/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 28 | loss=0.4087 | test_acc=0.8190 | time=13648.8s\n",
      "  [QuantumNoEnt] Epoch 29/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 29 | loss=0.4080 | test_acc=0.8170 | time=14140.2s\n",
      "  [QuantumNoEnt] Epoch 30/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 30 | loss=0.4037 | test_acc=0.8200 | time=14615.9s\n",
      "  [QuantumNoEnt] Epoch 31/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 31 | loss=0.4016 | test_acc=0.8173 | time=15104.1s\n",
      "  [QuantumNoEnt] Epoch 32/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 32 | loss=0.3984 | test_acc=0.8210 | time=15596.0s\n",
      "  [QuantumNoEnt] Epoch 33/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 33 | loss=0.3953 | test_acc=0.8220 | time=16081.3s\n",
      "  [QuantumNoEnt] Epoch 34/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 34 | loss=0.3929 | test_acc=0.8200 | time=16570.7s\n",
      "  [QuantumNoEnt] Epoch 35/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 35 | loss=0.3921 | test_acc=0.8217 | time=17055.7s\n",
      "  [QuantumNoEnt] Epoch 36/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 36 | loss=0.3878 | test_acc=0.8243 | time=17546.2s\n",
      "  [QuantumNoEnt] Epoch 37/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 37 | loss=0.3851 | test_acc=0.8253 | time=18040.1s\n",
      "  [QuantumNoEnt] Epoch 38/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 38 | loss=0.3829 | test_acc=0.8187 | time=18533.6s\n",
      "  [QuantumNoEnt] Epoch 39/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 39 | loss=0.3796 | test_acc=0.8243 | time=19028.7s\n",
      "  [QuantumNoEnt] Epoch 40/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 40 | loss=0.3769 | test_acc=0.8220 | time=19522.1s\n",
      "  [QuantumNoEnt] Epoch 41/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 41 | loss=0.3735 | test_acc=0.8263 | time=19995.4s\n",
      "  [QuantumNoEnt] Epoch 42/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 42 | loss=0.3723 | test_acc=0.8247 | time=20481.3s\n",
      "  [QuantumNoEnt] Epoch 43/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 43 | loss=0.3693 | test_acc=0.8330 | time=20975.5s\n",
      "  [QuantumNoEnt] Epoch 44/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 44 | loss=0.3670 | test_acc=0.8283 | time=21461.4s\n",
      "  [QuantumNoEnt] Epoch 45/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 45 | loss=0.3665 | test_acc=0.8280 | time=21954.3s\n",
      "  [QuantumNoEnt] Epoch 46/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 46 | loss=0.3643 | test_acc=0.8283 | time=22447.9s\n",
      "  [QuantumNoEnt] Epoch 47/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 47 | loss=0.3632 | test_acc=0.8320 | time=22931.4s\n",
      "  [QuantumNoEnt] Epoch 48/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 48 | loss=0.3608 | test_acc=0.8257 | time=23433.2s\n",
      "  [QuantumNoEnt] Epoch 49/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 49 | loss=0.3599 | test_acc=0.8290 | time=23880.0s\n",
      "  [QuantumNoEnt] Epoch 50/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 50 | loss=0.3588 | test_acc=0.8320 | time=24327.3s\n",
      "  [QuantumNoEnt] Epoch 51/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 51 | loss=0.3585 | test_acc=0.8277 | time=24778.0s\n",
      "  [QuantumNoEnt] Epoch 52/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 52 | loss=0.3578 | test_acc=0.8300 | time=25240.8s\n",
      "  [QuantumNoEnt] Epoch 53/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumNoEnt] Epoch 53 | loss=0.3557 | test_acc=0.8330 | time=25685.7s\n",
      "  [QuantumNoEnt] Early stop at epoch 53 (no improvement for 10 epochs)\n",
      "\n",
      "======================================================================\n",
      "QUANTUM (NO ENTANGLEMENT) SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Accuracy:   0.8203 ± 0.0094\n",
      "Time:       22284.7s ± 2414.2s\n",
      "Epochs:     47.3 ± 4.0\n",
      "Parameters: 162 trainable (head), 12,722 total\n",
      "\n",
      "Per-seed results:\n",
      "  Seed 42: acc=0.8173, time=20324.1s, epochs=44\n",
      "  Seed 123: acc=0.8107, time=20844.4s, epochs=45\n",
      "  Seed 456: acc=0.8330, time=25685.7s, epochs=53\n",
      "\n",
      "✓ Saved results to: quantum_noent_results.pt\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Block 3: Quantum (No Entanglement) Training\n",
    "============================================\n",
    "Loads frozen preprocessor from Block 1 and trains quantum head WITHOUT entanglement.\n",
    "Uses Lightning-GPU acceleration.\n",
    "\n",
    "Requirements:\n",
    "- realnet_results.pt (from Block 1)\n",
    "- pennylane, pennylane-lightning-gpu\n",
    "\n",
    "Outputs:\n",
    "- quantum_noent_results.pt: Contains quantum (no ent) results\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "try:\n",
    "    import pennylane as qml\n",
    "    QUANTUM_DEVICE = \"lightning.gpu\"\n",
    "    PENNYLANE_AVAILABLE = True\n",
    "    print(\"✓ Using lightning.gpu device\")\n",
    "except ImportError:\n",
    "    PENNYLANE_AVAILABLE = False\n",
    "    print(\"✗ PennyLane not installed\")\n",
    "    exit(1)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "print(f\"Using PyTorch device: {device}\")\n",
    "\n",
    "# ============================================\n",
    "# Comprehensive seed setting for reproducibility\n",
    "# ============================================\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    \"\"\"Set seeds for all RNG sources for reproducibility\"\"\"\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    # Python\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # NumPy\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # PyTorch deterministic mode\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # CuPy (if available)\n",
    "    try:\n",
    "        import cupy as cp\n",
    "        cp.random.seed(seed)\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    # PennyLane (if available)\n",
    "    try:\n",
    "        import pennylane as qml\n",
    "        qml.numpy.random.seed(seed)\n",
    "    except (ImportError, AttributeError):\n",
    "        pass\n",
    "\n",
    "# ============================================\n",
    "# Shared Preprocessor (must match Block 1)\n",
    "# ============================================\n",
    "\n",
    "class SharedPreprocessor(nn.Module):\n",
    "    \"\"\"Shared classical feature extractor: 784 → 16\"\"\"\n",
    "    def __init__(self, input_dim=784, bottleneck_dim=16):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, bottleneck_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.tanh(self.fc(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Quantum Head (No Entanglement)\n",
    "# ============================================\n",
    "\n",
    "class QuantumHead(nn.Module):\n",
    "    \"\"\"\n",
    "    VQC with 4 qubits, 3 layers, NO entanglement → 10 classes.\n",
    "    Uses Lightning acceleration (GPU).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_qubits=4, n_layers=3, num_classes=10, device_name=None):\n",
    "        super().__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Map 16 features → n_qubits\n",
    "        self.feature_select = nn.Linear(16, n_qubits)\n",
    "\n",
    "        # Use specified device or default\n",
    "        if device_name is None:\n",
    "            device_name = QUANTUM_DEVICE\n",
    "        \n",
    "        # Quantum device\n",
    "        self.dev = qml.device(device_name, wires=n_qubits)\n",
    "\n",
    "        # Use adjoint differentiation for lightning (much faster)\n",
    "        diff_method = \"adjoint\" if \"lightning\" in device_name else \"parameter-shift\"\n",
    "\n",
    "        @qml.qnode(self.dev, interface=\"torch\", diff_method=diff_method)\n",
    "        def quantum_circuit(inputs, weights):\n",
    "            \"\"\"\n",
    "            Single-sample circuit WITHOUT entanglement.\n",
    "            inputs: (n_qubits,)\n",
    "            weights: (n_layers, n_qubits, 2)\n",
    "            \"\"\"\n",
    "            for layer in range(n_layers):\n",
    "                # Data re-uploading\n",
    "                for i in range(n_qubits):\n",
    "                    qml.RY(inputs[i], wires=i)\n",
    "\n",
    "                # Trainable rotations\n",
    "                for i in range(n_qubits):\n",
    "                    qml.RY(weights[layer, i, 0], wires=i)\n",
    "                    qml.RZ(weights[layer, i, 1], wires=i)\n",
    "\n",
    "                # NO ENTANGLEMENT\n",
    "\n",
    "            return [\n",
    "                qml.expval(qml.PauliZ(0)),\n",
    "                qml.expval(qml.PauliZ(1)),\n",
    "                qml.expval(qml.PauliZ(2)),\n",
    "                qml.expval(qml.PauliZ(3)),\n",
    "                qml.expval(qml.PauliZ(0) @ qml.PauliZ(1)),\n",
    "                qml.expval(qml.PauliZ(2) @ qml.PauliZ(3))\n",
    "            ]\n",
    "\n",
    "        self.quantum_circuit = quantum_circuit\n",
    "\n",
    "        weight_shape = (n_layers, n_qubits, 2)\n",
    "        self.q_weights = nn.Parameter(torch.randn(weight_shape) * 0.1)\n",
    "        self.fc_out = nn.Linear(6, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Process samples with progress tracking.\n",
    "        x: (batch, 16) real bottleneck features\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        x = torch.tanh(self.feature_select(x))\n",
    "\n",
    "        # Process in chunks for memory management\n",
    "        chunk_size = 32\n",
    "        quantum_outputs = []\n",
    "\n",
    "        for start_idx in range(0, batch_size, chunk_size):\n",
    "            end_idx = min(start_idx + chunk_size, batch_size)\n",
    "            chunk = x[start_idx:end_idx]\n",
    "\n",
    "            chunk_outputs = []\n",
    "            for i in range(chunk.size(0)):\n",
    "                q_raw = self.quantum_circuit(chunk[i], self.q_weights)\n",
    "                if isinstance(q_raw, (list, tuple)):\n",
    "                    q_out = torch.stack(q_raw)\n",
    "                else:\n",
    "                    q_out = q_raw\n",
    "                chunk_outputs.append(q_out)\n",
    "\n",
    "            quantum_outputs.extend(chunk_outputs)\n",
    "\n",
    "        # Convert to tensor (cast to float32)\n",
    "        quantum_outputs = torch.stack(quantum_outputs).float()\n",
    "        quantum_outputs = quantum_outputs.to(self.fc_out.weight.dtype)\n",
    "\n",
    "        output = self.fc_out(quantum_outputs)\n",
    "        return output\n",
    "\n",
    "\n",
    "class QuantumNet(nn.Module):\n",
    "    \"\"\"Complete Quantum network: Preprocessor + QuantumHead (no ent)\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.preprocessor = SharedPreprocessor(784, 16)\n",
    "        self.head = QuantumHead(n_qubits=4, n_layers=3, num_classes=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.preprocessor(x)\n",
    "        return self.head(features)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Training and Evaluation\n",
    "# ============================================\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, show_progress=True):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(loader):\n",
    "        # Keep data on CPU for quantum models\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        total_samples += x.size(0)\n",
    "\n",
    "        if show_progress and batch_idx % 20 == 0:\n",
    "            print(f\"    Batch {batch_idx}/{len(loader)}, samples: {total_samples}\", end=\"\\r\")\n",
    "\n",
    "    if show_progress:\n",
    "        print()\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            # Keep data on CPU\n",
    "            logits = model(x)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += x.size(0)\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "def train_with_early_stopping(model, train_loader, test_loader, optimizer,\n",
    "                              max_epochs=200, patience=10, name=\"Model\"):\n",
    "    best_acc = 0.0\n",
    "    epochs_without_improvement = 0\n",
    "    start = time.time()\n",
    "    last_acc = 0.0\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        print(f\"  [{name}] Epoch {epoch}/{max_epochs}\")\n",
    "        loss = train_one_epoch(model, train_loader, optimizer, show_progress=True)\n",
    "        acc = evaluate(model, test_loader)\n",
    "        last_acc = acc\n",
    "\n",
    "        elapsed = time.time() - start\n",
    "        print(f\"  [{name}] Epoch {epoch:2d} | loss={loss:.4f} \"\n",
    "              f\"| test_acc={acc:.4f} | time={elapsed:.1f}s\")\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"  [{name}] Early stop at epoch {epoch} \"\n",
    "                  f\"(no improvement for {patience} epochs)\")\n",
    "            break\n",
    "\n",
    "    total_time = time.time() - start\n",
    "    return {\n",
    "        \"best_acc\": best_acc,\n",
    "        \"final_acc\": last_acc,\n",
    "        \"time\": total_time,\n",
    "        \"epochs\": epoch,\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Data utilities\n",
    "# ============================================\n",
    "\n",
    "def stratified_sample_from_targets(dataset, n_samples_per_class, seed=42):\n",
    "    \"\"\"\n",
    "    Create a stratified sample with n_samples_per_class from each class.\n",
    "    Uses dataset.targets directly - NO image loading during sampling.\n",
    "    \"\"\"\n",
    "    # FashionMNIST and MNIST expose targets as a tensor\n",
    "    targets = dataset.targets\n",
    "    rng = np.random.RandomState(seed)\n",
    "    \n",
    "    sampled_indices = []\n",
    "    for c in range(10):\n",
    "        # Find all indices for class c\n",
    "        idx_c = (targets == c).nonzero(as_tuple=False).view(-1).cpu().numpy()\n",
    "        k = min(n_samples_per_class, len(idx_c))\n",
    "        # Sample without replacement\n",
    "        selected = rng.choice(idx_c, size=k, replace=False).tolist()\n",
    "        sampled_indices.extend(selected)\n",
    "    \n",
    "    # Shuffle the combined indices\n",
    "    rng.shuffle(sampled_indices)\n",
    "    return sampled_indices\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Main\n",
    "# ============================================\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"BLOCK 3: Quantum (NO Entanglement) Training\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nConfiguration:\")\n",
    "    print(\"  • Batch size: 32\")\n",
    "    print(\"  • Patience: 10\")\n",
    "    print(\"  • Max epochs: 200\")\n",
    "    print(\"  • Seeds: [42, 123, 456]\")\n",
    "    print(\"  • Architecture: [frozen 784→16] → 4 qubits (3 layers, NO ent) → 10\")\n",
    "    print(f\"  • Quantum device: {QUANTUM_DEVICE}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Load frozen preprocessor from Block 1\n",
    "    print(\"\\nLoading frozen preprocessor from Block 1...\")\n",
    "    try:\n",
    "        realnet_data = torch.load(\"realnet_results.pt\", weights_only=False)\n",
    "        preprocessor_state = realnet_data[\"preprocessor_state\"]\n",
    "        print(\"✓ Loaded preprocessor state from realnet_results.pt\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"✗ ERROR: realnet_results.pt not found!\")\n",
    "        print(\"  Run Block 1 first to train RealNet and save preprocessor.\")\n",
    "        return\n",
    "\n",
    "    # Create frozen preprocessor (CPU for quantum)\n",
    "    shared_preprocessor = SharedPreprocessor(784, 16)\n",
    "    shared_preprocessor.load_state_dict(preprocessor_state)\n",
    "    for p in shared_preprocessor.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    preprocessor_params = sum(p.numel() for p in shared_preprocessor.parameters())\n",
    "    print(f\"  Preprocessor frozen with {preprocessor_params:,} params\")\n",
    "\n",
    "    # Load data\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.2860,), (0.3530,))\n",
    "    ])\n",
    "\n",
    "    full_train_ds = datasets.FashionMNIST(root=\"./data\", train=True,\n",
    "                                         download=True, transform=transform)\n",
    "    full_test_ds = datasets.FashionMNIST(root=\"./data\", train=False,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "    # Create stratified samples (fast - uses targets only, no image loading)\n",
    "    print(\"\\nCreating stratified samples...\")\n",
    "    t0 = time.time()\n",
    "    train_indices = stratified_sample_from_targets(full_train_ds, n_samples_per_class=1500, seed=42)\n",
    "    test_indices = stratified_sample_from_targets(full_test_ds, n_samples_per_class=300, seed=42)\n",
    "    print(f\"  Sampling took {time.time()-t0:.2f}s\")\n",
    "    \n",
    "    train_ds = Subset(full_train_ds, train_indices)\n",
    "    test_ds = Subset(full_test_ds, test_indices)\n",
    "    \n",
    "    print(f\"  Train samples: {len(train_ds)} (stratified, 1500 per class)\")\n",
    "    print(f\"  Test samples:  {len(test_ds)} (stratified, 300 per class)\")\n",
    "\n",
    "    # Smaller batches for quantum\n",
    "    train_loader = DataLoader(train_ds, batch_size=32,\n",
    "                              shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_ds, batch_size=64,\n",
    "                             shuffle=False, num_workers=0)\n",
    "\n",
    "    seeds = [42, 123, 456]\n",
    "    all_results = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(f\"SEED {seed}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        set_all_seeds(seed)\n",
    "\n",
    "        print(f\"\\n  Training QuantumNet (NO entanglement, seed={seed})...\")\n",
    "        quantum_model = QuantumNet()\n",
    "        quantum_model.preprocessor = shared_preprocessor  # Use frozen preprocessor\n",
    "        \n",
    "        quantum_opt = torch.optim.Adam(quantum_model.head.parameters(), lr=1e-3)\n",
    "        \n",
    "        result = train_with_early_stopping(\n",
    "            quantum_model, train_loader, test_loader, quantum_opt,\n",
    "            max_epochs=200, patience=10, name=\"QuantumNoEnt\"\n",
    "        )\n",
    "        \n",
    "        result[\"trainable_params\"] = sum(p.numel() for p in quantum_model.parameters()\n",
    "                                         if p.requires_grad)\n",
    "        result[\"total_params\"] = sum(p.numel() for p in quantum_model.parameters())\n",
    "        result[\"seed\"] = seed\n",
    "        \n",
    "        all_results.append(result)\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"QUANTUM (NO ENTANGLEMENT) SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    accs = [r[\"best_acc\"] for r in all_results]\n",
    "    times = [r[\"time\"] for r in all_results]\n",
    "    epochs = [r[\"epochs\"] for r in all_results]\n",
    "    trainable = all_results[0][\"trainable_params\"]\n",
    "    total = all_results[0][\"total_params\"]\n",
    "    \n",
    "    print(f\"\\nAccuracy:   {np.mean(accs):.4f} ± {np.std(accs):.4f}\")\n",
    "    print(f\"Time:       {np.mean(times):.1f}s ± {np.std(times):.1f}s\")\n",
    "    print(f\"Epochs:     {np.mean(epochs):.1f} ± {np.std(epochs):.1f}\")\n",
    "    print(f\"Parameters: {trainable:,} trainable (head), {total:,} total\")\n",
    "    \n",
    "    print(\"\\nPer-seed results:\")\n",
    "    for r in all_results:\n",
    "        print(f\"  Seed {r['seed']}: acc={r['best_acc']:.4f}, \"\n",
    "              f\"time={r['time']:.1f}s, epochs={r['epochs']}\")\n",
    "\n",
    "    # Save results\n",
    "    save_dict = {\n",
    "        \"results\": all_results,\n",
    "        \"summary\": {\n",
    "            \"mean_acc\": np.mean(accs),\n",
    "            \"std_acc\": np.std(accs),\n",
    "            \"mean_time\": np.mean(times),\n",
    "            \"trainable_params\": trainable,\n",
    "            \"total_params\": total\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    torch.save(save_dict, \"quantum_noent_results.pt\")\n",
    "    print(f\"\\n✓ Saved results to: quantum_noent_results.pt\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88f3912-4a8e-4846-b2c1-1d2784a3cde8",
   "metadata": {},
   "source": [
    "# Quant Ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2411fc78-3424-4175-bdd1-79a343ff7f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using lightning.gpu device\n",
      "Using PyTorch device: cuda\n",
      "======================================================================\n",
      "BLOCK 4: Quantum (WITH Entanglement) Training\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "  • Batch size: 32\n",
      "  • Patience: 10\n",
      "  • Max epochs: 200\n",
      "  • Seeds: [42, 123, 456]\n",
      "  • Architecture: [frozen 784→16] → 4 qubits (3 layers, WITH ent) → 10\n",
      "  • Quantum device: lightning.gpu\n",
      "======================================================================\n",
      "\n",
      "Loading frozen preprocessor from Block 1...\n",
      "✓ Loaded preprocessor state from realnet_results.pt\n",
      "  Preprocessor frozen with 12,560 params\n",
      "\n",
      "Creating stratified samples...\n",
      "  Sampling took 0.00s\n",
      "  Train samples: 15000 (stratified, 1500 per class)\n",
      "  Test samples:  3000 (stratified, 300 per class)\n",
      "\n",
      "======================================================================\n",
      "SEED 42\n",
      "======================================================================\n",
      "\n",
      "  Training QuantumNet (WITH entanglement, seed=42)...\n",
      "  [QuantumEnt] Epoch 1/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  1 | loss=2.0657 | test_acc=0.5053 | time=509.1s\n",
      "  [QuantumEnt] Epoch 2/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  2 | loss=1.5693 | test_acc=0.6583 | time=1028.1s\n",
      "  [QuantumEnt] Epoch 3/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  3 | loss=1.2250 | test_acc=0.7433 | time=1535.6s\n",
      "  [QuantumEnt] Epoch 4/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  4 | loss=1.0254 | test_acc=0.7173 | time=2046.4s\n",
      "  [QuantumEnt] Epoch 5/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  5 | loss=0.9001 | test_acc=0.7783 | time=2554.8s\n",
      "  [QuantumEnt] Epoch 6/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  6 | loss=0.8065 | test_acc=0.7970 | time=3062.1s\n",
      "  [QuantumEnt] Epoch 7/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  7 | loss=0.7320 | test_acc=0.8077 | time=3572.5s\n",
      "  [QuantumEnt] Epoch 8/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  8 | loss=0.6714 | test_acc=0.8113 | time=4085.0s\n",
      "  [QuantumEnt] Epoch 9/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  9 | loss=0.6226 | test_acc=0.8153 | time=4580.4s\n",
      "  [QuantumEnt] Epoch 10/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 10 | loss=0.5830 | test_acc=0.8190 | time=5082.5s\n",
      "  [QuantumEnt] Epoch 11/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 11 | loss=0.5516 | test_acc=0.8180 | time=5608.5s\n",
      "  [QuantumEnt] Epoch 12/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 12 | loss=0.5264 | test_acc=0.8207 | time=6131.7s\n",
      "  [QuantumEnt] Epoch 13/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 13 | loss=0.5067 | test_acc=0.8210 | time=6655.3s\n",
      "  [QuantumEnt] Epoch 14/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 14 | loss=0.4908 | test_acc=0.8177 | time=7175.4s\n",
      "  [QuantumEnt] Epoch 15/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 15 | loss=0.4782 | test_acc=0.8237 | time=7695.7s\n",
      "  [QuantumEnt] Epoch 16/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 16 | loss=0.4672 | test_acc=0.8180 | time=8217.9s\n",
      "  [QuantumEnt] Epoch 17/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 17 | loss=0.4585 | test_acc=0.8190 | time=8733.5s\n",
      "  [QuantumEnt] Epoch 18/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 18 | loss=0.4508 | test_acc=0.8207 | time=9251.3s\n",
      "  [QuantumEnt] Epoch 19/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 19 | loss=0.4439 | test_acc=0.8253 | time=9762.9s\n",
      "  [QuantumEnt] Epoch 20/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 20 | loss=0.4385 | test_acc=0.8227 | time=10280.6s\n",
      "  [QuantumEnt] Epoch 21/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 21 | loss=0.4336 | test_acc=0.8243 | time=10794.0s\n",
      "  [QuantumEnt] Epoch 22/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 22 | loss=0.4294 | test_acc=0.8210 | time=11314.1s\n",
      "  [QuantumEnt] Epoch 23/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 23 | loss=0.4263 | test_acc=0.8203 | time=11831.8s\n",
      "  [QuantumEnt] Epoch 24/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 24 | loss=0.4219 | test_acc=0.8207 | time=12344.0s\n",
      "  [QuantumEnt] Epoch 25/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 25 | loss=0.4190 | test_acc=0.8223 | time=12867.8s\n",
      "  [QuantumEnt] Epoch 26/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 26 | loss=0.4164 | test_acc=0.8193 | time=13412.7s\n",
      "  [QuantumEnt] Epoch 27/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 27 | loss=0.4130 | test_acc=0.8210 | time=13965.0s\n",
      "  [QuantumEnt] Epoch 28/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 28 | loss=0.4107 | test_acc=0.8220 | time=14501.9s\n",
      "  [QuantumEnt] Epoch 29/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 29 | loss=0.4079 | test_acc=0.8217 | time=15044.0s\n",
      "  [QuantumEnt] Early stop at epoch 29 (no improvement for 10 epochs)\n",
      "\n",
      "======================================================================\n",
      "SEED 123\n",
      "======================================================================\n",
      "\n",
      "  Training QuantumNet (WITH entanglement, seed=123)...\n",
      "  [QuantumEnt] Epoch 1/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  1 | loss=2.0182 | test_acc=0.4850 | time=524.8s\n",
      "  [QuantumEnt] Epoch 2/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  2 | loss=1.5977 | test_acc=0.5593 | time=1061.3s\n",
      "  [QuantumEnt] Epoch 3/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  3 | loss=1.3484 | test_acc=0.6243 | time=1593.0s\n",
      "  [QuantumEnt] Epoch 4/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  4 | loss=1.1866 | test_acc=0.6480 | time=2112.6s\n",
      "  [QuantumEnt] Epoch 5/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  5 | loss=1.0638 | test_acc=0.6953 | time=2622.1s\n",
      "  [QuantumEnt] Epoch 6/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  6 | loss=0.9588 | test_acc=0.7190 | time=3147.1s\n",
      "  [QuantumEnt] Epoch 7/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  7 | loss=0.8691 | test_acc=0.7407 | time=3658.3s\n",
      "  [QuantumEnt] Epoch 8/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  8 | loss=0.7992 | test_acc=0.7537 | time=4167.5s\n",
      "  [QuantumEnt] Epoch 9/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  9 | loss=0.7439 | test_acc=0.7673 | time=4681.7s\n",
      "  [QuantumEnt] Epoch 10/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 10 | loss=0.6997 | test_acc=0.7693 | time=5188.9s\n",
      "  [QuantumEnt] Epoch 11/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 11 | loss=0.6647 | test_acc=0.7713 | time=5712.2s\n",
      "  [QuantumEnt] Epoch 12/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 12 | loss=0.6355 | test_acc=0.7700 | time=6233.9s\n",
      "  [QuantumEnt] Epoch 13/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 13 | loss=0.6121 | test_acc=0.7747 | time=6751.5s\n",
      "  [QuantumEnt] Epoch 14/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 14 | loss=0.5927 | test_acc=0.7743 | time=7268.6s\n",
      "  [QuantumEnt] Epoch 15/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 15 | loss=0.5762 | test_acc=0.7787 | time=7772.1s\n",
      "  [QuantumEnt] Epoch 16/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 16 | loss=0.5620 | test_acc=0.7797 | time=8296.3s\n",
      "  [QuantumEnt] Epoch 17/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 17 | loss=0.5502 | test_acc=0.7777 | time=8812.7s\n",
      "  [QuantumEnt] Epoch 18/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 18 | loss=0.5396 | test_acc=0.7820 | time=9346.5s\n",
      "  [QuantumEnt] Epoch 19/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 19 | loss=0.5304 | test_acc=0.7787 | time=9874.8s\n",
      "  [QuantumEnt] Epoch 20/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 20 | loss=0.5224 | test_acc=0.7837 | time=10398.8s\n",
      "  [QuantumEnt] Epoch 21/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 21 | loss=0.5144 | test_acc=0.7863 | time=10928.6s\n",
      "  [QuantumEnt] Epoch 22/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 22 | loss=0.5073 | test_acc=0.7850 | time=11454.3s\n",
      "  [QuantumEnt] Epoch 23/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 23 | loss=0.5012 | test_acc=0.7877 | time=11960.9s\n",
      "  [QuantumEnt] Epoch 24/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 24 | loss=0.4950 | test_acc=0.7873 | time=12468.4s\n",
      "  [QuantumEnt] Epoch 25/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 25 | loss=0.4899 | test_acc=0.7910 | time=12998.1s\n",
      "  [QuantumEnt] Epoch 26/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 26 | loss=0.4846 | test_acc=0.7887 | time=13514.7s\n",
      "  [QuantumEnt] Epoch 27/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 27 | loss=0.4805 | test_acc=0.7917 | time=14030.1s\n",
      "  [QuantumEnt] Epoch 28/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 28 | loss=0.4747 | test_acc=0.7927 | time=14538.9s\n",
      "  [QuantumEnt] Epoch 29/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 29 | loss=0.4714 | test_acc=0.7920 | time=15048.5s\n",
      "  [QuantumEnt] Epoch 30/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 30 | loss=0.4665 | test_acc=0.7917 | time=15590.6s\n",
      "  [QuantumEnt] Epoch 31/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 31 | loss=0.4625 | test_acc=0.7927 | time=16109.7s\n",
      "  [QuantumEnt] Epoch 32/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 32 | loss=0.4591 | test_acc=0.7927 | time=16623.9s\n",
      "  [QuantumEnt] Epoch 33/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 33 | loss=0.4549 | test_acc=0.7997 | time=17173.0s\n",
      "  [QuantumEnt] Epoch 34/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 34 | loss=0.4523 | test_acc=0.7993 | time=17730.8s\n",
      "  [QuantumEnt] Epoch 35/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 35 | loss=0.4478 | test_acc=0.7997 | time=18269.7s\n",
      "  [QuantumEnt] Epoch 36/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 36 | loss=0.4447 | test_acc=0.7993 | time=18788.1s\n",
      "  [QuantumEnt] Epoch 37/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 37 | loss=0.4419 | test_acc=0.8017 | time=19301.7s\n",
      "  [QuantumEnt] Epoch 38/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 38 | loss=0.4384 | test_acc=0.8013 | time=19833.0s\n",
      "  [QuantumEnt] Epoch 39/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 39 | loss=0.4358 | test_acc=0.7997 | time=20408.5s\n",
      "  [QuantumEnt] Epoch 40/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 40 | loss=0.4331 | test_acc=0.8013 | time=20977.2s\n",
      "  [QuantumEnt] Epoch 41/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 41 | loss=0.4304 | test_acc=0.8053 | time=21533.5s\n",
      "  [QuantumEnt] Epoch 42/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 42 | loss=0.4277 | test_acc=0.8043 | time=22053.6s\n",
      "  [QuantumEnt] Epoch 43/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 43 | loss=0.4253 | test_acc=0.8030 | time=22590.0s\n",
      "  [QuantumEnt] Epoch 44/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 44 | loss=0.4229 | test_acc=0.8073 | time=23126.4s\n",
      "  [QuantumEnt] Epoch 45/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 45 | loss=0.4210 | test_acc=0.8060 | time=23660.8s\n",
      "  [QuantumEnt] Epoch 46/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 46 | loss=0.4192 | test_acc=0.8083 | time=24204.2s\n",
      "  [QuantumEnt] Epoch 47/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 47 | loss=0.4163 | test_acc=0.8057 | time=24741.5s\n",
      "  [QuantumEnt] Epoch 48/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 48 | loss=0.4143 | test_acc=0.8100 | time=25276.0s\n",
      "  [QuantumEnt] Epoch 49/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 49 | loss=0.4131 | test_acc=0.8117 | time=25786.9s\n",
      "  [QuantumEnt] Epoch 50/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 50 | loss=0.4110 | test_acc=0.8090 | time=26315.1s\n",
      "  [QuantumEnt] Epoch 51/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 51 | loss=0.4097 | test_acc=0.8080 | time=26828.1s\n",
      "  [QuantumEnt] Epoch 52/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 52 | loss=0.4085 | test_acc=0.8093 | time=27345.7s\n",
      "  [QuantumEnt] Epoch 53/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 53 | loss=0.4063 | test_acc=0.8093 | time=27856.1s\n",
      "  [QuantumEnt] Epoch 54/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 54 | loss=0.4059 | test_acc=0.8087 | time=28377.4s\n",
      "  [QuantumEnt] Epoch 55/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 55 | loss=0.4044 | test_acc=0.8110 | time=28900.4s\n",
      "  [QuantumEnt] Epoch 56/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 56 | loss=0.4030 | test_acc=0.8127 | time=29414.6s\n",
      "  [QuantumEnt] Epoch 57/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 57 | loss=0.4023 | test_acc=0.8100 | time=29932.6s\n",
      "  [QuantumEnt] Epoch 58/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 58 | loss=0.4005 | test_acc=0.8123 | time=30441.2s\n",
      "  [QuantumEnt] Epoch 59/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 59 | loss=0.3993 | test_acc=0.8130 | time=30966.1s\n",
      "  [QuantumEnt] Epoch 60/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 60 | loss=0.3984 | test_acc=0.8107 | time=31471.1s\n",
      "  [QuantumEnt] Epoch 61/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 61 | loss=0.3978 | test_acc=0.8120 | time=31991.0s\n",
      "  [QuantumEnt] Epoch 62/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 62 | loss=0.3968 | test_acc=0.8120 | time=32502.4s\n",
      "  [QuantumEnt] Epoch 63/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 63 | loss=0.3964 | test_acc=0.8107 | time=33007.3s\n",
      "  [QuantumEnt] Epoch 64/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 64 | loss=0.3953 | test_acc=0.8113 | time=33523.1s\n",
      "  [QuantumEnt] Epoch 65/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 65 | loss=0.3944 | test_acc=0.8147 | time=34036.5s\n",
      "  [QuantumEnt] Epoch 66/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 66 | loss=0.3943 | test_acc=0.8147 | time=34558.7s\n",
      "  [QuantumEnt] Epoch 67/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 67 | loss=0.3936 | test_acc=0.8100 | time=35066.6s\n",
      "  [QuantumEnt] Epoch 68/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 68 | loss=0.3921 | test_acc=0.8110 | time=35594.7s\n",
      "  [QuantumEnt] Epoch 69/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 69 | loss=0.3918 | test_acc=0.8140 | time=36111.6s\n",
      "  [QuantumEnt] Epoch 70/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 70 | loss=0.3906 | test_acc=0.8103 | time=36640.7s\n",
      "  [QuantumEnt] Epoch 71/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 71 | loss=0.3904 | test_acc=0.8117 | time=37152.6s\n",
      "  [QuantumEnt] Epoch 72/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 72 | loss=0.3889 | test_acc=0.8117 | time=37661.4s\n",
      "  [QuantumEnt] Epoch 73/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 73 | loss=0.3895 | test_acc=0.8130 | time=38176.0s\n",
      "  [QuantumEnt] Epoch 74/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 74 | loss=0.3884 | test_acc=0.8123 | time=38690.6s\n",
      "  [QuantumEnt] Epoch 75/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 75 | loss=0.3881 | test_acc=0.8153 | time=39212.6s\n",
      "  [QuantumEnt] Epoch 76/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 76 | loss=0.3879 | test_acc=0.8137 | time=39726.7s\n",
      "  [QuantumEnt] Epoch 77/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 77 | loss=0.3874 | test_acc=0.8097 | time=40249.0s\n",
      "  [QuantumEnt] Epoch 78/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 78 | loss=0.3861 | test_acc=0.8130 | time=40772.9s\n",
      "  [QuantumEnt] Epoch 79/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 79 | loss=0.3858 | test_acc=0.8123 | time=41278.3s\n",
      "  [QuantumEnt] Epoch 80/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 80 | loss=0.3862 | test_acc=0.8170 | time=41794.8s\n",
      "  [QuantumEnt] Epoch 81/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 81 | loss=0.3855 | test_acc=0.8120 | time=42303.7s\n",
      "  [QuantumEnt] Epoch 82/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 82 | loss=0.3839 | test_acc=0.8147 | time=42833.8s\n",
      "  [QuantumEnt] Epoch 83/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 83 | loss=0.3843 | test_acc=0.8133 | time=43352.6s\n",
      "  [QuantumEnt] Epoch 84/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 84 | loss=0.3839 | test_acc=0.8177 | time=43877.0s\n",
      "  [QuantumEnt] Epoch 85/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 85 | loss=0.3839 | test_acc=0.8130 | time=44412.9s\n",
      "  [QuantumEnt] Epoch 86/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 86 | loss=0.3834 | test_acc=0.8203 | time=44950.4s\n",
      "  [QuantumEnt] Epoch 87/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 87 | loss=0.3824 | test_acc=0.8177 | time=45485.6s\n",
      "  [QuantumEnt] Epoch 88/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 88 | loss=0.3830 | test_acc=0.8130 | time=46023.7s\n",
      "  [QuantumEnt] Epoch 89/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 89 | loss=0.3827 | test_acc=0.8147 | time=46555.4s\n",
      "  [QuantumEnt] Epoch 90/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 90 | loss=0.3814 | test_acc=0.8157 | time=47067.2s\n",
      "  [QuantumEnt] Epoch 91/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 91 | loss=0.3817 | test_acc=0.8140 | time=47582.2s\n",
      "  [QuantumEnt] Epoch 92/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 92 | loss=0.3820 | test_acc=0.8150 | time=48090.8s\n",
      "  [QuantumEnt] Epoch 93/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 93 | loss=0.3817 | test_acc=0.8143 | time=48609.7s\n",
      "  [QuantumEnt] Epoch 94/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 94 | loss=0.3811 | test_acc=0.8160 | time=49123.1s\n",
      "  [QuantumEnt] Epoch 95/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 95 | loss=0.3811 | test_acc=0.8133 | time=49640.5s\n",
      "  [QuantumEnt] Epoch 96/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 96 | loss=0.3798 | test_acc=0.8170 | time=50153.9s\n",
      "  [QuantumEnt] Early stop at epoch 96 (no improvement for 10 epochs)\n",
      "\n",
      "======================================================================\n",
      "SEED 456\n",
      "======================================================================\n",
      "\n",
      "  Training QuantumNet (WITH entanglement, seed=456)...\n",
      "  [QuantumEnt] Epoch 1/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  1 | loss=2.0313 | test_acc=0.4410 | time=521.5s\n",
      "  [QuantumEnt] Epoch 2/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  2 | loss=1.5290 | test_acc=0.6360 | time=1042.1s\n",
      "  [QuantumEnt] Epoch 3/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  3 | loss=1.2444 | test_acc=0.6903 | time=1544.0s\n",
      "  [QuantumEnt] Epoch 4/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  4 | loss=1.0755 | test_acc=0.7357 | time=2086.0s\n",
      "  [QuantumEnt] Epoch 5/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  5 | loss=0.9571 | test_acc=0.7783 | time=2599.9s\n",
      "  [QuantumEnt] Epoch 6/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  6 | loss=0.8636 | test_acc=0.7807 | time=3115.7s\n",
      "  [QuantumEnt] Epoch 7/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  7 | loss=0.7879 | test_acc=0.7910 | time=3622.6s\n",
      "  [QuantumEnt] Epoch 8/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  8 | loss=0.7281 | test_acc=0.7947 | time=4127.6s\n",
      "  [QuantumEnt] Epoch 9/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch  9 | loss=0.6803 | test_acc=0.7990 | time=4643.5s\n",
      "  [QuantumEnt] Epoch 10/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 10 | loss=0.6418 | test_acc=0.8090 | time=5150.9s\n",
      "  [QuantumEnt] Epoch 11/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 11 | loss=0.6101 | test_acc=0.8100 | time=5678.4s\n",
      "  [QuantumEnt] Epoch 12/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 12 | loss=0.5837 | test_acc=0.8117 | time=6181.7s\n",
      "  [QuantumEnt] Epoch 13/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 13 | loss=0.5617 | test_acc=0.8103 | time=6697.8s\n",
      "  [QuantumEnt] Epoch 14/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 14 | loss=0.5420 | test_acc=0.8140 | time=7207.6s\n",
      "  [QuantumEnt] Epoch 15/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 15 | loss=0.5252 | test_acc=0.8147 | time=7713.5s\n",
      "  [QuantumEnt] Epoch 16/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 16 | loss=0.5101 | test_acc=0.8157 | time=8223.5s\n",
      "  [QuantumEnt] Epoch 17/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 17 | loss=0.4969 | test_acc=0.8143 | time=8730.8s\n",
      "  [QuantumEnt] Epoch 18/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 18 | loss=0.4859 | test_acc=0.8157 | time=9241.9s\n",
      "  [QuantumEnt] Epoch 19/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 19 | loss=0.4752 | test_acc=0.8170 | time=9744.6s\n",
      "  [QuantumEnt] Epoch 20/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 20 | loss=0.4664 | test_acc=0.8213 | time=10247.6s\n",
      "  [QuantumEnt] Epoch 21/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 21 | loss=0.4580 | test_acc=0.8190 | time=10765.7s\n",
      "  [QuantumEnt] Epoch 22/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 22 | loss=0.4516 | test_acc=0.8213 | time=11270.7s\n",
      "  [QuantumEnt] Epoch 23/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 23 | loss=0.4455 | test_acc=0.8203 | time=11791.8s\n",
      "  [QuantumEnt] Epoch 24/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 24 | loss=0.4400 | test_acc=0.8220 | time=12297.8s\n",
      "  [QuantumEnt] Epoch 25/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 25 | loss=0.4351 | test_acc=0.8217 | time=12813.0s\n",
      "  [QuantumEnt] Epoch 26/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 26 | loss=0.4305 | test_acc=0.8140 | time=13320.7s\n",
      "  [QuantumEnt] Epoch 27/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 27 | loss=0.4267 | test_acc=0.8223 | time=13826.8s\n",
      "  [QuantumEnt] Epoch 28/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 28 | loss=0.4221 | test_acc=0.8263 | time=14337.6s\n",
      "  [QuantumEnt] Epoch 29/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 29 | loss=0.4201 | test_acc=0.8197 | time=14847.4s\n",
      "  [QuantumEnt] Epoch 30/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 30 | loss=0.4164 | test_acc=0.8240 | time=15380.6s\n",
      "  [QuantumEnt] Epoch 31/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 31 | loss=0.4148 | test_acc=0.8190 | time=15880.2s\n",
      "  [QuantumEnt] Epoch 32/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 32 | loss=0.4111 | test_acc=0.8240 | time=16413.3s\n",
      "  [QuantumEnt] Epoch 33/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 33 | loss=0.4086 | test_acc=0.8250 | time=16922.8s\n",
      "  [QuantumEnt] Epoch 34/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 34 | loss=0.4057 | test_acc=0.8230 | time=17430.2s\n",
      "  [QuantumEnt] Epoch 35/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 35 | loss=0.4046 | test_acc=0.8243 | time=17945.7s\n",
      "  [QuantumEnt] Epoch 36/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 36 | loss=0.4023 | test_acc=0.8267 | time=18455.9s\n",
      "  [QuantumEnt] Epoch 37/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 37 | loss=0.4007 | test_acc=0.8267 | time=18980.6s\n",
      "  [QuantumEnt] Epoch 38/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 38 | loss=0.3988 | test_acc=0.8247 | time=19490.7s\n",
      "  [QuantumEnt] Epoch 39/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 39 | loss=0.3979 | test_acc=0.8253 | time=20013.6s\n",
      "  [QuantumEnt] Epoch 40/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 40 | loss=0.3958 | test_acc=0.8233 | time=20521.3s\n",
      "  [QuantumEnt] Epoch 41/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 41 | loss=0.3937 | test_acc=0.8263 | time=21039.5s\n",
      "  [QuantumEnt] Epoch 42/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 42 | loss=0.3925 | test_acc=0.8247 | time=21555.5s\n",
      "  [QuantumEnt] Epoch 43/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 43 | loss=0.3912 | test_acc=0.8267 | time=22055.8s\n",
      "  [QuantumEnt] Epoch 44/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 44 | loss=0.3897 | test_acc=0.8263 | time=22573.3s\n",
      "  [QuantumEnt] Epoch 45/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 45 | loss=0.3887 | test_acc=0.8283 | time=23077.7s\n",
      "  [QuantumEnt] Epoch 46/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 46 | loss=0.3877 | test_acc=0.8283 | time=23597.3s\n",
      "  [QuantumEnt] Epoch 47/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 47 | loss=0.3865 | test_acc=0.8277 | time=24111.7s\n",
      "  [QuantumEnt] Epoch 48/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 48 | loss=0.3845 | test_acc=0.8250 | time=24632.5s\n",
      "  [QuantumEnt] Epoch 49/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 49 | loss=0.3842 | test_acc=0.8237 | time=25157.4s\n",
      "  [QuantumEnt] Epoch 50/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 50 | loss=0.3825 | test_acc=0.8280 | time=25687.8s\n",
      "  [QuantumEnt] Epoch 51/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 51 | loss=0.3829 | test_acc=0.8280 | time=26198.0s\n",
      "  [QuantumEnt] Epoch 52/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 52 | loss=0.3813 | test_acc=0.8273 | time=26710.6s\n",
      "  [QuantumEnt] Epoch 53/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 53 | loss=0.3795 | test_acc=0.8280 | time=27232.3s\n",
      "  [QuantumEnt] Epoch 54/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 54 | loss=0.3803 | test_acc=0.8310 | time=27753.4s\n",
      "  [QuantumEnt] Epoch 55/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 55 | loss=0.3789 | test_acc=0.8287 | time=28293.2s\n",
      "  [QuantumEnt] Epoch 56/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 56 | loss=0.3773 | test_acc=0.8290 | time=28830.4s\n",
      "  [QuantumEnt] Epoch 57/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 57 | loss=0.3760 | test_acc=0.8287 | time=29370.5s\n",
      "  [QuantumEnt] Epoch 58/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 58 | loss=0.3760 | test_acc=0.8303 | time=29894.2s\n",
      "  [QuantumEnt] Epoch 59/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 59 | loss=0.3751 | test_acc=0.8310 | time=30407.9s\n",
      "  [QuantumEnt] Epoch 60/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 60 | loss=0.3733 | test_acc=0.8290 | time=30924.0s\n",
      "  [QuantumEnt] Epoch 61/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 61 | loss=0.3743 | test_acc=0.8307 | time=31434.7s\n",
      "  [QuantumEnt] Epoch 62/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 62 | loss=0.3727 | test_acc=0.8293 | time=31949.0s\n",
      "  [QuantumEnt] Epoch 63/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 63 | loss=0.3720 | test_acc=0.8307 | time=32464.5s\n",
      "  [QuantumEnt] Epoch 64/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 64 | loss=0.3708 | test_acc=0.8330 | time=32983.3s\n",
      "  [QuantumEnt] Epoch 65/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 65 | loss=0.3704 | test_acc=0.8320 | time=33503.4s\n",
      "  [QuantumEnt] Epoch 66/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 66 | loss=0.3699 | test_acc=0.8277 | time=34016.8s\n",
      "  [QuantumEnt] Epoch 67/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 67 | loss=0.3687 | test_acc=0.8300 | time=34528.5s\n",
      "  [QuantumEnt] Epoch 68/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 68 | loss=0.3685 | test_acc=0.8317 | time=35041.1s\n",
      "  [QuantumEnt] Epoch 69/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 69 | loss=0.3676 | test_acc=0.8310 | time=35556.4s\n",
      "  [QuantumEnt] Epoch 70/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 70 | loss=0.3663 | test_acc=0.8280 | time=36073.7s\n",
      "  [QuantumEnt] Epoch 71/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 71 | loss=0.3665 | test_acc=0.8303 | time=36593.2s\n",
      "  [QuantumEnt] Epoch 72/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 72 | loss=0.3663 | test_acc=0.8293 | time=37105.3s\n",
      "  [QuantumEnt] Epoch 73/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 73 | loss=0.3651 | test_acc=0.8263 | time=37624.5s\n",
      "  [QuantumEnt] Epoch 74/200\n",
      "    Batch 460/469, samples: 14752\n",
      "  [QuantumEnt] Epoch 74 | loss=0.3640 | test_acc=0.8323 | time=38141.5s\n",
      "  [QuantumEnt] Early stop at epoch 74 (no improvement for 10 epochs)\n",
      "\n",
      "======================================================================\n",
      "QUANTUM (WITH ENTANGLEMENT) SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Accuracy:   0.8262 ± 0.0052\n",
      "Time:       34446.4s ± 14569.8s\n",
      "Epochs:     66.3 ± 27.9\n",
      "Parameters: 162 trainable (head), 12,722 total\n",
      "\n",
      "Per-seed results:\n",
      "  Seed 42: acc=0.8253, time=15044.0s, epochs=29\n",
      "  Seed 123: acc=0.8203, time=50153.9s, epochs=96\n",
      "  Seed 456: acc=0.8330, time=38141.5s, epochs=74\n",
      "\n",
      "✓ Saved results to: quantum_ent_results.pt\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Block 4: Quantum (WITH Entanglement) Training\n",
    "==============================================\n",
    "Loads frozen preprocessor from Block 1 and trains quantum head WITH entanglement.\n",
    "Uses Lightning-GPU acceleration.\n",
    "\n",
    "Requirements:\n",
    "- realnet_results.pt (from Block 1)\n",
    "- pennylane, pennylane-lightning-gpu\n",
    "\n",
    "Outputs:\n",
    "- quantum_ent_results.pt: Contains quantum (with ent) results\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "try:\n",
    "    import pennylane as qml\n",
    "    QUANTUM_DEVICE = \"lightning.gpu\"\n",
    "    PENNYLANE_AVAILABLE = True\n",
    "    print(\"✓ Using lightning.gpu device\")\n",
    "except ImportError:\n",
    "    PENNYLANE_AVAILABLE = False\n",
    "    print(\"✗ PennyLane not installed\")\n",
    "    exit(1)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "print(f\"Using PyTorch device: {device}\")\n",
    "\n",
    "# ============================================\n",
    "# Comprehensive seed setting for reproducibility\n",
    "# ============================================\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    \"\"\"Set seeds for all RNG sources for reproducibility\"\"\"\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    # Python\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # NumPy\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # PyTorch deterministic mode\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # CuPy (if available)\n",
    "    try:\n",
    "        import cupy as cp\n",
    "        cp.random.seed(seed)\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    # PennyLane (if available)\n",
    "    try:\n",
    "        import pennylane as qml\n",
    "        qml.numpy.random.seed(seed)\n",
    "    except (ImportError, AttributeError):\n",
    "        pass\n",
    "\n",
    "# ============================================\n",
    "# Shared Preprocessor (must match Block 1)\n",
    "# ============================================\n",
    "\n",
    "class SharedPreprocessor(nn.Module):\n",
    "    \"\"\"Shared classical feature extractor: 784 → 16\"\"\"\n",
    "    def __init__(self, input_dim=784, bottleneck_dim=16):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, bottleneck_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.tanh(self.fc(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Quantum Head (WITH Entanglement)\n",
    "# ============================================\n",
    "\n",
    "class QuantumHead(nn.Module):\n",
    "    \"\"\"\n",
    "    VQC with 4 qubits, 3 layers, WITH entanglement → 10 classes.\n",
    "    Uses Lightning acceleration (GPU).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_qubits=4, n_layers=3, num_classes=10, device_name=None):\n",
    "        super().__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Map 16 features → n_qubits\n",
    "        self.feature_select = nn.Linear(16, n_qubits)\n",
    "\n",
    "        # Use specified device or default\n",
    "        if device_name is None:\n",
    "            device_name = QUANTUM_DEVICE\n",
    "        \n",
    "        # Quantum device\n",
    "        self.dev = qml.device(device_name, wires=n_qubits)\n",
    "\n",
    "        # Use adjoint differentiation for lightning (much faster)\n",
    "        diff_method = \"adjoint\" if \"lightning\" in device_name else \"parameter-shift\"\n",
    "\n",
    "        @qml.qnode(self.dev, interface=\"torch\", diff_method=diff_method)\n",
    "        def quantum_circuit(inputs, weights):\n",
    "            \"\"\"\n",
    "            Single-sample circuit WITH entanglement.\n",
    "            inputs: (n_qubits,)\n",
    "            weights: (n_layers, n_qubits, 2)\n",
    "            \"\"\"\n",
    "            for layer in range(n_layers):\n",
    "                # Data re-uploading\n",
    "                for i in range(n_qubits):\n",
    "                    qml.RY(inputs[i], wires=i)\n",
    "\n",
    "                # Trainable rotations\n",
    "                for i in range(n_qubits):\n",
    "                    qml.RY(weights[layer, i, 0], wires=i)\n",
    "                    qml.RZ(weights[layer, i, 1], wires=i)\n",
    "\n",
    "                # ENTANGLEMENT: CNOT ring\n",
    "                for i in range(n_qubits - 1):\n",
    "                    qml.CNOT(wires=[i, i + 1])\n",
    "                if n_qubits > 2:\n",
    "                    qml.CNOT(wires=[n_qubits - 1, 0])\n",
    "\n",
    "            return [\n",
    "                qml.expval(qml.PauliZ(0)),\n",
    "                qml.expval(qml.PauliZ(1)),\n",
    "                qml.expval(qml.PauliZ(2)),\n",
    "                qml.expval(qml.PauliZ(3)),\n",
    "                qml.expval(qml.PauliZ(0) @ qml.PauliZ(1)),\n",
    "                qml.expval(qml.PauliZ(2) @ qml.PauliZ(3))\n",
    "            ]\n",
    "\n",
    "        self.quantum_circuit = quantum_circuit\n",
    "\n",
    "        weight_shape = (n_layers, n_qubits, 2)\n",
    "        self.q_weights = nn.Parameter(torch.randn(weight_shape) * 0.1)\n",
    "        self.fc_out = nn.Linear(6, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Process samples with progress tracking.\n",
    "        x: (batch, 16) real bottleneck features\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        x = torch.tanh(self.feature_select(x))\n",
    "\n",
    "        # Process in chunks for memory management\n",
    "        chunk_size = 32\n",
    "        quantum_outputs = []\n",
    "\n",
    "        for start_idx in range(0, batch_size, chunk_size):\n",
    "            end_idx = min(start_idx + chunk_size, batch_size)\n",
    "            chunk = x[start_idx:end_idx]\n",
    "\n",
    "            chunk_outputs = []\n",
    "            for i in range(chunk.size(0)):\n",
    "                q_raw = self.quantum_circuit(chunk[i], self.q_weights)\n",
    "                if isinstance(q_raw, (list, tuple)):\n",
    "                    q_out = torch.stack(q_raw)\n",
    "                else:\n",
    "                    q_out = q_raw\n",
    "                chunk_outputs.append(q_out)\n",
    "\n",
    "            quantum_outputs.extend(chunk_outputs)\n",
    "\n",
    "        # Convert to tensor (cast to float32)\n",
    "        quantum_outputs = torch.stack(quantum_outputs).float()\n",
    "        quantum_outputs = quantum_outputs.to(self.fc_out.weight.dtype)\n",
    "\n",
    "        output = self.fc_out(quantum_outputs)\n",
    "        return output\n",
    "\n",
    "\n",
    "class QuantumNet(nn.Module):\n",
    "    \"\"\"Complete Quantum network: Preprocessor + QuantumHead (with ent)\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.preprocessor = SharedPreprocessor(784, 16)\n",
    "        self.head = QuantumHead(n_qubits=4, n_layers=3, num_classes=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.preprocessor(x)\n",
    "        return self.head(features)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Training and Evaluation\n",
    "# ============================================\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, show_progress=True):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(loader):\n",
    "        # Keep data on CPU for quantum models\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        total_samples += x.size(0)\n",
    "\n",
    "        if show_progress and batch_idx % 20 == 0:\n",
    "            print(f\"    Batch {batch_idx}/{len(loader)}, samples: {total_samples}\", end=\"\\r\")\n",
    "\n",
    "    if show_progress:\n",
    "        print()\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            # Keep data on CPU\n",
    "            logits = model(x)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += x.size(0)\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "def train_with_early_stopping(model, train_loader, test_loader, optimizer,\n",
    "                              max_epochs=200, patience=10, name=\"Model\"):\n",
    "    best_acc = 0.0\n",
    "    epochs_without_improvement = 0\n",
    "    start = time.time()\n",
    "    last_acc = 0.0\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        print(f\"  [{name}] Epoch {epoch}/{max_epochs}\")\n",
    "        loss = train_one_epoch(model, train_loader, optimizer, show_progress=True)\n",
    "        acc = evaluate(model, test_loader)\n",
    "        last_acc = acc\n",
    "\n",
    "        elapsed = time.time() - start\n",
    "        print(f\"  [{name}] Epoch {epoch:2d} | loss={loss:.4f} \"\n",
    "              f\"| test_acc={acc:.4f} | time={elapsed:.1f}s\")\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"  [{name}] Early stop at epoch {epoch} \"\n",
    "                  f\"(no improvement for {patience} epochs)\")\n",
    "            break\n",
    "\n",
    "    total_time = time.time() - start\n",
    "    return {\n",
    "        \"best_acc\": best_acc,\n",
    "        \"final_acc\": last_acc,\n",
    "        \"time\": total_time,\n",
    "        \"epochs\": epoch,\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Data utilities\n",
    "# ============================================\n",
    "\n",
    "def stratified_sample_from_targets(dataset, n_samples_per_class, seed=42):\n",
    "    \"\"\"\n",
    "    Create a stratified sample with n_samples_per_class from each class.\n",
    "    Uses dataset.targets directly - NO image loading during sampling.\n",
    "    \"\"\"\n",
    "    # FashionMNIST and MNIST expose targets as a tensor\n",
    "    targets = dataset.targets\n",
    "    rng = np.random.RandomState(seed)\n",
    "    \n",
    "    sampled_indices = []\n",
    "    for c in range(10):\n",
    "        # Find all indices for class c\n",
    "        idx_c = (targets == c).nonzero(as_tuple=False).view(-1).cpu().numpy()\n",
    "        k = min(n_samples_per_class, len(idx_c))\n",
    "        # Sample without replacement\n",
    "        selected = rng.choice(idx_c, size=k, replace=False).tolist()\n",
    "        sampled_indices.extend(selected)\n",
    "    \n",
    "    # Shuffle the combined indices\n",
    "    rng.shuffle(sampled_indices)\n",
    "    return sampled_indices\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Main\n",
    "# ============================================\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"BLOCK 4: Quantum (WITH Entanglement) Training\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nConfiguration:\")\n",
    "    print(\"  • Batch size: 32\")\n",
    "    print(\"  • Patience: 10\")\n",
    "    print(\"  • Max epochs: 200\")\n",
    "    print(\"  • Seeds: [42, 123, 456]\")\n",
    "    print(\"  • Architecture: [frozen 784→16] → 4 qubits (3 layers, WITH ent) → 10\")\n",
    "    print(f\"  • Quantum device: {QUANTUM_DEVICE}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Load frozen preprocessor from Block 1\n",
    "    print(\"\\nLoading frozen preprocessor from Block 1...\")\n",
    "    try:\n",
    "        realnet_data = torch.load(\"realnet_results.pt\", weights_only=False)\n",
    "        preprocessor_state = realnet_data[\"preprocessor_state\"]\n",
    "        print(\"✓ Loaded preprocessor state from realnet_results.pt\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"✗ ERROR: realnet_results.pt not found!\")\n",
    "        print(\"  Run Block 1 first to train RealNet and save preprocessor.\")\n",
    "        return\n",
    "\n",
    "    # Create frozen preprocessor (CPU for quantum)\n",
    "    shared_preprocessor = SharedPreprocessor(784, 16)\n",
    "    shared_preprocessor.load_state_dict(preprocessor_state)\n",
    "    for p in shared_preprocessor.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    preprocessor_params = sum(p.numel() for p in shared_preprocessor.parameters())\n",
    "    print(f\"  Preprocessor frozen with {preprocessor_params:,} params\")\n",
    "\n",
    "    # Load data\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.2860,), (0.3530,))\n",
    "    ])\n",
    "\n",
    "    full_train_ds = datasets.FashionMNIST(root=\"./data\", train=True,\n",
    "                                         download=True, transform=transform)\n",
    "    full_test_ds = datasets.FashionMNIST(root=\"./data\", train=False,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "    # Create stratified samples (fast - uses targets only, no image loading)\n",
    "    print(\"\\nCreating stratified samples...\")\n",
    "    t0 = time.time()\n",
    "    train_indices = stratified_sample_from_targets(full_train_ds, n_samples_per_class=1500, seed=42)\n",
    "    test_indices = stratified_sample_from_targets(full_test_ds, n_samples_per_class=300, seed=42)\n",
    "    print(f\"  Sampling took {time.time()-t0:.2f}s\")\n",
    "    \n",
    "    train_ds = Subset(full_train_ds, train_indices)\n",
    "    test_ds = Subset(full_test_ds, test_indices)\n",
    "    \n",
    "    print(f\"  Train samples: {len(train_ds)} (stratified, 1500 per class)\")\n",
    "    print(f\"  Test samples:  {len(test_ds)} (stratified, 300 per class)\")\n",
    "\n",
    "    # Smaller batches for quantum\n",
    "    train_loader = DataLoader(train_ds, batch_size=32,\n",
    "                              shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_ds, batch_size=64,\n",
    "                             shuffle=False, num_workers=0)\n",
    "\n",
    "    seeds = [42, 123, 456]\n",
    "    all_results = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(f\"SEED {seed}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        set_all_seeds(seed)\n",
    "\n",
    "        print(f\"\\n  Training QuantumNet (WITH entanglement, seed={seed})...\")\n",
    "        quantum_model = QuantumNet()\n",
    "        quantum_model.preprocessor = shared_preprocessor  # Use frozen preprocessor\n",
    "        \n",
    "        quantum_opt = torch.optim.Adam(quantum_model.head.parameters(), lr=1e-3)\n",
    "        \n",
    "        result = train_with_early_stopping(\n",
    "            quantum_model, train_loader, test_loader, quantum_opt,\n",
    "            max_epochs=200, patience=10, name=\"QuantumEnt\"\n",
    "        )\n",
    "        \n",
    "        result[\"trainable_params\"] = sum(p.numel() for p in quantum_model.parameters()\n",
    "                                         if p.requires_grad)\n",
    "        result[\"total_params\"] = sum(p.numel() for p in quantum_model.parameters())\n",
    "        result[\"seed\"] = seed\n",
    "        \n",
    "        all_results.append(result)\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"QUANTUM (WITH ENTANGLEMENT) SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    accs = [r[\"best_acc\"] for r in all_results]\n",
    "    times = [r[\"time\"] for r in all_results]\n",
    "    epochs = [r[\"epochs\"] for r in all_results]\n",
    "    trainable = all_results[0][\"trainable_params\"]\n",
    "    total = all_results[0][\"total_params\"]\n",
    "    \n",
    "    print(f\"\\nAccuracy:   {np.mean(accs):.4f} ± {np.std(accs):.4f}\")\n",
    "    print(f\"Time:       {np.mean(times):.1f}s ± {np.std(times):.1f}s\")\n",
    "    print(f\"Epochs:     {np.mean(epochs):.1f} ± {np.std(epochs):.1f}\")\n",
    "    print(f\"Parameters: {trainable:,} trainable (head), {total:,} total\")\n",
    "    \n",
    "    print(\"\\nPer-seed results:\")\n",
    "    for r in all_results:\n",
    "        print(f\"  Seed {r['seed']}: acc={r['best_acc']:.4f}, \"\n",
    "              f\"time={r['time']:.1f}s, epochs={r['epochs']}\")\n",
    "\n",
    "    # Save results\n",
    "    save_dict = {\n",
    "        \"results\": all_results,\n",
    "        \"summary\": {\n",
    "            \"mean_acc\": np.mean(accs),\n",
    "            \"std_acc\": np.std(accs),\n",
    "            \"mean_time\": np.mean(times),\n",
    "            \"trainable_params\": trainable,\n",
    "            \"total_params\": total\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    torch.save(save_dict, \"quantum_ent_results.pt\")\n",
    "    print(f\"\\n✓ Saved results to: quantum_ent_results.pt\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46aeb1c-b285-4f93-88e7-722dca4278b8",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7dcb002-3456-4f65-85c7-dcfc2ebb7c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "BLOCK 5: AGGREGATE RESULTS AND COMPARATIVE ANALYSIS\n",
      "==========================================================================================\n",
      "✓ Loaded RealNet results\n",
      "✓ Loaded QuatNet results\n",
      "✓ Loaded Quantum (no ent) results\n",
      "✓ Loaded Quantum (with ent) results\n",
      "\n",
      "==========================================================================================\n",
      "AGGREGATED RESULTS (mean ± std over 3 seeds)\n",
      "==========================================================================================\n",
      "Model           Accuracy             Time (s)             Epochs          Parameters          \n",
      "------------------------------------------------------------------------------------------\n",
      "Real            0.8460 ± 0.0012      32.0 ± 3.3           23.0 ± 2.2      14,298              \n",
      "Quat            0.8447 ± 0.0005      56.9 ± 6.3           24.0 ± 2.4      1,000 (head)        \n",
      "QNoEnt          0.8203 ± 0.0094      22284.7 ± 2414.2     47.3 ± 4.0      162 (head)          \n",
      "QEnt            0.8262 ± 0.0052      34446.4 ± 14569.8    66.3 ± 27.9     162 (head)          \n",
      "==========================================================================================\n",
      "\n",
      "======================================================================\n",
      "PER-SEED RESULTS\n",
      "======================================================================\n",
      "\n",
      "Seed 42:\n",
      "Model           Accuracy     Time (s)     Epochs    \n",
      "--------------------------------------------------\n",
      "Real            0.8453       33.3         24        \n",
      "Quat            0.8450       55.1         24        \n",
      "QNoEnt          0.8173       20324.1      44        \n",
      "QEnt            0.8253       15044.0      29        \n",
      "\n",
      "Seed 123:\n",
      "Model           Accuracy     Time (s)     Epochs    \n",
      "--------------------------------------------------\n",
      "Real            0.8477       35.2         25        \n",
      "Quat            0.8450       50.2         21        \n",
      "QNoEnt          0.8107       20844.4      45        \n",
      "QEnt            0.8203       50153.9      96        \n",
      "\n",
      "Seed 456:\n",
      "Model           Accuracy     Time (s)     Epochs    \n",
      "--------------------------------------------------\n",
      "Real            0.8450       27.5         20        \n",
      "Quat            0.8440       65.3         27        \n",
      "QNoEnt          0.8330       25685.7      53        \n",
      "QEnt            0.8330       38141.5      74        \n",
      "\n",
      "==========================================================================================\n",
      "COMPARATIVE ANALYSIS: Answering Research Questions\n",
      "==========================================================================================\n",
      "\n",
      "1. QUATERNION vs REAL MLP:\n",
      "   ----------------------------------------------------------------------\n",
      "   Real MLP accuracy:        0.8460 ± 0.0012\n",
      "   Quaternion accuracy:      0.8447 ± 0.0005\n",
      "   Gap:                      0.13 percentage points\n",
      "   Performance retention:    99.8%\n",
      "\n",
      "   → Classical SU(2) (quaternions) captures 99.8% of\n",
      "     standard MLP performance with structured algebraic constraints.\n",
      "\n",
      "2. QUANTUM (no entanglement) vs REAL MLP:\n",
      "   ----------------------------------------------------------------------\n",
      "   Real MLP accuracy:        0.8460 ± 0.0012\n",
      "   Quantum (no ent) accuracy:0.8203 ± 0.0094\n",
      "   Gap:                      2.57 percentage points\n",
      "   Performance retention:    97.0%\n",
      "\n",
      "   → Quantum circuits WITHOUT entanglement capture 97.0%\n",
      "     of MLP performance, suggesting limited benefit over classical rotation gates.\n",
      "\n",
      "3. QUANTUM (with entanglement) vs REAL MLP:\n",
      "   ----------------------------------------------------------------------\n",
      "   Real MLP accuracy:        0.8460 ± 0.0012\n",
      "   Quantum (with ent) accuracy:0.8262 ± 0.0052\n",
      "   Gap:                      1.98 percentage points\n",
      "   Performance retention:    97.7%\n",
      "\n",
      "   → Quantum circuits WITH entanglement capture 97.7%\n",
      "     of MLP performance.\n",
      "\n",
      "4. ENTANGLEMENT EFFECT:\n",
      "   ----------------------------------------------------------------------\n",
      "   Quantum (no ent) accuracy:0.8203 ± 0.0094\n",
      "   Quantum (with ent) accuracy:0.8262 ± 0.0052\n",
      "   Improvement:              0.59 percentage points\n",
      "\n",
      "   → Entanglement IMPROVES performance by 0.7%\n",
      "     (relative improvement over non-entangled baseline)\n",
      "\n",
      "5. QUATERNION vs QUANTUM (no entanglement) - Core Research Question:\n",
      "   ----------------------------------------------------------------------\n",
      "   Quaternion accuracy:      0.8447 ± 0.0005\n",
      "   Quantum (no ent) accuracy:0.8203 ± 0.0094\n",
      "   Gap:                      2.43 percentage points\n",
      "\n",
      "   → Quaternion networks OUTPERFORM quantum circuits without entanglement\n",
      "     by 2.43 percentage points, suggesting classical SU(2)\n",
      "     provides a more effective representation on this task.\n",
      "\n",
      "6. QUATERNION vs QUANTUM (with entanglement):\n",
      "   ----------------------------------------------------------------------\n",
      "   Quaternion accuracy:      0.8447 ± 0.0005\n",
      "   Quantum (with ent) accuracy:0.8262 ± 0.0052\n",
      "   Gap:                      1.84 percentage points\n",
      "\n",
      "   → Quaternions match or exceed entangled quantum performance,\n",
      "     suggesting entanglement provides limited benefit on this task.\n",
      "\n",
      "==========================================================================================\n",
      "KEY TAKEAWAYS FOR PAPER\n",
      "==========================================================================================\n",
      "\n",
      "✓ EMPIRICAL FINDINGS:\n",
      "  1. Quaternion networks achieve 99.8% of Real MLP performance\n",
      "     with structured SU(2) algebraic constraints\n",
      "\n",
      "  2. Quaternions vs Quantum (no ent): 2.43 percentage point gap\n",
      "\n",
      "  3. Entanglement provides minimal benefit (0.7%) on FashionMNIST\n",
      "     → Single-qubit rotations sufficient for this classification task\n",
      "\n",
      "✓ IMPLICATIONS:\n",
      "  • Quaternion networks provide an efficient classical surrogate for\n",
      "    single-qubit quantum models (SU(2) rotations without entanglement)\n",
      "  • For tasks where entanglement is unnecessary, classical quaternion\n",
      "    algebra may be preferable (faster training, no quantum hardware)\n",
      "  • When entanglement provides measurable benefit, quantum advantage emerges\n",
      "\n",
      "✓ COMPUTATIONAL EFFICIENCY:\n",
      "  • Real MLP:  32.0s (baseline)\n",
      "  • Quaternion: 56.9s (1.78x Real)\n",
      "  • Quantum (no ent): 22284.7s (696.5x Real)\n",
      "    → Even with lightning.gpu, quantum is 391.9x slower than quaternions\n",
      "==========================================================================================\n",
      "\n",
      "==========================================================================================\n",
      "ANALYSIS COMPLETE\n",
      "==========================================================================================\n",
      "\n",
      "All results saved in:\n",
      "  • realnet_results.pt\n",
      "  • quatnet_results.pt\n",
      "  • quantum_noent_results.pt\n",
      "  • quantum_ent_results.pt\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Block 5: Aggregate Results and Comparative Analysis\n",
    "====================================================\n",
    "Loads results from Blocks 1-4 and performs comprehensive comparison.\n",
    "\n",
    "Requirements:\n",
    "- realnet_results.pt (from Block 1)\n",
    "- quatnet_results.pt (from Block 2)\n",
    "- quantum_noent_results.pt (from Block 3)\n",
    "- quantum_ent_results.pt (from Block 4)\n",
    "\n",
    "Outputs:\n",
    "- Comprehensive comparison tables\n",
    "- Statistical analysis\n",
    "- Research question answers\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def load_results():\n",
    "    \"\"\"Load all results files\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    try:\n",
    "        real_data = torch.load(\"realnet_results.pt\", weights_only=False)\n",
    "        results[\"Real\"] = real_data[\"results\"]\n",
    "        print(\"✓ Loaded RealNet results\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"✗ realnet_results.pt not found\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        quat_data = torch.load(\"quatnet_results.pt\", weights_only=False)\n",
    "        results[\"Quat\"] = quat_data[\"results\"]\n",
    "        print(\"✓ Loaded QuatNet results\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"✗ quatnet_results.pt not found\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        qno_data = torch.load(\"quantum_noent_results.pt\", weights_only=False)\n",
    "        results[\"QNoEnt\"] = qno_data[\"results\"]\n",
    "        print(\"✓ Loaded Quantum (no ent) results\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"⚠ quantum_noent_results.pt not found (skipping)\")\n",
    "        results[\"QNoEnt\"] = []\n",
    "    \n",
    "    try:\n",
    "        qent_data = torch.load(\"quantum_ent_results.pt\", weights_only=False)\n",
    "        results[\"QEnt\"] = qent_data[\"results\"]\n",
    "        print(\"✓ Loaded Quantum (with ent) results\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"⚠ quantum_ent_results.pt not found (skipping)\")\n",
    "        results[\"QEnt\"] = []\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def print_summary_table(results):\n",
    "    \"\"\"Print aggregated summary table\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(\"AGGREGATED RESULTS (mean ± std over 3 seeds)\")\n",
    "    print(\"=\" * 90)\n",
    "    print(f\"{'Model':<15} {'Accuracy':<20} {'Time (s)':<20} {'Epochs':<15} {'Parameters':<20}\")\n",
    "    print(\"-\" * 90)\n",
    "    \n",
    "    for name in [\"Real\", \"Quat\", \"QNoEnt\", \"QEnt\"]:\n",
    "        if not results[name]:\n",
    "            continue\n",
    "            \n",
    "        accs = [r[\"best_acc\"] for r in results[name]]\n",
    "        times = [r[\"time\"] for r in results[name]]\n",
    "        epochs = [r[\"epochs\"] for r in results[name]]\n",
    "        \n",
    "        acc_str = f\"{np.mean(accs):.4f} ± {np.std(accs):.4f}\"\n",
    "        time_str = f\"{np.mean(times):.1f} ± {np.std(times):.1f}\"\n",
    "        epoch_str = f\"{np.mean(epochs):.1f} ± {np.std(epochs):.1f}\"\n",
    "        \n",
    "        if name == \"Real\":\n",
    "            params = results[name][0][\"params\"]\n",
    "            param_str = f\"{params:,}\"\n",
    "        else:\n",
    "            trainable = results[name][0][\"trainable_params\"]\n",
    "            total = results[name][0][\"total_params\"]\n",
    "            param_str = f\"{trainable:,} (head)\"\n",
    "        \n",
    "        print(f\"{name:<15} {acc_str:<20} {time_str:<20} {epoch_str:<15} {param_str:<20}\")\n",
    "    \n",
    "    print(\"=\" * 90)\n",
    "\n",
    "\n",
    "def print_per_seed_table(results):\n",
    "    \"\"\"Print detailed per-seed results\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"PER-SEED RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    seeds = [42, 123, 456]\n",
    "    \n",
    "    for seed_idx, seed in enumerate(seeds):\n",
    "        print(f\"\\nSeed {seed}:\")\n",
    "        print(f\"{'Model':<15} {'Accuracy':<12} {'Time (s)':<12} {'Epochs':<10}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for name in [\"Real\", \"Quat\", \"QNoEnt\", \"QEnt\"]:\n",
    "            if not results[name]:\n",
    "                continue\n",
    "            \n",
    "            r = results[name][seed_idx]\n",
    "            print(f\"{name:<15} {r['best_acc']:<12.4f} {r['time']:<12.1f} {r['epochs']:<10}\")\n",
    "\n",
    "\n",
    "def comparative_analysis(results):\n",
    "    \"\"\"Perform comparative analysis answering research questions\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(\"COMPARATIVE ANALYSIS: Answering Research Questions\")\n",
    "    print(\"=\" * 90)\n",
    "    \n",
    "    real_accs = [r[\"best_acc\"] for r in results[\"Real\"]]\n",
    "    quat_accs = [r[\"best_acc\"] for r in results[\"Quat\"]]\n",
    "    \n",
    "    def pct_gap(a, b):\n",
    "        \"\"\"Percentage point gap (a - b)\"\"\"\n",
    "        return (np.mean(a) - np.mean(b)) * 100.0\n",
    "    \n",
    "    def retention(a, b):\n",
    "        \"\"\"Percentage of performance retained\"\"\"\n",
    "        return (np.mean(a) / np.mean(b)) * 100.0\n",
    "    \n",
    "    print(\"\\n1. QUATERNION vs REAL MLP:\")\n",
    "    print(\"   \" + \"-\" * 70)\n",
    "    print(f\"   Real MLP accuracy:        {np.mean(real_accs):.4f} ± {np.std(real_accs):.4f}\")\n",
    "    print(f\"   Quaternion accuracy:      {np.mean(quat_accs):.4f} ± {np.std(quat_accs):.4f}\")\n",
    "    print(f\"   Gap:                      {pct_gap(real_accs, quat_accs):.2f} percentage points\")\n",
    "    print(f\"   Performance retention:    {retention(quat_accs, real_accs):.1f}%\")\n",
    "    print(f\"\\n   → Classical SU(2) (quaternions) captures {retention(quat_accs, real_accs):.1f}% of\")\n",
    "    print(f\"     standard MLP performance with structured algebraic constraints.\")\n",
    "    \n",
    "    if results[\"QNoEnt\"]:\n",
    "        qno_accs = [r[\"best_acc\"] for r in results[\"QNoEnt\"]]\n",
    "        \n",
    "        print(\"\\n2. QUANTUM (no entanglement) vs REAL MLP:\")\n",
    "        print(\"   \" + \"-\" * 70)\n",
    "        print(f\"   Real MLP accuracy:        {np.mean(real_accs):.4f} ± {np.std(real_accs):.4f}\")\n",
    "        print(f\"   Quantum (no ent) accuracy:{np.mean(qno_accs):.4f} ± {np.std(qno_accs):.4f}\")\n",
    "        print(f\"   Gap:                      {pct_gap(real_accs, qno_accs):.2f} percentage points\")\n",
    "        print(f\"   Performance retention:    {retention(qno_accs, real_accs):.1f}%\")\n",
    "        print(f\"\\n   → Quantum circuits WITHOUT entanglement capture {retention(qno_accs, real_accs):.1f}%\")\n",
    "        print(f\"     of MLP performance, suggesting limited benefit over classical rotation gates.\")\n",
    "    \n",
    "    if results[\"QEnt\"]:\n",
    "        qent_accs = [r[\"best_acc\"] for r in results[\"QEnt\"]]\n",
    "        \n",
    "        print(\"\\n3. QUANTUM (with entanglement) vs REAL MLP:\")\n",
    "        print(\"   \" + \"-\" * 70)\n",
    "        print(f\"   Real MLP accuracy:        {np.mean(real_accs):.4f} ± {np.std(real_accs):.4f}\")\n",
    "        print(f\"   Quantum (with ent) accuracy:{np.mean(qent_accs):.4f} ± {np.std(qent_accs):.4f}\")\n",
    "        print(f\"   Gap:                      {pct_gap(real_accs, qent_accs):.2f} percentage points\")\n",
    "        print(f\"   Performance retention:    {retention(qent_accs, real_accs):.1f}%\")\n",
    "        print(f\"\\n   → Quantum circuits WITH entanglement capture {retention(qent_accs, real_accs):.1f}%\")\n",
    "        print(f\"     of MLP performance.\")\n",
    "    \n",
    "    if results[\"QNoEnt\"] and results[\"QEnt\"]:\n",
    "        print(\"\\n4. ENTANGLEMENT EFFECT:\")\n",
    "        print(\"   \" + \"-\" * 70)\n",
    "        print(f\"   Quantum (no ent) accuracy:{np.mean(qno_accs):.4f} ± {np.std(qno_accs):.4f}\")\n",
    "        print(f\"   Quantum (with ent) accuracy:{np.mean(qent_accs):.4f} ± {np.std(qent_accs):.4f}\")\n",
    "        print(f\"   Improvement:              {pct_gap(qent_accs, qno_accs):.2f} percentage points\")\n",
    "        \n",
    "        if np.mean(qent_accs) > np.mean(qno_accs):\n",
    "            improvement_pct = ((np.mean(qent_accs) - np.mean(qno_accs)) / np.mean(qno_accs)) * 100\n",
    "            print(f\"\\n   → Entanglement IMPROVES performance by {improvement_pct:.1f}%\")\n",
    "            print(f\"     (relative improvement over non-entangled baseline)\")\n",
    "        else:\n",
    "            print(f\"\\n   → Entanglement DOES NOT improve performance on this task\")\n",
    "    \n",
    "    if results[\"QNoEnt\"]:\n",
    "        print(\"\\n5. QUATERNION vs QUANTUM (no entanglement) - Core Research Question:\")\n",
    "        print(\"   \" + \"-\" * 70)\n",
    "        print(f\"   Quaternion accuracy:      {np.mean(quat_accs):.4f} ± {np.std(quat_accs):.4f}\")\n",
    "        print(f\"   Quantum (no ent) accuracy:{np.mean(qno_accs):.4f} ± {np.std(qno_accs):.4f}\")\n",
    "        print(f\"   Gap:                      {pct_gap(quat_accs, qno_accs):.2f} percentage points\")\n",
    "        \n",
    "        gap_abs = abs(pct_gap(quat_accs, qno_accs))\n",
    "        if gap_abs < 2.0:\n",
    "            print(f\"\\n   → Quaternion networks (classical SU(2)) CLOSELY APPROXIMATE quantum circuits\")\n",
    "            print(f\"     without entanglement (quantum SU(2)). Gap < 2 percentage points.\")\n",
    "            print(f\"\\n   → This supports the hypothesis that classical quaternion algebra can serve\")\n",
    "            print(f\"     as an effective surrogate for single-qubit quantum models on FashionMNIST.\")\n",
    "        elif np.mean(quat_accs) > np.mean(qno_accs):\n",
    "            print(f\"\\n   → Quaternion networks OUTPERFORM quantum circuits without entanglement\")\n",
    "            print(f\"     by {gap_abs:.2f} percentage points, suggesting classical SU(2)\")\n",
    "            print(f\"     provides a more effective representation on this task.\")\n",
    "        else:\n",
    "            print(f\"\\n   → Quantum circuits without entanglement OUTPERFORM quaternions\")\n",
    "            print(f\"     by {gap_abs:.2f} percentage points.\")\n",
    "    \n",
    "    if results[\"QEnt\"]:\n",
    "        print(\"\\n6. QUATERNION vs QUANTUM (with entanglement):\")\n",
    "        print(\"   \" + \"-\" * 70)\n",
    "        print(f\"   Quaternion accuracy:      {np.mean(quat_accs):.4f} ± {np.std(quat_accs):.4f}\")\n",
    "        print(f\"   Quantum (with ent) accuracy:{np.mean(qent_accs):.4f} ± {np.std(qent_accs):.4f}\")\n",
    "        print(f\"   Gap:                      {pct_gap(quat_accs, qent_accs):.2f} percentage points\")\n",
    "        \n",
    "        if np.mean(qent_accs) > np.mean(quat_accs):\n",
    "            print(f\"\\n   → Entangled quantum circuits outperform quaternions, demonstrating\")\n",
    "            print(f\"     the value of quantum correlations beyond classical SU(2) rotations.\")\n",
    "        else:\n",
    "            print(f\"\\n   → Quaternions match or exceed entangled quantum performance,\")\n",
    "            print(f\"     suggesting entanglement provides limited benefit on this task.\")\n",
    "\n",
    "\n",
    "def key_takeaways(results):\n",
    "    \"\"\"Summarize key takeaways for paper\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(\"KEY TAKEAWAYS FOR PAPER\")\n",
    "    print(\"=\" * 90)\n",
    "    \n",
    "    real_accs = [r[\"best_acc\"] for r in results[\"Real\"]]\n",
    "    quat_accs = [r[\"best_acc\"] for r in results[\"Quat\"]]\n",
    "    \n",
    "    def retention(a, b):\n",
    "        return (np.mean(a) / np.mean(b)) * 100.0\n",
    "    \n",
    "    print(\"\\n✓ EMPIRICAL FINDINGS:\")\n",
    "    print(f\"  1. Quaternion networks achieve {retention(quat_accs, real_accs):.1f}% of Real MLP performance\")\n",
    "    print(f\"     with structured SU(2) algebraic constraints\")\n",
    "    \n",
    "    if results[\"QNoEnt\"]:\n",
    "        qno_accs = [r[\"best_acc\"] for r in results[\"QNoEnt\"]]\n",
    "        gap = abs((np.mean(quat_accs) - np.mean(qno_accs)) * 100.0)\n",
    "        print(f\"\\n  2. Quaternions vs Quantum (no ent): {gap:.2f} percentage point gap\")\n",
    "        if gap < 2.0:\n",
    "            print(f\"     → Classical SU(2) effectively approximates quantum SU(2) without entanglement\")\n",
    "    \n",
    "    if results[\"QEnt\"] and results[\"QNoEnt\"]:\n",
    "        qent_accs = [r[\"best_acc\"] for r in results[\"QEnt\"]]\n",
    "        qno_accs = [r[\"best_acc\"] for r in results[\"QNoEnt\"]]\n",
    "        ent_improvement = ((np.mean(qent_accs) - np.mean(qno_accs)) / np.mean(qno_accs)) * 100\n",
    "        if ent_improvement > 1.0:\n",
    "            print(f\"\\n  3. Entanglement improves quantum performance by {ent_improvement:.1f}%\")\n",
    "            print(f\"     → Demonstrates measurable value of quantum correlations on FashionMNIST\")\n",
    "        else:\n",
    "            print(f\"\\n  3. Entanglement provides minimal benefit ({ent_improvement:.1f}%) on FashionMNIST\")\n",
    "            print(f\"     → Single-qubit rotations sufficient for this classification task\")\n",
    "    \n",
    "    print(\"\\n✓ IMPLICATIONS:\")\n",
    "    print(\"  • Quaternion networks provide an efficient classical surrogate for\")\n",
    "    print(\"    single-qubit quantum models (SU(2) rotations without entanglement)\")\n",
    "    print(\"  • For tasks where entanglement is unnecessary, classical quaternion\")\n",
    "    print(\"    algebra may be preferable (faster training, no quantum hardware)\")\n",
    "    print(\"  • When entanglement provides measurable benefit, quantum advantage emerges\")\n",
    "    \n",
    "    print(\"\\n✓ COMPUTATIONAL EFFICIENCY:\")\n",
    "    real_time = np.mean([r[\"time\"] for r in results[\"Real\"]])\n",
    "    quat_time = np.mean([r[\"time\"] for r in results[\"Quat\"]])\n",
    "    print(f\"  • Real MLP:  {real_time:.1f}s (baseline)\")\n",
    "    print(f\"  • Quaternion: {quat_time:.1f}s ({quat_time/real_time:.2f}x Real)\")\n",
    "    \n",
    "    if results[\"QNoEnt\"]:\n",
    "        qno_time = np.mean([r[\"time\"] for r in results[\"QNoEnt\"]])\n",
    "        print(f\"  • Quantum (no ent): {qno_time:.1f}s ({qno_time/real_time:.1f}x Real)\")\n",
    "        print(f\"    → Even with lightning.gpu, quantum is {qno_time/quat_time:.1f}x slower than quaternions\")\n",
    "    \n",
    "    print(\"=\" * 90)\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 90)\n",
    "    print(\"BLOCK 5: AGGREGATE RESULTS AND COMPARATIVE ANALYSIS\")\n",
    "    print(\"=\" * 90)\n",
    "    \n",
    "    results = load_results()\n",
    "    \n",
    "    if results is None:\n",
    "        print(\"\\n✗ ERROR: Required results files not found.\")\n",
    "        print(\"Run Blocks 1 and 2 at minimum (Real and Quat).\")\n",
    "        return\n",
    "    \n",
    "    # Summary table\n",
    "    print_summary_table(results)\n",
    "    \n",
    "    # Per-seed details\n",
    "    print_per_seed_table(results)\n",
    "    \n",
    "    # Comparative analysis\n",
    "    comparative_analysis(results)\n",
    "    \n",
    "    # Key takeaways\n",
    "    key_takeaways(results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(\"ANALYSIS COMPLETE\")\n",
    "    print(\"=\" * 90)\n",
    "    print(\"\\nAll results saved in:\")\n",
    "    print(\"  • realnet_results.pt\")\n",
    "    print(\"  • quatnet_results.pt\")\n",
    "    if results[\"QNoEnt\"]:\n",
    "        print(\"  • quantum_noent_results.pt\")\n",
    "    if results[\"QEnt\"]:\n",
    "        print(\"  • quantum_ent_results.pt\")\n",
    "    print(\"=\" * 90)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3e3e5c-7bff-46c7-a0a7-91a313e86b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch-env-clone)",
   "language": "python",
   "name": "torch-env-clone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
