{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646f4471-e302-417e-b21f-15fe5f4d4e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using lightning.gpu device\n",
      "Using PyTorch device: cuda\n",
      "======================================================================\n",
      "BLOCK 4: Quantum (WITH Entanglement) Training\n",
      "======================================================================\n",
      "\n",
      "Configuration:\n",
      "  • Batch size: 32\n",
      "  • Patience: 10\n",
      "  • Max epochs: 200\n",
      "  • Seeds: [42, 123, 456]\n",
      "  • Architecture: [frozen 784→16] → 4 qubits (3 layers, WITH ent) → 10\n",
      "  • Quantum device: lightning.gpu\n",
      "======================================================================\n",
      "\n",
      "Loading frozen preprocessor from Block 1...\n",
      "✓ Loaded preprocessor state from realnet_results.pt\n",
      "  Preprocessor frozen with 12,560 params\n",
      "\n",
      "Creating stratified samples...\n",
      "  Train samples: 15000 (stratified, 1500 per class)\n",
      "  Test samples:  3000 (stratified, 300 per class)\n",
      "\n",
      "======================================================================\n",
      "SEED 42\n",
      "======================================================================\n",
      "\n",
      "  Training QuantumNet (WITH entanglement, seed=42)...\n",
      "  [QuantumEnt] Epoch 1/200\n",
      "    Batch 40/469, samples: 1312"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Block 4: Quantum (WITH Entanglement) Training\n",
    "==============================================\n",
    "Loads frozen preprocessor from Block 1 and trains quantum head WITH entanglement.\n",
    "Uses Lightning-GPU acceleration.\n",
    "\n",
    "Requirements:\n",
    "- realnet_results.pt (from Block 1)\n",
    "- pennylane, pennylane-lightning-gpu\n",
    "\n",
    "Outputs:\n",
    "- quantum_ent_results.pt: Contains quantum (with ent) results\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from collections import defaultdict\n",
    "\n",
    "try:\n",
    "    import pennylane as qml\n",
    "    QUANTUM_DEVICE = \"lightning.gpu\"\n",
    "    PENNYLANE_AVAILABLE = True\n",
    "    print(\"✓ Using lightning.gpu device\")\n",
    "except ImportError:\n",
    "    PENNYLANE_AVAILABLE = False\n",
    "    print(\"✗ PennyLane not installed\")\n",
    "    exit(1)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "print(f\"Using PyTorch device: {device}\")\n",
    "\n",
    "# ============================================\n",
    "# Comprehensive seed setting for reproducibility\n",
    "# ============================================\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    \"\"\"Set seeds for all RNG sources for reproducibility\"\"\"\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    # Python\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # NumPy\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # PyTorch deterministic mode\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # CuPy (if available)\n",
    "    try:\n",
    "        import cupy as cp\n",
    "        cp.random.seed(seed)\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    # PennyLane (if available)\n",
    "    try:\n",
    "        import pennylane as qml\n",
    "        qml.numpy.random.seed(seed)\n",
    "    except (ImportError, AttributeError):\n",
    "        pass\n",
    "\n",
    "# ============================================\n",
    "# Shared Preprocessor (must match Block 1)\n",
    "# ============================================\n",
    "\n",
    "class SharedPreprocessor(nn.Module):\n",
    "    \"\"\"Shared classical feature extractor: 784 → 16\"\"\"\n",
    "    def __init__(self, input_dim=784, bottleneck_dim=16):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, bottleneck_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.tanh(self.fc(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Quantum Head (WITH Entanglement)\n",
    "# ============================================\n",
    "\n",
    "class QuantumHead(nn.Module):\n",
    "    \"\"\"\n",
    "    VQC with 4 qubits, 3 layers, WITH entanglement → 10 classes.\n",
    "    Uses Lightning acceleration (GPU).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_qubits=4, n_layers=3, num_classes=10, device_name=None):\n",
    "        super().__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Map 16 features → n_qubits\n",
    "        self.feature_select = nn.Linear(16, n_qubits)\n",
    "\n",
    "        # Use specified device or default\n",
    "        if device_name is None:\n",
    "            device_name = QUANTUM_DEVICE\n",
    "        \n",
    "        # Quantum device\n",
    "        self.dev = qml.device(device_name, wires=n_qubits)\n",
    "\n",
    "        # Use adjoint differentiation for lightning (much faster)\n",
    "        diff_method = \"adjoint\" if \"lightning\" in device_name else \"parameter-shift\"\n",
    "\n",
    "        @qml.qnode(self.dev, interface=\"torch\", diff_method=diff_method)\n",
    "        def quantum_circuit(inputs, weights):\n",
    "            \"\"\"\n",
    "            Single-sample circuit WITH entanglement.\n",
    "            inputs: (n_qubits,)\n",
    "            weights: (n_layers, n_qubits, 2)\n",
    "            \"\"\"\n",
    "            for layer in range(n_layers):\n",
    "                # Data re-uploading\n",
    "                for i in range(n_qubits):\n",
    "                    qml.RY(inputs[i], wires=i)\n",
    "\n",
    "                # Trainable rotations\n",
    "                for i in range(n_qubits):\n",
    "                    qml.RY(weights[layer, i, 0], wires=i)\n",
    "                    qml.RZ(weights[layer, i, 1], wires=i)\n",
    "\n",
    "                # ENTANGLEMENT: CNOT ring\n",
    "                for i in range(n_qubits - 1):\n",
    "                    qml.CNOT(wires=[i, i + 1])\n",
    "                if n_qubits > 2:\n",
    "                    qml.CNOT(wires=[n_qubits - 1, 0])\n",
    "\n",
    "            return [\n",
    "                qml.expval(qml.PauliZ(0)),\n",
    "                qml.expval(qml.PauliZ(1)),\n",
    "                qml.expval(qml.PauliZ(2)),\n",
    "                qml.expval(qml.PauliZ(3)),\n",
    "                qml.expval(qml.PauliZ(0) @ qml.PauliZ(1)),\n",
    "                qml.expval(qml.PauliZ(2) @ qml.PauliZ(3))\n",
    "            ]\n",
    "\n",
    "        self.quantum_circuit = quantum_circuit\n",
    "\n",
    "        weight_shape = (n_layers, n_qubits, 2)\n",
    "        self.q_weights = nn.Parameter(torch.randn(weight_shape) * 0.1)\n",
    "        self.fc_out = nn.Linear(6, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Process samples with progress tracking.\n",
    "        x: (batch, 16) real bottleneck features\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        x = torch.tanh(self.feature_select(x))\n",
    "\n",
    "        # Process in chunks for memory management\n",
    "        chunk_size = 32\n",
    "        quantum_outputs = []\n",
    "\n",
    "        for start_idx in range(0, batch_size, chunk_size):\n",
    "            end_idx = min(start_idx + chunk_size, batch_size)\n",
    "            chunk = x[start_idx:end_idx]\n",
    "\n",
    "            chunk_outputs = []\n",
    "            for i in range(chunk.size(0)):\n",
    "                q_raw = self.quantum_circuit(chunk[i], self.q_weights)\n",
    "                if isinstance(q_raw, (list, tuple)):\n",
    "                    q_out = torch.stack(q_raw)\n",
    "                else:\n",
    "                    q_out = q_raw\n",
    "                chunk_outputs.append(q_out)\n",
    "\n",
    "            quantum_outputs.extend(chunk_outputs)\n",
    "\n",
    "        # Convert to tensor (cast to float32)\n",
    "        quantum_outputs = torch.stack(quantum_outputs).float()\n",
    "        quantum_outputs = quantum_outputs.to(self.fc_out.weight.dtype)\n",
    "\n",
    "        output = self.fc_out(quantum_outputs)\n",
    "        return output\n",
    "\n",
    "\n",
    "class QuantumNet(nn.Module):\n",
    "    \"\"\"Complete Quantum network: Preprocessor + QuantumHead (with ent)\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.preprocessor = SharedPreprocessor(784, 16)\n",
    "        self.head = QuantumHead(n_qubits=4, n_layers=3, num_classes=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.preprocessor(x)\n",
    "        return self.head(features)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Training and Evaluation\n",
    "# ============================================\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, show_progress=True):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(loader):\n",
    "        # Keep data on CPU for quantum models\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        total_samples += x.size(0)\n",
    "\n",
    "        if show_progress and batch_idx % 20 == 0:\n",
    "            print(f\"    Batch {batch_idx}/{len(loader)}, samples: {total_samples}\", end=\"\\r\")\n",
    "\n",
    "    if show_progress:\n",
    "        print()\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            # Keep data on CPU\n",
    "            logits = model(x)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += x.size(0)\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "def train_with_early_stopping(model, train_loader, test_loader, optimizer,\n",
    "                              max_epochs=200, patience=10, name=\"Model\"):\n",
    "    best_acc = 0.0\n",
    "    epochs_without_improvement = 0\n",
    "    start = time.time()\n",
    "    last_acc = 0.0\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        print(f\"  [{name}] Epoch {epoch}/{max_epochs}\")\n",
    "        loss = train_one_epoch(model, train_loader, optimizer, show_progress=True)\n",
    "        acc = evaluate(model, test_loader)\n",
    "        last_acc = acc\n",
    "\n",
    "        elapsed = time.time() - start\n",
    "        print(f\"  [{name}] Epoch {epoch:2d} | loss={loss:.4f} \"\n",
    "              f\"| test_acc={acc:.4f} | time={elapsed:.1f}s\")\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"  [{name}] Early stop at epoch {epoch} \"\n",
    "                  f\"(no improvement for {patience} epochs)\")\n",
    "            break\n",
    "\n",
    "    total_time = time.time() - start\n",
    "    return {\n",
    "        \"best_acc\": best_acc,\n",
    "        \"final_acc\": last_acc,\n",
    "        \"time\": total_time,\n",
    "        \"epochs\": epoch,\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Data utilities\n",
    "# ============================================\n",
    "\n",
    "def stratified_sample(dataset, n_samples_per_class):\n",
    "    \"\"\"\n",
    "    Create a stratified sample with n_samples_per_class from each class.\n",
    "    Maintains class balance.\n",
    "    \"\"\"\n",
    "    # Group indices by class\n",
    "    class_indices = defaultdict(list)\n",
    "    for idx, (_, label) in enumerate(dataset):\n",
    "        class_indices[label].append(idx)\n",
    "    \n",
    "    # Sample from each class\n",
    "    sampled_indices = []\n",
    "    for class_label in sorted(class_indices.keys()):\n",
    "        indices = class_indices[class_label]\n",
    "        # Use fixed random seed for reproducibility\n",
    "        rng = np.random.RandomState(42)\n",
    "        selected = rng.choice(indices, size=min(n_samples_per_class, len(indices)), \n",
    "                             replace=False)\n",
    "        sampled_indices.extend(selected)\n",
    "    \n",
    "    return sampled_indices\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Main\n",
    "# ============================================\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"BLOCK 4: Quantum (WITH Entanglement) Training\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nConfiguration:\")\n",
    "    print(\"  • Batch size: 32\")\n",
    "    print(\"  • Patience: 10\")\n",
    "    print(\"  • Max epochs: 200\")\n",
    "    print(\"  • Seeds: [42, 123, 456]\")\n",
    "    print(\"  • Architecture: [frozen 784→16] → 4 qubits (3 layers, WITH ent) → 10\")\n",
    "    print(f\"  • Quantum device: {QUANTUM_DEVICE}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Load frozen preprocessor from Block 1\n",
    "    print(\"\\nLoading frozen preprocessor from Block 1...\")\n",
    "    try:\n",
    "        realnet_data = torch.load(\"realnet_results.pt\", weights_only=False)\n",
    "        preprocessor_state = realnet_data[\"preprocessor_state\"]\n",
    "        print(\"✓ Loaded preprocessor state from realnet_results.pt\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"✗ ERROR: realnet_results.pt not found!\")\n",
    "        print(\"  Run Block 1 first to train RealNet and save preprocessor.\")\n",
    "        return\n",
    "\n",
    "    # Create frozen preprocessor (CPU for quantum)\n",
    "    shared_preprocessor = SharedPreprocessor(784, 16)\n",
    "    shared_preprocessor.load_state_dict(preprocessor_state)\n",
    "    for p in shared_preprocessor.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    preprocessor_params = sum(p.numel() for p in shared_preprocessor.parameters())\n",
    "    print(f\"  Preprocessor frozen with {preprocessor_params:,} params\")\n",
    "\n",
    "    # Load data\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    full_train_ds = datasets.MNIST(root=\"./data\", train=True,\n",
    "                                   download=True, transform=transform)\n",
    "    full_test_ds = datasets.MNIST(root=\"./data\", train=False,\n",
    "                                  download=True, transform=transform)\n",
    "\n",
    "    # Create stratified samples (same as Block 1)\n",
    "    print(\"\\nCreating stratified samples...\")\n",
    "    train_indices = stratified_sample(full_train_ds, n_samples_per_class=1500)\n",
    "    test_indices = stratified_sample(full_test_ds, n_samples_per_class=300)\n",
    "    \n",
    "    train_ds = Subset(full_train_ds, train_indices)\n",
    "    test_ds = Subset(full_test_ds, test_indices)\n",
    "    \n",
    "    print(f\"  Train samples: {len(train_ds)} (stratified, 1500 per class)\")\n",
    "    print(f\"  Test samples:  {len(test_ds)} (stratified, 300 per class)\")\n",
    "\n",
    "    # Smaller batches for quantum\n",
    "    train_loader = DataLoader(train_ds, batch_size=32,\n",
    "                              shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_ds, batch_size=64,\n",
    "                             shuffle=False, num_workers=0)\n",
    "\n",
    "    seeds = [42, 123, 456]\n",
    "    all_results = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(f\"SEED {seed}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        set_all_seeds(seed)\n",
    "\n",
    "        print(f\"\\n  Training QuantumNet (WITH entanglement, seed={seed})...\")\n",
    "        quantum_model = QuantumNet()\n",
    "        quantum_model.preprocessor = shared_preprocessor  # Use frozen preprocessor\n",
    "        \n",
    "        quantum_opt = torch.optim.Adam(quantum_model.head.parameters(), lr=1e-3)\n",
    "        \n",
    "        result = train_with_early_stopping(\n",
    "            quantum_model, train_loader, test_loader, quantum_opt,\n",
    "            max_epochs=200, patience=10, name=\"QuantumEnt\"\n",
    "        )\n",
    "        \n",
    "        result[\"trainable_params\"] = sum(p.numel() for p in quantum_model.parameters()\n",
    "                                         if p.requires_grad)\n",
    "        result[\"total_params\"] = sum(p.numel() for p in quantum_model.parameters())\n",
    "        result[\"seed\"] = seed\n",
    "        \n",
    "        all_results.append(result)\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"QUANTUM (WITH ENTANGLEMENT) SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    accs = [r[\"best_acc\"] for r in all_results]\n",
    "    times = [r[\"time\"] for r in all_results]\n",
    "    epochs = [r[\"epochs\"] for r in all_results]\n",
    "    trainable = all_results[0][\"trainable_params\"]\n",
    "    total = all_results[0][\"total_params\"]\n",
    "    \n",
    "    print(f\"\\nAccuracy:   {np.mean(accs):.4f} ± {np.std(accs):.4f}\")\n",
    "    print(f\"Time:       {np.mean(times):.1f}s ± {np.std(times):.1f}s\")\n",
    "    print(f\"Epochs:     {np.mean(epochs):.1f} ± {np.std(epochs):.1f}\")\n",
    "    print(f\"Parameters: {trainable:,} trainable (head), {total:,} total\")\n",
    "    \n",
    "    print(\"\\nPer-seed results:\")\n",
    "    for r in all_results:\n",
    "        print(f\"  Seed {r['seed']}: acc={r['best_acc']:.4f}, \"\n",
    "              f\"time={r['time']:.1f}s, epochs={r['epochs']}\")\n",
    "\n",
    "    # Save results\n",
    "    save_dict = {\n",
    "        \"results\": all_results,\n",
    "        \"summary\": {\n",
    "            \"mean_acc\": np.mean(accs),\n",
    "            \"std_acc\": np.std(accs),\n",
    "            \"mean_time\": np.mean(times),\n",
    "            \"trainable_params\": trainable,\n",
    "            \"total_params\": total\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    torch.save(save_dict, \"quantum_ent_results.pt\")\n",
    "    print(f\"\\n✓ Saved results to: quantum_ent_results.pt\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d75c9cf-2895-4035-8c86-0b5f6c837da8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
